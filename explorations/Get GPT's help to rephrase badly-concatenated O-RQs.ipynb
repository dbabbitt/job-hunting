{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pretty printing has been turned OFF\n"
     ]
    }
   ],
   "source": [
    "\n",
    "%pprint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "%matplotlib inline\n",
    "from IPython.display import display\n",
    "from PIL import Image\n",
    "from datetime import datetime\n",
    "from langchain import PromptTemplate\n",
    "from langchain.chains import LLMChain\n",
    "from langchain.chains.question_answering import load_qa_chain\n",
    "from langchain.embeddings.cohere import CohereEmbeddings\n",
    "from langchain.llms import Cohere\n",
    "from langchain.llms import OpenAI\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.text_splitter import CharacterTextSplitter\n",
    "from langchain.text_splitter import SpacyTextSplitter\n",
    "from langchain.vectorstores import Chroma\n",
    "from neo4j.exceptions import ServiceUnavailable\n",
    "from openai.error import RateLimitError\n",
    "from pandas import DataFrame\n",
    "from stability_sdk import client as stability_client\n",
    "import cohere\n",
    "import getpass\n",
    "import humanize\n",
    "import io\n",
    "import os\n",
    "import pandas as pd\n",
    "import random\n",
    "import re\n",
    "import stability_sdk.interfaces.gooseai.generation.generation_pb2 as stability_generation\n",
    "import sys\n",
    "import time\n",
    "import warnings\n",
    "import winsound\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "duration = 1000  # milliseconds\n",
    "freq = 880  # Hz\n",
    "\n",
    "# Insert at 1, 0 is the script path (or '' in REPL)\n",
    "sys.path.insert(1, '../py')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Get the Storage object\n",
    "from storage import Storage\n",
    "s = Storage(\n",
    "    data_folder_path=os.path.abspath('../data'),\n",
    "    saves_folder_path=os.path.abspath('../saves')\n",
    ")\n",
    "\n",
    "# Get the WebScrapingUtilities object\n",
    "from scrape_utils import WebScrapingUtilities\n",
    "wsu = WebScrapingUtilities(\n",
    "    s=s,\n",
    "    secrets_json_path=os.path.abspath('../data/secrets/jh_secrets.json')\n",
    ")\n",
    "\n",
    "os.environ['SERPAPI_API_KEY'] = wsu.secrets_json['SERPAPI_API_KEY']\n",
    "\n",
    "# Paste your API key here. Remember to not share it publicly\n",
    "co_key = wsu.secrets_json['Cohere_API_Key']\n",
    "os.environ['COHERE_API_KEY'] = co_key\n",
    "co = cohere.Client(co_key)\n",
    "\n",
    "# To get your API key, visit https://beta.dreamstudio.ai/membership\n",
    "os.environ['STABILITY_KEY'] = wsu.secrets_json['Dream_Studio_API_Key']\n",
    "stability_api = stability_client.StabilityInference(\n",
    "    key=os.environ['STABILITY_KEY'], \n",
    "    verbose=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I have 49,163 labeled parts of speech in here\n",
      "predict_single is now available\n",
      "Parts-of-speech logistic regression elements built in 16 seconds\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Get the HeaderAnalysis object\n",
    "from ha_utils import HeaderAnalysis\n",
    "ha = HeaderAnalysis(s=s, verbose=False)\n",
    "\n",
    "# Get the CypherUtilities object and Neo4j driver\n",
    "from cypher_utils import CypherUtilities\n",
    "uri = wsu.secrets_json['neo4j']['connect_url']\n",
    "user =  wsu.secrets_json['neo4j']['username']\n",
    "password = wsu.secrets_json['neo4j']['password']\n",
    "cu = CypherUtilities(\n",
    "    uri=uri, user=user, password=password, driver=None, s=s, ha=ha\n",
    ")\n",
    "\n",
    "# Get the SectionLRClassifierUtilities object\n",
    "from section_classifier_utils import SectionLRClassifierUtilities\n",
    "slrcu = SectionLRClassifierUtilities(ha=ha, cu=cu, verbose=False)\n",
    "\n",
    "# Check if the slrcu has built its parts-of-speech logistic regression elements\n",
    "t1 = time.time()\n",
    "if not hasattr(slrcu, 'pos_predict_percent_fit_dict'):\n",
    "    slrcu.build_pos_logistic_regression_elements(sampling_strategy_limit=None, verbose=True)\n",
    "if hasattr(slrcu, 'pos_predict_percent_fit_dict'):\n",
    "    print('predict_single is now available')\n",
    "else:\n",
    "    print('predict_single is not available')\n",
    "duration_str = humanize.precisedelta(time.time() - t1, minimum_unit='seconds', format='%0.0f')\n",
    "print(f'Parts-of-speech logistic regression elements built in {duration_str}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "predict_single is now available\n",
      "Parts-of-speech conditional random field elements built in 2 seconds\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Get the SectionCRFClassifierUtilities object\n",
    "from section_classifier_utils import SectionCRFClassifierUtilities\n",
    "scrfcu = SectionCRFClassifierUtilities(cu=cu, ha=ha, verbose=False)\n",
    "\n",
    "# Check if the scrfcu has built its parts-of-speech conditional random field elements\n",
    "t1 = time.time()\n",
    "if not hasattr(scrfcu, 'pos_symbol_crf'):\n",
    "    scrfcu.build_pos_conditional_random_field_elements(verbose=True)\n",
    "if hasattr(scrfcu, 'pos_predict_percent_fit_dict'):\n",
    "    print('predict_single is now available')\n",
    "else:\n",
    "    print('predict_single is not available')\n",
    "duration_str = humanize.precisedelta(time.time() - t1, minimum_unit='seconds', format='%0.0f')\n",
    "print(f'Parts-of-speech conditional random field elements built in {duration_str}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I have 49,163 labeled parts of speech in here\n",
      "predict_single is now available\n",
      "Parts-of-speech stochastic gradient descent elements built in 15 seconds\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Get the SectionSGDClassifierUtilities object\n",
    "from section_classifier_utils import SectionSGDClassifierUtilities\n",
    "ssgdcu = SectionSGDClassifierUtilities(ha=ha, cu=cu, verbose=False)\n",
    "\n",
    "# Check if the ssgdcu has built its parts-of-speech stochastic gradient decent elements\n",
    "t1 = time.time()\n",
    "if not hasattr(ssgdcu, 'pos_predict_percent_fit_dict'):\n",
    "    ssgdcu.build_pos_stochastic_gradient_descent_elements(sampling_strategy_limit=None, verbose=True)\n",
    "if hasattr(ssgdcu, 'pos_predict_percent_fit_dict'):\n",
    "    print('predict_single is now available')\n",
    "else:\n",
    "    print('predict_single is not available')\n",
    "duration_str = humanize.precisedelta(time.time() - t1, minimum_unit='seconds', format='%0.0f')\n",
    "print(f'Parts-of-speech stochastic gradient descent elements built in {duration_str}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def combine_adjacent(split_strs_list):\n",
    "    combined_list = []\n",
    "    for i, s in enumerate(split_strs_list):\n",
    "        if i == 0:\n",
    "            combined_list.append(s)\n",
    "        elif combined_list[-1].lower().endswith(' and'):\n",
    "            combined_list[-1] = combined_list[-1] + ' ' + s\n",
    "        else:\n",
    "            combined_list.append(s)\n",
    "    \n",
    "    return combined_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "orq_cypher_str = '''\n",
    "    // Filter for NavigableParents nodes with an ambiguous SUMMARIZES relationship\n",
    "    MATCH (np:NavigableParents)\n",
    "    WHERE size((np)<-[:SUMMARIZES]-(:PartsOfSpeech)) >= 1\n",
    "    WITH np\n",
    "\n",
    "    // Find all NavigableParents nodes in the graph with an incoming SUMMARIZES relationship to a PartsOfSpeech node\n",
    "    MATCH (np)<-[r:SUMMARIZES]-(pos:PartsOfSpeech)\n",
    "    WHERE pos.pos_symbol = \"O-RQ\"\n",
    "\n",
    "    // Return the navigable parent\n",
    "    RETURN np.navigable_parent AS navigable_parent\n",
    "    ORDER BY size(navigable_parent) DESC;'''\n",
    "cleanup_regex = re.compile(r'^<orq>Ability to \\(\\d+%\\) ([A-Z])')\n",
    "cleanup_cypher_str = '''\n",
    "    MATCH (np:NavigableParents)\n",
    "    WHERE np.navigable_parent = $old_child_str\n",
    "    SET np.navigable_parent = $new_child_str;'''\n",
    "fake_stops_list = ['e.g.', 'etc.', 'M.S.', 'B.S.', 'Ph.D.', '(ex.', '(Ex.',\n",
    "                   'U.S.', 'i.e.', '&amp;', 'E.g.', 'Bsc.', 'MSc.', 'incl.']\n",
    "replacements_list = ['eg', 'etc', 'MS', 'BS', 'PhD', '(eg', '(eg', 'US',\n",
    "                     'ie', '&', 'eg', 'BS', 'MS', 'include']\n",
    "tag_regex = re.compile('<([a-z][a-z0-9]*)[^<>]*>')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "----\n",
    "# Make this Work for Job Hunting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>split_str</th>\n",
       "      <th>char_count</th>\n",
       "      <th>tag_name</th>\n",
       "      <th>orq_score</th>\n",
       "      <th>opq_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>4096</th>\n",
       "      <td>Ability to jAVA</td>\n",
       "      <td>15</td>\n",
       "      <td>&lt;orq&gt;</td>\n",
       "      <td>0.885057</td>\n",
       "      <td>9.855221e-44</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4533</th>\n",
       "      <td>Ability to o</td>\n",
       "      <td>12</td>\n",
       "      <td>&lt;orq&gt;</td>\n",
       "      <td>0.887790</td>\n",
       "      <td>1.386220e-47</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5297</th>\n",
       "      <td>Ability to o</td>\n",
       "      <td>12</td>\n",
       "      <td>&lt;orq&gt;</td>\n",
       "      <td>0.887790</td>\n",
       "      <td>1.386220e-47</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9262</th>\n",
       "      <td>Ability to wORKDAY</td>\n",
       "      <td>18</td>\n",
       "      <td>&lt;orq&gt;</td>\n",
       "      <td>0.890161</td>\n",
       "      <td>3.848835e-44</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18623</th>\n",
       "      <td>Ability to lead</td>\n",
       "      <td>15</td>\n",
       "      <td>&lt;orq&gt;</td>\n",
       "      <td>0.905593</td>\n",
       "      <td>2.914283e-48</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                split_str  char_count tag_name  orq_score     opq_score\n",
       "4096      Ability to jAVA          15    <orq>   0.885057  9.855221e-44\n",
       "4533         Ability to o          12    <orq>   0.887790  1.386220e-47\n",
       "5297         Ability to o          12    <orq>   0.887790  1.386220e-47\n",
       "9262   Ability to wORKDAY          18    <orq>   0.890161  3.848835e-44\n",
       "18623     Ability to lead          15    <orq>   0.905593  2.914283e-48"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>split_str</th>\n",
       "      <th>char_count</th>\n",
       "      <th>tag_name</th>\n",
       "      <th>orq_score</th>\n",
       "      <th>opq_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>18623</th>\n",
       "      <td>Ability to lead</td>\n",
       "      <td>15</td>\n",
       "      <td>&lt;orq&gt;</td>\n",
       "      <td>0.905593</td>\n",
       "      <td>2.914283e-48</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16333</th>\n",
       "      <td>Programming (P3 - Advanced)</td>\n",
       "      <td>27</td>\n",
       "      <td>&lt;orq&gt;</td>\n",
       "      <td>0.908975</td>\n",
       "      <td>4.871548e-37</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16239</th>\n",
       "      <td>Programming (P3 - Advanced)</td>\n",
       "      <td>27</td>\n",
       "      <td>&lt;orq&gt;</td>\n",
       "      <td>0.908975</td>\n",
       "      <td>4.871548e-37</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16074</th>\n",
       "      <td>Programming (P3 - Advanced)</td>\n",
       "      <td>27</td>\n",
       "      <td>&lt;orq&gt;</td>\n",
       "      <td>0.908975</td>\n",
       "      <td>4.871548e-37</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14427</th>\n",
       "      <td>Architecture (P2 - Intermediate)</td>\n",
       "      <td>32</td>\n",
       "      <td>&lt;orq&gt;</td>\n",
       "      <td>0.920499</td>\n",
       "      <td>1.634123e-43</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                              split_str  char_count tag_name  orq_score  \\\n",
       "18623                   Ability to lead          15    <orq>   0.905593   \n",
       "16333       Programming (P3 - Advanced)          27    <orq>   0.908975   \n",
       "16239       Programming (P3 - Advanced)          27    <orq>   0.908975   \n",
       "16074       Programming (P3 - Advanced)          27    <orq>   0.908975   \n",
       "14427  Architecture (P2 - Intermediate)          32    <orq>   0.920499   \n",
       "\n",
       "          opq_score  \n",
       "18623  2.914283e-48  \n",
       "16333  4.871548e-37  \n",
       "16239  4.871548e-37  \n",
       "16074  4.871548e-37  \n",
       "14427  1.634123e-43  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "# Break the long HTML string into sentences and check if each is a qualification string\n",
    "if not s.pickle_exists('split_orqs_df'):\n",
    "    t1 = time.time()\n",
    "    with cu.driver.session() as session: df = DataFrame(session.write_transaction(cu.do_cypher_tx, orq_cypher_str))\n",
    "        \n",
    "    # Clean up the \"Ability to (5%)\" stuff\n",
    "    mask_series = df.navigable_parent.map(lambda x: bool(re.search(r'^<orq>Ability to \\(\\d+%\\) ([A-Z])', x)))\n",
    "    for navigable_parent in df[mask_series].navigable_parent:\n",
    "        new_child_str = cleanup_regex.sub(r'<orq>Ability to \\g<1>', navigable_parent)\n",
    "        new_child_str = new_child_str[:16] + new_child_str[16:17].lower() + new_child_str[17:]\n",
    "        def do_cypher_tx(tx, old_child_str, new_child_str):\n",
    "            tx.run(query=cleanup_cypher_str, parameters={'old_child_str': old_child_str, 'new_child_str': new_child_str})\n",
    "        with cu.driver.session() as session: session.write_transaction(\n",
    "            do_cypher_tx, old_child_str=navigable_parent, new_child_str=new_child_str\n",
    "        )\n",
    "    \n",
    "    with cu.driver.session() as session: df = DataFrame(session.write_transaction(cu.do_cypher_tx, orq_cypher_str))\n",
    "    text_splitter = SpacyTextSplitter()\n",
    "    rows_list = []\n",
    "    for html_str in df.navigable_parent:\n",
    "        unhtml_str = re.sub('</?[^><]+>', '', html_str)\n",
    "        for fake_stop, replacement in zip(fake_stops_list, replacements_list):\n",
    "            unhtml_str = unhtml_str.replace(fake_stop, replacement)\n",
    "        split_strs_list = combine_adjacent([str(split_str) for split_str in text_splitter._tokenizer(unhtml_str).sents])\n",
    "        for split_str in split_strs_list:\n",
    "            row_dict = {}\n",
    "            split_str = re.sub(r'\\s*[:;.*]+\\s*$', '', split_str)\n",
    "            row_dict['split_str'] = split_str\n",
    "            row_dict['char_count'] = len(split_str)\n",
    "            match_obj = tag_regex.search(html_str)\n",
    "            if match_obj:\n",
    "                tag_name = match_obj.group()\n",
    "                split_str = f'<{tag_name}>{split_str}</{tag_name}>'\n",
    "            else:\n",
    "                tag_name = 'plaintext'\n",
    "            row_dict['tag_name'] = tag_name\n",
    "            score = 1.0\n",
    "            score *= slrcu.pos_predict_percent_fit_dict['O-RQ'](split_str)\n",
    "            score *= scrfcu.pos_predict_percent_fit_dict['O-RQ'](split_str)\n",
    "            score *= ssgdcu.pos_predict_percent_fit_dict['O-RQ'](split_str)\n",
    "            row_dict['orq_score'] = score\n",
    "            score = 1.0\n",
    "            score *= slrcu.pos_predict_percent_fit_dict['O-PQ'](split_str)\n",
    "            score *= scrfcu.pos_predict_percent_fit_dict['O-PQ'](split_str)\n",
    "            score *= ssgdcu.pos_predict_percent_fit_dict['O-PQ'](split_str)\n",
    "            row_dict['opq_score'] = score\n",
    "            rows_list.append(row_dict)\n",
    "    split_orqs_df = DataFrame(rows_list)\n",
    "    duration_str = humanize.precisedelta(time.time() - t1, minimum_unit='seconds', format='%0.0f')\n",
    "    print(f'Split O-RQs DataFrame built in {duration_str}')\n",
    "    s.store_objects(split_orqs_df=split_orqs_df)\n",
    "else:\n",
    "    split_orqs_df = s.load_object('split_orqs_df')\n",
    "    df = split_orqs_df.copy()\n",
    "    \n",
    "    # Avoid the \"<orq>Ability to \" hack\n",
    "    mask_series = (split_orqs_df.tag_name == '<orq>') & split_orqs_df.split_str.map(lambda x: x.startswith('Ability to '))\n",
    "    display(split_orqs_df[mask_series].sort_values('orq_score').tail())\n",
    "    min_split_score = split_orqs_df[mask_series].orq_score.max()\n",
    "    mask_series = (split_orqs_df.orq_score >= min_split_score)\n",
    "    display(split_orqs_df[mask_series].sort_values('orq_score').head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "----\n",
    "## Get GPT's help to rephrase badly-concatenated O-RQs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We have 2725 more \"Ability to\" requirements to fix\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>navigable_parent</th>\n",
       "      <th>pos_symbol</th>\n",
       "      <th>llm_suggestion</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>52</th>\n",
       "      <td>&lt;orq&gt;Ability to aWS Certification.&lt;/orq&gt;</td>\n",
       "      <td>O-RQ</td>\n",
       "      <td>&lt;orq&gt;AWS Certification is required.&lt;/orq&gt;</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>&lt;orq&gt;Ability to lead&lt;/orq&gt;</td>\n",
       "      <td>O-RQ</td>\n",
       "      <td>&lt;orq&gt;Leadership skills are required.&lt;/orq&gt;</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>51</th>\n",
       "      <td>&lt;orq&gt;Ability to ; Average FTE:91%.&lt;/orq&gt;</td>\n",
       "      <td>O-RQ</td>\n",
       "      <td>&lt;orq&gt;Average FTE of 91% is required.&lt;/orq&gt;</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>55</th>\n",
       "      <td>&lt;orq&gt;Ability to ; Average FTE:63%.&lt;/orq&gt;</td>\n",
       "      <td>O-RQ</td>\n",
       "      <td>&lt;orq&gt;Average FTE of 63% is required.&lt;/orq&gt;</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>513</th>\n",
       "      <td>&lt;orq&gt;Ability to someone with strong excel skil...</td>\n",
       "      <td>O-RQ</td>\n",
       "      <td>&lt;orq&gt;Strong Excel skills are a must.&lt;/orq&gt;</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                      navigable_parent pos_symbol  \\\n",
       "52            <orq>Ability to aWS Certification.</orq>       O-RQ   \n",
       "0                           <orq>Ability to lead</orq>       O-RQ   \n",
       "51            <orq>Ability to ; Average FTE:91%.</orq>       O-RQ   \n",
       "55            <orq>Ability to ; Average FTE:63%.</orq>       O-RQ   \n",
       "513  <orq>Ability to someone with strong excel skil...       O-RQ   \n",
       "\n",
       "                                 llm_suggestion  \n",
       "52    <orq>AWS Certification is required.</orq>  \n",
       "0    <orq>Leadership skills are required.</orq>  \n",
       "51   <orq>Average FTE of 91% is required.</orq>  \n",
       "55   <orq>Average FTE of 63% is required.</orq>  \n",
       "513  <orq>Strong Excel skills are a must.</orq>  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "if not s.pickle_exists('ability_to_df'):\n",
    "    def do_cypher_tx(tx):\n",
    "        cypher_str = '''\n",
    "            // Filter for NavigableParents nodes with an ambiguous SUMMARIZES relationship\n",
    "            MATCH (np:NavigableParents)\n",
    "            WHERE size((np)<-[:SUMMARIZES]-(:PartsOfSpeech)) >= 1\n",
    "            WITH np\n",
    "\n",
    "            // Find all NavigableParents nodes in the graph with an\n",
    "            // incoming SUMMARIZES relationship to a PartsOfSpeech node\n",
    "            MATCH (np)<-[r:SUMMARIZES]-(pos:PartsOfSpeech)\n",
    "            WHERE (np.navigable_parent STARTS WITH \"<orq>Ability to \")\n",
    "\n",
    "            // Return the navigable parent\n",
    "            RETURN\n",
    "                np.navigable_parent AS navigable_parent,\n",
    "                pos.pos_symbol AS pos_symbol\n",
    "            ORDER BY size(navigable_parent) ASC;'''\n",
    "        results_list = tx.run(query=cypher_str, parameters={})\n",
    "\n",
    "        return [dict(record.items()) for record in results_list]\n",
    "    row_objs_list = []\n",
    "    with cu.driver.session() as session:\n",
    "        row_objs_list = session.write_transaction(do_cypher_tx)\n",
    "    if row_objs_list:\n",
    "        ability_to_df = DataFrame(row_objs_list)\n",
    "        print(ability_to_df.shape) # (4159, 2)\n",
    "        \n",
    "        # Hand-labeled examples\n",
    "        mask_series = (ability_to_df.navigable_parent == '<orq>Ability to data Lead</orq>')\n",
    "        ability_to_df.loc[mask_series, 'llm_suggestion'] = '<orq>Skills as a Data Lead are required.</orq>'\n",
    "        mask_series = (ability_to_df.navigable_parent == '<orq>Ability to data Lake</orq>')\n",
    "        ability_to_df.loc[mask_series, 'llm_suggestion'] = '<orq>Experience working with a Data Lake are required.</orq>'\n",
    "        mask_series = (ability_to_df.navigable_parent == '<orq>Ability to no travel.</orq>')\n",
    "        ability_to_df.loc[mask_series, 'llm_suggestion'] = '<orq>Ability to not have to travel is required.</orq>'\n",
    "        mask_series = (ability_to_df.navigable_parent == '<orq>Ability to oracle HCM</orq>')\n",
    "        ability_to_df.loc[mask_series, 'llm_suggestion'] = '<orq>Experience working with Oracle HCM is required.</orq>'\n",
    "        mask_series = (ability_to_df.navigable_parent == '<orq>Ability to no Travel.</orq>')\n",
    "        ability_to_df.loc[mask_series, 'llm_suggestion'] = '<orq>Ability to not have to travel is required.</orq>'\n",
    "        \n",
    "        # Suggested and fed back in\n",
    "        mask_series = (ability_to_df.navigable_parent == '<orq>Ability to lead</orq>')\n",
    "        ability_to_df.loc[mask_series, 'llm_suggestion'] = '<orq>Leadership skills are required.</orq>'\n",
    "        mask_series = (ability_to_df.navigable_parent == '<orq>Ability to jIRA).</orq>')\n",
    "        ability_to_df.loc[mask_series, 'llm_suggestion'] = '<orq>Proficiency in JIRA is necessary.</orq>'\n",
    "        mask_series = (ability_to_df.navigable_parent == '<orq>Ability to elysian.</orq>')\n",
    "        ability_to_df.loc[mask_series, 'llm_suggestion'] = '<orq>Proficiency in Elysian is required.</orq>'\n",
    "        mask_series = (ability_to_df.navigable_parent == '<orq>Ability to pMO Lead</orq>')\n",
    "        ability_to_df.loc[mask_series, 'llm_suggestion'] = '<orq>Leadership skills in project management are necessary.</orq>'\n",
    "        mask_series = (ability_to_df.navigable_parent == '<orq>Ability to pVC Lead</orq>')\n",
    "        ability_to_df.loc[mask_series, 'llm_suggestion'] = '<orq>Leadership skills in venture capital are necessary.</orq>'\n",
    "        mask_series = (ability_to_df.navigable_parent == '<orq>Ability to qA Director</orq>')\n",
    "        ability_to_df.loc[mask_series, 'llm_suggestion'] = '<orq>Experience as a QA Director is required.</orq>'\n",
    "        mask_series = (ability_to_df.navigable_parent == '<orq>Ability to tax reports</orq>')\n",
    "        ability_to_df.loc[mask_series, 'llm_suggestion'] = '<orq>Knowledge of tax reports is required.</orq>'\n",
    "        mask_series = (ability_to_df.navigable_parent == '<orq>Ability to pMO Analyst</orq>')\n",
    "        ability_to_df.loc[mask_series, 'llm_suggestion'] = '<orq>Experience as a PMO Analyst is required.</orq>'\n",
    "        mask_series = (ability_to_df.navigable_parent == '<orq>Ability to team Support</orq>')\n",
    "        ability_to_df.loc[mask_series, 'llm_suggestion'] = '<orq>Ability to provide team support is required.</orq>'\n",
    "        mask_series = (ability_to_df.navigable_parent == '<orq>Ability to data Synapse</orq>')\n",
    "        ability_to_df.loc[mask_series, 'llm_suggestion'] = '<orq>Experience working with Data Synapse is required.</orq>'\n",
    "        mask_series = (ability_to_df.navigable_parent == '<orq>Ability to data Support</orq>')\n",
    "        ability_to_df.loc[mask_series, 'llm_suggestion'] = '<orq>Skills in data support are required.</orq>'\n",
    "        mask_series = (ability_to_df.navigable_parent == '<orq>Ability to oCM Advisory</orq>')\n",
    "        ability_to_df.loc[mask_series, 'llm_suggestion'] = '<orq>Experience in OCM Advisory is required.</orq>'\n",
    "        mask_series = (ability_to_df.navigable_parent == '<orq>Ability to training Lead</orq>')\n",
    "        ability_to_df.loc[mask_series, 'llm_suggestion'] = '<orq>Leadership in training is required.</orq>'\n",
    "        mask_series = (ability_to_df.navigable_parent == '<orq>Ability to security Lead</orq>')\n",
    "        ability_to_df.loc[mask_series, 'llm_suggestion'] = '<orq>Experience as a Security Lead is required.</orq>'\n",
    "        \n",
    "        mask_series = ~ability_to_df.llm_suggestion.isnull() & ~ability_to_df.llm_suggestion.map(lambda x: str(x).startswith('<orq>'))\n",
    "        ability_to_df.loc[mask_series, 'llm_suggestion'] = ability_to_df[mask_series].llm_suggestion.map(lambda x: '<orq>' + x + '</orq>')\n",
    "        \n",
    "        s.store_objects(ability_to_df=ability_to_df)\n",
    "else:\n",
    "    ability_to_df = s.load_object('ability_to_df')\n",
    "    df = ability_to_df.copy()\n",
    "    df['suggestion_size'] = df.llm_suggestion.map(lambda x: len(str(x)))\n",
    "    mask_series = df.llm_suggestion.isnull()\n",
    "    print(f'We have {df[mask_series].shape[0]} more \"Ability to\" requirements to fix')\n",
    "    columns_list = ['navigable_parent', 'pos_symbol', 'llm_suggestion']\n",
    "    display(df[~mask_series].sort_values('suggestion_size')[columns_list].head(5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "template = 'Rephrase the following HTML STRING so that it reads like a MINIMUM REQUIREMENT.'\n",
    "\n",
    "# Hand-labeled examples\n",
    "template += '\\n\\nHTML STRING: \"<orq>Ability to data Lead</orq>\"\\n=========\\n'\n",
    "template += 'MINIMUM REQUIREMENT:\\n'\n",
    "template += \"Skills as a Data Lead are required.\"\n",
    "template += '\\n\\nHTML STRING: \"<orq>Ability to data Lake</orq>\"\\n=========\\n'\n",
    "template += 'MINIMUM REQUIREMENT:\\n'\n",
    "template += \"Experience working with a Data Lake are required.\"\n",
    "template += '\\n\\nHTML STRING: \"<orq>Ability to no travel.</orq>\"\\n=========\\n'\n",
    "template += 'MINIMUM REQUIREMENT:\\n'\n",
    "template += \"Ability to not have to travel is required.\"\n",
    "template += '\\n\\nHTML STRING: \"<orq>Ability to oracle HCM</orq>\"\\n=========\\n'\n",
    "template += 'MINIMUM REQUIREMENT:\\n'\n",
    "template += \"Experience working with Oracle HCM is required.\"\n",
    "template += '\\n\\nHTML STRING: \"<orq>Ability to no Travel.</orq>\"\\n=========\\n'\n",
    "template += 'MINIMUM REQUIREMENT:\\n'\n",
    "template += \"Ability to not have to travel is required.\"\n",
    "template += '\\n\\nHTML STRING: \"<orq>Ability to this role is for International Paper (IP) which is on Microsoft Azure Platform, It is a'\n",
    "template += ' telecom / network engineer role.</orq>\"\\n=========\\n'\n",
    "template += 'MINIMUM REQUIREMENT:\\n'\n",
    "template += \"The ability to work with International Paper (IP), on Microsoft Azure Platform, in a telecom / network engineer role.\"\n",
    "template += '\\n\\nHTML STRING: \"<orq>Ability to the platform will need to support thousands of applications globally, and real time'\n",
    "template += ' onboarding into secure environments.</orq>\"\\n=========\\n'\n",
    "template += 'MINIMUM REQUIREMENT:\\n'\n",
    "template += \"The ability to get the platform to support thousands of applications globally, and real time onboarding into secure environments is required.\"\n",
    "template += '\\n\\nHTML STRING: \"<orq>Ability to support the Learning/Training Lead by designing, developing and deploying the training and'\n",
    "template += ' performance support materials.</orq>\"\\n=========\\n'\n",
    "template += 'MINIMUM REQUIREMENT:\\n'\n",
    "template += \"The ability to support the Learning/Training Lead by designing, developing and deploying the training and performance support materials is required.\"\n",
    "template += '\\n\\nHTML STRING: \"<orq>Ability to experience building Ab initio Real-time service SOAP, Restful and queue based streaming system using'\n",
    "template += ' IBM MQ,Rabbit Queue.</orq>\"\\n=========\\n'\n",
    "template += 'MINIMUM REQUIREMENT:\\n'\n",
    "template += \"We are looking for a candidate with experience building Ab initio with real-time service SOAP, Restful and queue based streaming system\"\n",
    "template += \" using IBM MQ or Rabbit Queue.\"\n",
    "template += '\\n\\nHTML STRING: \"<orq>Ability to the client is looking to automate FRR processes and will require several Informatica/ETL developers'\n",
    "template += ' to support this work.</orq>\"\\n=========\\n'\n",
    "template += 'MINIMUM REQUIREMENT:\\n'\n",
    "template += \"The ability to be a Informatica/ETL developer is required.\"\n",
    "template += '\\n\\nHTML STRING: \"<orq>Ability to we?re looking more for an app arch that can help support the Insights Marketplace web application'\n",
    "template += ' from an arch perspective.</orq>\"\\n=========\\n'\n",
    "template += 'MINIMUM REQUIREMENT:\\n'\n",
    "template += \"The ability to be an Application Architect who can help support the Insights Marketplace web application from an architectural perspective is required.\"\n",
    "template += '\\n\\nHTML STRING: \"<orq>Ability to ***US PERSONS ONLY*** Client team is in need of an experienced SAP PP resource to support'\n",
    "template += ' an ongoing SAP PEO implementation.</orq>\"\\n=========\\n'\n",
    "template += 'MINIMUM REQUIREMENT:\\n'\n",
    "template += \"US Persons Only: we are looking for an experienced SAP PP resource to support an ongoing SAP PEO implementation.\"\n",
    "template += '\\n\\nHTML STRING: \"<orq>Ability to responsible for the configuration and testing of functions in DMS applications/ DERMS based on'\n",
    "template += ' requirements from Client team.</orq>\"\\n=========\\n'\n",
    "template += 'MINIMUM REQUIREMENT:\\n'\n",
    "template += \"We are looking for a candidate who is responsible for the configuration and testing of functions in DMS applications/\"\n",
    "template += \"DERMS based on requirements from Client team.\"\n",
    "template += '\\n\\nHTML STRING: \"<orq>Ability to the PeopleSoft Finances Functional Lead will work with the client to analyze impacts from the new'\n",
    "template += ' PUM and lead implementation.</orq>\"\\n=========\\n'\n",
    "template += 'MINIMUM REQUIREMENT:\\n'\n",
    "template += \"The ability to be a PeopleSoft Finances Functional Lead who will work with the client to analyze impacts from the new PUM and lead implementation.\"\n",
    "template += '\\n\\nHTML STRING: \"<orq>Ability to need some one who has strong PCF expertise , eCommerce domain expertise and LS industry expertise'\n",
    "template += ' ...specially LS Distribution .</orq>\"\\n=========\\n'\n",
    "template += 'MINIMUM REQUIREMENT:\\n'\n",
    "template += \"We are looking for a candidate with strong PCF expertise, eCommerce domain expertise, and LS industry expertise. Especially LS Distribution.\"\n",
    "\n",
    "# Suggested and fed back in\n",
    "# template += '\\n\\nHTML STRING: \"<orq>Ability to lead</orq>\"\\n=========\\n'\n",
    "# template += 'MINIMUM REQUIREMENT:\\n'\n",
    "# template += \"Leadership skills are required.\"\n",
    "# template += '\\n\\nHTML STRING: \"<orq>Ability to jIRA).</orq>\"\\n=========\\n'\n",
    "# template += 'MINIMUM REQUIREMENT:\\n'\n",
    "# template += \"Proficiency in JIRA is necessary.\"\n",
    "# template += '\\n\\nHTML STRING: \"<orq>Ability to elysian.</orq>\"\\n=========\\n'\n",
    "# template += 'MINIMUM REQUIREMENT:\\n'\n",
    "# template += \"Proficiency in Elysian is required.\"\n",
    "# template += '\\n\\nHTML STRING: \"<orq>Ability to pMO Lead</orq>\"\\n=========\\n'\n",
    "# template += 'MINIMUM REQUIREMENT:\\n'\n",
    "# template += \"Leadership skills in project management are necessary.\"\n",
    "# template += '\\n\\nHTML STRING: \"<orq>Ability to pVC Lead</orq>\"\\n=========\\n'\n",
    "# template += 'MINIMUM REQUIREMENT:\\n'\n",
    "# template += \"Leadership skills in venture capital are necessary.\"\n",
    "# template += '\\n\\nHTML STRING: \"<orq>Ability to qA Director</orq>\"\\n=========\\n'\n",
    "# template += 'MINIMUM REQUIREMENT:\\n'\n",
    "# template += \"Experience as a QA Director is required.\"\n",
    "# template += '\\n\\nHTML STRING: \"<orq>Ability to tax reports</orq>\"\\n=========\\n'\n",
    "# template += 'MINIMUM REQUIREMENT:\\n'\n",
    "# template += \"Knowledge of tax reports is required.\"\n",
    "# template += '\\n\\nHTML STRING: \"<orq>Ability to pMO Analyst</orq>\"\\n=========\\n'\n",
    "# template += 'MINIMUM REQUIREMENT:\\n'\n",
    "# template += \"Experience as a PMO Analyst is required.\"\n",
    "# template += '\\n\\nHTML STRING: \"<orq>Ability to team Support</orq>\"\\n=========\\n'\n",
    "# template += 'MINIMUM REQUIREMENT:\\n'\n",
    "# template += \"Ability to provide team support is required.\"\n",
    "# template += '\\n\\nHTML STRING: \"<orq>Ability to data Synapse</orq>\"\\n=========\\n'\n",
    "# template += 'MINIMUM REQUIREMENT:\\n'\n",
    "# template += \"Experience working with Data Synapse is required.\"\n",
    "template += '\\n\\nHTML STRING: \"<orq>Ability to data Support</orq>\"\\n=========\\n'\n",
    "template += 'MINIMUM REQUIREMENT:\\n'\n",
    "template += \"Skills in data support are required.\"\n",
    "template += '\\n\\nHTML STRING: \"<orq>Ability to oCM Advisory</orq>\"\\n=========\\n'\n",
    "template += 'MINIMUM REQUIREMENT:\\n'\n",
    "template += \"Experience in OCM Advisory is required.\"\n",
    "template += '\\n\\nHTML STRING: \"<orq>Ability to training Lead</orq>\"\\n=========\\n'\n",
    "template += 'MINIMUM REQUIREMENT:\\n'\n",
    "template += \"Leadership in training is required.\"\n",
    "template += '\\n\\nHTML STRING: \"<orq>Ability to security Lead</orq>\"\\n=========\\n'\n",
    "template += 'MINIMUM REQUIREMENT:\\n'\n",
    "template += \"Experience as a Security Lead is required.\"\n",
    "template += '\\n\\nHTML STRING: \"<orq>Ability to interact with business and works with dev team to incorporate new features.</orq>\"\\n=========\\n'\n",
    "template += 'MINIMUM REQUIREMENT:\\n'\n",
    "template += \"A candidate who is able to interact with business and works with dev team to incorporate new features is required.\"\n",
    "template += '\\n\\nHTML STRING: \"<orq>Ability to work with other architects to model, design, and lead the data modeling, application, interface'\n",
    "template += ' and database activities.</orq>\"\\n=========\\n'\n",
    "template += 'MINIMUM REQUIREMENT:\\n'\n",
    "template += \"We are looking for a Data Architect who can work with other architects to model, design, and lead the data modeling, application,\"\n",
    "template += \" interface and database activities.\"\n",
    "template += '\\n\\nHTML STRING: \"<orq>Ability to resource must understand end-to-end Treasury process dependencies, integration with other'\n",
    "template += ' Finance and cross-team modules.</orq>\"\\n=========\\n'\n",
    "template += 'MINIMUM REQUIREMENT:\\n'\n",
    "template += \"We are looking for a resource who understands end-to-end Treasury process dependencies, integration with other Finance and cross-team modules.\"\n",
    "template += '\\n\\nHTML STRING: \"<orq>Ability to responsible for development of required custom solution on top of out-of-box ORMB features to suit'\n",
    "template += ' client business needs.</orq>\"\\n=========\\n'\n",
    "template += 'MINIMUM REQUIREMENT:\\n'\n",
    "template += \"We are looking for a candidate who is responsible for development of required custom solution on top of out-of-box ORMB features to suit\"\n",
    "template += \" client business needs.\"\n",
    "template += '\\n\\nHTML STRING: \"<orq>Ability to as a Developer, these will be your key responsibilities: Coding and developing by applying and'\n",
    "template += ' design/code best practices.</orq>\"\\n=========\\n'\n",
    "template += 'MINIMUM REQUIREMENT:\\n'\n",
    "template += \"We are looking for a Developer who can code and develop by applying and design/code best practices.\"\n",
    "template += '\\n\\nHTML STRING: \"<orq>Ability to required Skills: SuccessFactors Compensation certification, Workstream Lead of 3+ SuccessFactors'\n",
    "template += ' Compensation implementations</orq>\"\\n=========\\n'\n",
    "template += 'MINIMUM REQUIREMENT:\\n'\n",
    "template += \"We are looking for a candidate with SuccessFactors Compensation certification and Workstream Lead of 3+ SuccessFactors Compensation implementations.\"\n",
    "\n",
    "template += \"\\n\\nHTML STRING: {html_str}\\n=========\\nMINIMUM REQUIREMENT:\"\n",
    "prompt = PromptTemplate(\n",
    "    input_variables=['html_str'],\n",
    "    template=template,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# llm = OpenAI(model_name='text-davinci-003', temperature=1.0, max_retries=1)\n",
    "# What are the various strings (names of llm models that have\n",
    "# been pre-trained and made available within the `langchain` package)\n",
    "# that can be used as the input to the `model=` parameter of `langchain.llms.Cohere`?\n",
    "llm = Cohere(model='command-xlarge-nightly', temperature=0)\n",
    "chain = LLMChain(llm=llm, prompt=prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c4e7547a55cf4a948dea5b6c7f30b5a5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1469 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'NameError'> error: name 'CohereConnectionError' is not defined\n",
      "\n",
      "template += '\\n\\nHTML STRING: \"<orq>Ability to this resource will work with client to manage the development and support of the Risk adjustment unified data repository application.</orq>\"\\n=========\\n'\n",
      "template += 'MINIMUM REQUIREMENT:\\n'\n",
      "mask_series = (ability_to_df.navigable_parent == '<orq>Ability to this resource will work with client to manage the development and support of the Risk adjustment unified data repository application.</orq>')\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from tqdm.notebook import tqdm\n",
    "from cohere.error import CohereAPIError\n",
    "from ratelimit import limits, sleep_and_retry\n",
    "from IPython.display import clear_output\n",
    "\n",
    "ability_to_df = s.load_object('ability_to_df')\n",
    "mask_series = ability_to_df.llm_suggestion.isnull()\n",
    "\n",
    "# Define the Trial key rate limit - 5 API calls / minute\n",
    "@sleep_and_retry\n",
    "@limits(calls=5, period=60)\n",
    "def get_suggestion(chain, navigable_parent):\n",
    "    llm_suggestion = chain.run(navigable_parent).strip()\n",
    "    \n",
    "    return llm_suggestion\n",
    "\n",
    "for (row_index, row_series) in tqdm(ability_to_df[mask_series].iterrows(), total=ability_to_df[mask_series].shape[0]):\n",
    "    navigable_parent = row_series.navigable_parent\n",
    "    try:\n",
    "        llm_suggestion = get_suggestion(chain, navigable_parent)\n",
    "        time.sleep(1) # Sleep for 1 second between each call\n",
    "    except RateLimitError:\n",
    "        print(\"You've already spent 10 bucks on this; your wife will be pissed\")\n",
    "        break\n",
    "    except CohereAPIError as e:\n",
    "        message_str = str(e).strip()\n",
    "        if 'too many tokens' in message_str:\n",
    "            print(message_str)\n",
    "            break\n",
    "        time.sleep(60)\n",
    "        llm_suggestion = get_suggestion(chain, navigable_parent)\n",
    "        time.sleep(1) # Sleep for 1 second between each call\n",
    "    except Exception as e:\n",
    "        print(f'{e.__class__} error: {str(e).strip()}')\n",
    "        print()\n",
    "        print(f\"\"\"template += '\\\\n\\\\nHTML STRING: \"{navigable_parent}\"\\\\n=========\\\\n'\"\"\")\n",
    "        print(\"\"\"template += 'MINIMUM REQUIREMENT:\\\\n'\"\"\")\n",
    "        # print(f'template += \"{llm_suggestion}\"')\n",
    "        print(f\"mask_series = (ability_to_df.navigable_parent == '{navigable_parent}')\")\n",
    "        # print(f\"ability_to_df.loc[mask_series, 'llm_suggestion'] = '<orq>{llm_suggestion}</orq>'\")\n",
    "        break\n",
    "    score = 0.0\n",
    "    for scu_obj in [slrcu, scrfcu, ssgdcu]:\n",
    "        score = max(score, scu_obj.pos_predict_percent_fit_dict['O-RQ'](llm_suggestion))\n",
    "    if score < 0.38576917248603376:\n",
    "        clear_output(wait=True)\n",
    "        print(score)\n",
    "        print(f\"\"\"template += '\\\\n\\\\nHTML STRING: \"{navigable_parent}\"\\\\n=========\\\\n'\"\"\")\n",
    "        print(\"\"\"template += 'MINIMUM REQUIREMENT:\\\\n'\"\"\")\n",
    "        print(f'template += \"{llm_suggestion}\"')\n",
    "        print(f\"mask_series = (ability_to_df.navigable_parent == '{navigable_parent}')\")\n",
    "        print(f\"ability_to_df.loc[mask_series, 'llm_suggestion'] = '<opq>{llm_suggestion}</opq>'\")\n",
    "        print('s.store_objects(ability_to_df=ability_to_df, verbose=True)')\n",
    "        break\n",
    "    else:\n",
    "        ability_to_df.loc[row_index, 'llm_suggestion'] = '<orq>' + llm_suggestion + '</orq>'\n",
    "        s.store_objects(ability_to_df=ability_to_df, verbose=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "llm_suggestion = get_suggestion(chain, navigable_parent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'We are looking for a resource who will work with the client to manage the development and support of the Risk adjustment unified data repository application.'"
      ]
     },
     "execution_count": 135,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "llm_suggestion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6753\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print(len(prompt.format(html_str=navigable_parent)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0013165224780264965, 0.03767744998335852, 0.37688375253859563, "
     ]
    }
   ],
   "source": [
    "\n",
    "for scu_obj in [slrcu, scrfcu, ssgdcu]:\n",
    "    print(scu_obj.pos_predict_percent_fit_dict['O-RQ'](llm_suggestion), end=', ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Go ahead and update what you can in the database\n",
    "mask_series = ~ability_to_df.llm_suggestion.isnull() & (ability_to_df.pos_symbol == 'O-RQ')\n",
    "for row_index, row_series in ability_to_df[mask_series].iterrows():\n",
    "    old_child_str = row_series.navigable_parent\n",
    "    new_child_str = row_series.llm_suggestion\n",
    "    def do_cypher_tx(tx, old_child_str, new_child_str):\n",
    "        cypher_str = '''\n",
    "            MATCH (np:NavigableParents)\n",
    "            WHERE np.navigable_parent = $old_child_str\n",
    "            SET np.navigable_parent = $new_child_str;'''\n",
    "        tx.run(query=cypher_str, parameters={'old_child_str': old_child_str, 'new_child_str': new_child_str})\n",
    "    with cu.driver.session() as session: session.write_transaction(do_cypher_tx, old_child_str=navigable_parent, new_child_str=new_child_str)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "----"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 353
    },
    "id": "5jGrF82Yghg6",
    "outputId": "493116d6-19b0-4028-d051-db1b89a3005d"
   },
   "outputs": [],
   "source": [
    "\n",
    "print(llm('Tell me a joke'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "PfXQp12lhO87"
   },
   "outputs": [],
   "source": [
    "\n",
    "template = '''\n",
    "I want you to act as a classical music historian that knows the history of a composition when given the name of it.\n",
    "\n",
    "Here are some examples of historical descriptions:\n",
    "\n",
    "The Symphony No. 3 in D minor by Gustav Mahler was written in sketch beginning in 1893, composed primarily in 1895,'''\n",
    "template += \" and took final form in 1896. Consisting of six movements, it is Mahler's longest composition and is the\"\n",
    "template += ' longest symphony in the standard repertoire, with a typical performance lasting around 95 to 110 minutes.'\n",
    "template += ' It was voted one of the ten greatest symphonies of all time in a survey of conductors carried out by'\n",
    "template += ''' the BBC Music Magazine.\n",
    "\n",
    "The answer should detailed as if written by an expert.\n",
    "\n",
    "What is the history of {composition}?\n",
    "'''\n",
    "\n",
    "prompt = PromptTemplate(\n",
    "    input_variables=['composition'],\n",
    "    template=template,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "yoG_9069Tcgx"
   },
   "outputs": [],
   "source": [
    "\n",
    "print(prompt.format(composition='colorful socks'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "XGu-G9_wtw-C"
   },
   "outputs": [],
   "source": [
    "\n",
    "chain = LLMChain(llm=llm, prompt=prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "egFoGA4MTkO8"
   },
   "outputs": [],
   "source": [
    "\n",
    "# Run the chain only specifying the input variable\n",
    "print(chain.run('Alpine Symphony'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "LQX1bzJ3TlL3"
   },
   "outputs": [],
   "source": [
    "\n",
    "llm = OpenAI(temperature=0.9)\n",
    "prompt = PromptTemplate(\n",
    "    input_variables=['product'],\n",
    "    template='What is a good name for a company that makes {product}?',\n",
    ")\n",
    "chain = LLMChain(llm=llm, prompt=prompt)\n",
    "print(chain.run('colorful socks').strip())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from langchain.agents import load_tools\n",
    "from langchain.agents import initialize_agent\n",
    "\n",
    "# First, let's load the language model we're going to use to control the agent\n",
    "llm = OpenAI(temperature=0)\n",
    "\n",
    "# Next, let's load some tools to use. Note that the `llm-math` tool uses an LLM, so we need to pass that in.\n",
    "tools = load_tools(['serpapi', 'llm-math'], llm=llm)\n",
    "\n",
    "# Finally, let's initialize an agent with the tools, the language model, and the type of agent we want to use.\n",
    "agent = initialize_agent(tools, llm, agent='zero-shot-react-description', verbose=True)\n",
    "\n",
    "# Now let's test it out!\n",
    "agent.run('What was the high temperature in SF yesterday in Fahrenheit? What is that number raised to the .023 power?')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from langchain import ConversationChain\n",
    "\n",
    "llm = OpenAI(temperature=0)\n",
    "conversation = ConversationChain(llm=llm, verbose=True)\n",
    "\n",
    "conversation.predict(input='Hi there!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "conversation.predict(input=\"I'm doing well! Just having a conversation with an AI.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.schema import (\n",
    "    AIMessage,\n",
    "    HumanMessage,\n",
    "    SystemMessage\n",
    ")\n",
    "\n",
    "chat = ChatOpenAI(temperature=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "chat([HumanMessage(content='Translate this sentence from English to French. I love programming.')])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "messages = [\n",
    "    SystemMessage(content='You are a helpful assistant that translates English to French.'),\n",
    "    HumanMessage(content='Translate this sentence from English to French. I love programming.')\n",
    "]\n",
    "chat(messages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "batch_messages = [\n",
    "    [\n",
    "        SystemMessage(content='You are a helpful assistant that translates English to French.'),\n",
    "        HumanMessage(content='Translate this sentence from English to French. I love programming.')\n",
    "    ],\n",
    "    [\n",
    "        SystemMessage(content='You are a helpful assistant that translates English to French.'),\n",
    "        HumanMessage(content='Translate this sentence from English to French. I love artificial intelligence.')\n",
    "    ],\n",
    "]\n",
    "result = chat.generate(batch_messages)\n",
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from langchain.prompts.chat import (\n",
    "    ChatPromptTemplate,\n",
    "    SystemMessagePromptTemplate,\n",
    "    HumanMessagePromptTemplate,\n",
    ")\n",
    "\n",
    "chat = ChatOpenAI(temperature=0)\n",
    "\n",
    "template = 'You are a helpful assistant that translates {input_language} to {output_language}.'\n",
    "system_message_prompt = SystemMessagePromptTemplate.from_template(template)\n",
    "human_template='{text}'\n",
    "human_message_prompt = HumanMessagePromptTemplate.from_template(human_template)\n",
    "\n",
    "chat_prompt = ChatPromptTemplate.from_messages([system_message_prompt, human_message_prompt])\n",
    "\n",
    "# Get a chat completion from the formatted messages\n",
    "chat(chat_prompt.format_prompt(input_language='English', output_language='Navajo', text='I love programming.').to_messages())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "chat = ChatOpenAI(temperature=0)\n",
    "\n",
    "template='You are a helpful assistant that translates {input_language} to {output_language}.'\n",
    "system_message_prompt = SystemMessagePromptTemplate.from_template(template)\n",
    "human_template='{text}'\n",
    "human_message_prompt = HumanMessagePromptTemplate.from_template(human_template)\n",
    "chat_prompt = ChatPromptTemplate.from_messages([system_message_prompt, human_message_prompt])\n",
    "\n",
    "chain = LLMChain(llm=chat, prompt=chat_prompt)\n",
    "chain.run(input_language='English', output_language='Navajo', text='I love programming.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# First, let's load the language model we're going to use to control the agent\n",
    "chat = ChatOpenAI(temperature=0)\n",
    "\n",
    "# Next, let's load some tools to use. Note that the `llm-math` tool uses an LLM, so we need to pass that in.\n",
    "llm = OpenAI(temperature=0)\n",
    "tools = load_tools(['serpapi', 'llm-math'], llm=llm)\n",
    "\n",
    "\n",
    "# Finally, let's initialize an agent with the tools, the language model, and the type of agent we want to use.\n",
    "agent = initialize_agent(tools, chat, agent='chat-zero-shot-react-description', verbose=True)\n",
    "\n",
    "# Now let's test it out!\n",
    "agent.run(\"Who is Olivia Wilde's boyfriend? What is his current age raised to the 0.23 power?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from langchain.prompts import MessagesPlaceholder\n",
    "from langchain.chains import ConversationChain\n",
    "from langchain.memory import ConversationBufferMemory\n",
    "\n",
    "template_str = 'The following is a friendly conversation between a human and an AI. The AI is talkative and provides'\n",
    "template_str += ' lots of specific details from its context. If the AI does not know the answer to a question,'\n",
    "template_str += ' it truthfully says it does not know.'\n",
    "prompt = ChatPromptTemplate.from_messages([\n",
    "    SystemMessagePromptTemplate.from_template(template_str),\n",
    "    MessagesPlaceholder(variable_name='history'),\n",
    "    HumanMessagePromptTemplate.from_template('{input}')\n",
    "])\n",
    "\n",
    "llm = ChatOpenAI(temperature=0)\n",
    "memory = ConversationBufferMemory(return_messages=True)\n",
    "conversation = ConversationChain(memory=memory, prompt=prompt, llm=llm)\n",
    "\n",
    "conversation.predict(input='Hi there!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "conversation.predict(input=\"I'm doing well! Just having a conversation with an AI.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "conversation.predict(input='Tell me about yourself.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from langchain.utilities import PythonREPL\n",
    "\n",
    "python_repl = PythonREPL()\n",
    "print(python_repl.run(\"\"\"from langchain.utilities import PythonREPL;pr=PythonREPL();print(pr.run('print(1+1)'))\"\"\").strip())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import textwrap\n",
    "\n",
    "llm = OpenAI(model_name='text-davinci-003', temperature=0)\n",
    "print('\\n'.join(textwrap.wrap(llm('What is LangChain?').strip())))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "toplevel = 'https://langchain.readthedocs.io/en/latest'\n",
    "soup = wsu.get_page_soup(toplevel)\n",
    "print(soup.prettify())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "anchors_attrs = [anchor.attrs for anchor in soup.find_all('a')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "paths = []\n",
    "for anchor_attrs in anchors_attrs:\n",
    "    try:\n",
    "        classes = anchor_attrs['class']\n",
    "        link = anchor_attrs['href']\n",
    "        if 'reference' in classes:\n",
    "            if 'internal' in classes:\n",
    "                paths.append(link)\n",
    "            elif 'external' in classes:\n",
    "                if link.startswith('./'):\n",
    "                    paths.append(link[len('./'):])\n",
    "                else:\n",
    "                    pass # Not a link to docs\n",
    "            else:\n",
    "                pass # I didn't understand that reference\n",
    "        else:\n",
    "            pass # Not a reference\n",
    "    except KeyError:\n",
    "        print('No classes or href:', anchor_attrs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import requests\n",
    "\n",
    "paths = ['index.html'] + paths\n",
    "pages = []\n",
    "for path in paths:\n",
    "    try:\n",
    "        url = '/'.join([toplevel, path])\n",
    "        resp = requests.get(url)\n",
    "        resp.raise_for_status()\n",
    "    except Exception:\n",
    "        print(url)\n",
    "    finally:\n",
    "        pages.append({'content': resp.content, 'url': url})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import nltk\n",
    "\n",
    "nltk.download('punkt')\n",
    "nltk.download('averaged_perceptron_tagger')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from unstructured.partition.html import partition_html\n",
    "\n",
    "parsed_docs = [partition_html(text=page['content']) for page in pages]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "texts = []\n",
    "for doc in parsed_docs:\n",
    "    texts.append('\\n\\n'.join(\n",
    "        [str(el).strip() for el in doc]\n",
    "    ).strip().replace('\\\\n', '').replace(r'\\xe2\\x80\\x99', \"'\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "print(*textwrap.wrap(texts[0]), sep='\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "for page, text in zip(pages, texts):\n",
    "    page['text'] = text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "pages[0].keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import pandas as pd\n",
    "\n",
    "pd.DataFrame(pages).sample(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Chunk the text for use inside LLM prompts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from langchain.text_splitter import CharacterTextSplitter\n",
    "\n",
    "text_splitter = CharacterTextSplitter.from_tiktoken_encoder(\n",
    "    chunk_size=1024, chunk_overlap=128, separator=' '\n",
    ")\n",
    "documents = text_splitter.create_documents(\n",
    "    [page['text'] for page in pages], metadatas=[{'source': page['url']} for page in pages]\n",
    ")\n",
    "print(documents[0].metadata['source'], *textwrap.wrap(documents[0].page_content), sep='\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from langchain.embeddings.openai import OpenAIEmbeddings\n",
    "\n",
    "embeddings = OpenAIEmbeddings()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from langchain.vectorstores import FAISS\n",
    "\n",
    "docsearch = FAISS.from_documents(documents, embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from langchain.chains.qa_with_sources import load_qa_with_sources_chain\n",
    "\n",
    "chain = load_qa_with_sources_chain(llm, chain_type='stuff')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import langchain\n",
    "\n",
    "dir(langchain.chains)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from langchain.chains import sequential\n",
    "\n",
    "dir(sequential)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "query = 'What is LangChain?'\n",
    "docs = docsearch.similarity_search(query)\n",
    "result = chain({'input_documents': docs, 'question': query})\n",
    "text = '\\n'.join(textwrap.wrap(result['output_text']))\n",
    "text = '\\n\\nSOURCES:\\n'.join(map(lambda s: s.strip(), text.split('SOURCES:')))\n",
    "print(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "for doc in docs:\n",
    "    print()\n",
    "    print(doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "print(chain.llm_chain.prompt.template)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "assert scrfcu.pos_predict_percent_fit_dict['O-RQ']('*') == 0.0, \"You need to rerun this\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "t1 = time.time()\n",
    "cu.populate_pos_relationships(verbose=False)\n",
    "duration_str = humanize.precisedelta(time.time() - t1, minimum_unit='seconds', format='%0.0f')\n",
    "print(f'Parts-of-speech relationships repopulated in {duration_str}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "idx = split_orqs_df[mask_series].sort_values('orq_score').head(1).index[0]\n",
    "child_str = split_orqs_df.iloc[idx].tag_name + split_orqs_df.iloc[idx].split_str + split_orqs_df.iloc[idx].tag_name.replace('<', '</')\n",
    "child_str"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def do_cypher_tx(tx, navigable_parent, verbose=False):\n",
    "    cypher_str = '''\n",
    "        MATCH (np:NavigableParents)\n",
    "        WHERE np.navigable_parent CONTAINS $navigable_parent\n",
    "        SET\n",
    "            np.is_header = 'False',\n",
    "            np.is_task_scope = 'True',\n",
    "            np.is_minimum_qualification = 'False',\n",
    "            np.is_preferred_qualification = 'False',\n",
    "            np.is_educational_requirement = 'False',\n",
    "            np.is_legal_notification = 'False',\n",
    "            np.is_other = 'False',\n",
    "            np.is_corporate_scope = 'False',\n",
    "            np.is_job_title = 'False',\n",
    "            np.is_office_location = 'False',\n",
    "            np.is_job_duration = 'False',\n",
    "            np.is_supplemental_pay = 'False',\n",
    "            np.is_interview_procedure = 'False',\n",
    "            np.is_posting_date = 'False'\n",
    "        ''' + cu.return_everything_str + ';'\n",
    "    results_list = tx.run(query=cypher_str, parameters={'navigable_parent': navigable_parent})\n",
    "    \n",
    "    return [dict(record.items()) for record in results_list]\n",
    "with cu.driver.session() as session:\n",
    "    row_objs_list = session.write_transaction(do_cypher_tx, navigable_parent=child_str, verbose=False)\n",
    "row_objs_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "child_str"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "template = 'Given the following HTML strings, create a MINIMUM REQUIREMENTS section which parses out only the minimum requirements.\\n\\n'\n",
    "template += 'HTML STRING: \"'\n",
    "template += html_strs_list[0]\n",
    "template += '\"\\n=========\\n'\n",
    "template += 'MINIMUM REQUIREMENTS:'\n",
    "template += \"\\nMust possess a Bachelor's degree or foreign equivalent in Statistics, Mathematics, or a closely related field\"\n",
    "template += \"\\nAcademic coursework or professional experience must include descriptive and predictive statistics\"\n",
    "template += \"\\nAcademic coursework or professional experience must include experience working with statistical programming tools,\"\n",
    "template += \" such as R or Python\"\n",
    "template += \"\\nAcademic coursework or professional experience must include experience with Jupyter Notebooks, Matplotlib, Seaborn,\"\n",
    "template += \" Dask, Docker, or BigQuery\"\n",
    "template += \"\\nAcademic coursework or professional experience must include experience with Google Cloud Platform\"\n",
    "template += \"\\nAcademic coursework or professional experience must include development and deployment of code (version control and CI/CD)\"\n",
    "template += \"\\nBachelor's / Master's degree in computer science or equivalent\\n\"\n",
    "template += \"\\n2+ years of experience as a data analyst / engineer in a data focused environment and role\\n\"\n",
    "template += \"\\nKnowledge of best practices with data wrangling/mapping/cleansing,\"\n",
    "template += \" and other state-of-the-art relevant computational techniques and processes\"\n",
    "template += \"\\nStrong Python data science skill set\"\n",
    "template += \"\\nStrong SQL skills\"\n",
    "template += \"\\nStrong knowledge of engineering practices for development and deployment of code (version control + CI/CD)\"\n",
    "template += \"\\nSolid foundational knowledge of descriptive and predictive statistics\"\n",
    "template += \"\\nPassionate about data quality and able to clearly articulate its importance to internal and external partners in specific\"\n",
    "template += \" contexts\"\n",
    "template += \"\\nCollaborative self-starter\"\n",
    "template += \"\\nClear and strong written and verbal communication skills\"\n",
    "template += \"\\nClear and strong presentation skills\"\n",
    "template += \"\\nClear and strong documentation (technical and non-technical) skills\"\n",
    "template += \"\\nAbility to collaborate with other peers and senior / principal engineers\"\n",
    "template += \"\\nEffective time management based on priorities dictated by the business\"\n",
    "template += \"\\nSelf-assessment and situational assessment skills\"\n",
    "template += \"\\nService orientation toward customers (both internal and external)\"\n",
    "template += \"\\nResilient in the face of adversity\"\n",
    "template += \"\\nAbility to solve technical and business problems with help from other team members collaboratively\"\n",
    "template += \"\\n\\nHTML STRING: \" + html_strs_list[1] + \"\\n=========\\nMINIMUM REQUIREMENTS:\"\n",
    "template += \"\\nBachelors degree in Computer Science, Engineering (any) or a related field\"\n",
    "template += \"\\nOne (1) year of related work experience involving working across all phases of the Agile methodology\"\n",
    "template += \" project delivery lifecycle for designing, developing, testing and implementing big data solutions,\"\n",
    "template += \" data analytics, data visualization, data management, and OFSAA solutions for enterprise-level clients\"\n",
    "template += \"\\nOne (1) year of related work experience involving interacting directly with stakeholders in gathering\"\n",
    "template += \" functional and technical requirements, analyzing client requirements, and translating requirements\"\n",
    "template += \" into project designs to deliver solutions that satisfy business needs using Confluence wiki, Atlassian\"\n",
    "template += \" Jira, Git, Microsoft (MS) Powerpoint, Visio And Sharepoint\"\n",
    "template += \"\\nOne (1) year of related work experience involving utilizing Cloudera Hadoop, Confluent KAFKA, OFSAA,\"\n",
    "template += \" Arcadia Data, Unix, procedural language (PL/SQL) and SQL for the design and development of data solutions\"\n",
    "template += \"\\nOne (1) year of related work experience involving tracking and reporting on project and milestone\"\n",
    "template += \" deliverable status to ensure timely project delivery and communicating updates to internal and external\"\n",
    "template += \" stakeholders using Atlassian Jira, Confluence, Sharepoint, Powerpoint And Visio\"\n",
    "template += \"\\nOne (1) year of related work experience involving utilizing Bash shell, Intellij integrated development\"\n",
    "template += \" environment (IDE), Eclipse integrated development environment (IDE), Toad and SQL developer in the\"\n",
    "template += \" process of project-related responsibilities that include writing code in SQL and debugging code to solve\"\n",
    "template += \" defects\"\n",
    "template += \"\\nOne (1) year of related work experience involving analyzing and documenting current-state systems,\"\n",
    "template += \" processes, and environments to support project design of future-state systems, processes, and\"\n",
    "template += \" environments using Confluent KAFKA, Cloudera Hadoop, Ofsaa, Arcadia Data, Python, Unix, PL/SQL, and SQL\"\n",
    "template += \"\\nOne (1) year of related work experience involving supporting and performing unit testing to ensure\"\n",
    "template += \" successful design and implementation of project solutions\"\n",
    "template += \"\\nOne (1) year of related work experience involving designing and delivering user manuals and training\"\n",
    "template += \" for solutions to ensure successful implementation, adoption, and maintenance of data solutions in big\"\n",
    "template += \" data/Hive, OFSAA and using concepts in data warehouse, data lake, and data visualization\"\n",
    "template += \"\\nOne (1) year of related work experience involving supporting business development activities, including\"\n",
    "template += \" proposal development and responses to requests for proposals\"\n",
    "template += \"\\nOne (1) year of related work experience involving utilizing Confluent (KAFKA) KSQL/SQL, Tableau,\"\n",
    "template += \" Jupyter Notebook, Oracle and Apache Tools Impala, Spark, Livy, Zookeeper and Scoop for the completion\"\n",
    "template += \" of tasks that include writing code in query language to build data pipelines, creating test cases to\"\n",
    "template += \" evaluate products, data analysis, data migration, data modeling, data mapping and data reporting\"\n",
    "template += \"\\n\\nHTML STRING: \" + html_strs_list[2] + \"\\n=========\\nMINIMUM REQUIREMENTS:\"\n",
    "template += \"\\nStrong Python experience especially in data engineering/ML for ML based product development\"\n",
    "template += \"\\nKnowledge on different algorithms and corresponding Python packages e.g. fuzzy match of strings,\"\n",
    "template += \" graph algorithm to create connected lists, etc.\"\n",
    "template += \"\\nStrong coding skills in Pandas, Numpy\"\n",
    "template += \"\\nGood understanding of Pandas groupby, sort, merge, append, assignment, filters, map, apply\"\n",
    "template += \"\\nStrong knowledge in Python objects, tuples, list, dict, generators, lambda, etc.\"\n",
    "# template += \"\\n\\nHTML STRING: \" + html_strs_list[3] + \"\\n=========\\nMINIMUM REQUIREMENTS:\"\n",
    "# template += \"\\nEnthusiasm for troubleshooting, analyzing,and resolving complex problems\"\n",
    "# template += \"\\nDemonstrable strong problem-solving and communication skills\"\n",
    "# template += \"\\nPrepared to be an expert performance engineering resource on multiple initiatives of diverse scopes\"\n",
    "# template += \"\\nHands-on experience in designing, developing and implementing state of the art test simulation, analysis\"\n",
    "# template += \" tools and technologies to ensure platforms deliver industry-leading performance for high availability and\"\n",
    "# template += \" great performance for achieving targeting revenues to the clients\"\n",
    "# template += \"\\nExperience with load testing using JMeter, API, and Microservice testing using RestAssure\"\n",
    "# template += \"\\nDemonstrable ability to design and delivered performance Testing and Engineering frameworks for complex\"\n",
    "# template += \" enterprise applications.\"\n",
    "# template += \"\\nHas played an architect-level role in handling end-to-end (frontend, Middleware and backend systems)\"\n",
    "# template += \" performance tuning and optimization of the platform for at least 2 to 3 large engagements\"\n",
    "template += \"\\n\\nHTML STRING: {html_str}\\n=========\\nMINIMUM REQUIREMENTS:\"\n",
    "prompt = PromptTemplate(\n",
    "    input_variables=['html_str'],\n",
    "    template=template,\n",
    ")\n",
    "chain = LLMChain(llm=llm, prompt=prompt)\n",
    "print(f'\"{html_strs_list[4]}\"')\n",
    "reqs_list = chain.run(html_strs_list[4]).split('\\n')\n",
    "for req_str in reqs_list:\n",
    "    if req_str: print('\\n'+req_str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "print()\n",
    "for html_str in html_strs_list[3].split(' * '):\n",
    "    print(f'template += \"\\\\n{html_str}\"')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Job Hunting (Python 3.10.9)",
   "language": "python",
   "name": "jh_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
