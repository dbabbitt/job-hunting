{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1d6f630d-5e21-4fd8-b531-cebc13bd13d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pretty printing has been turned OFF\n"
     ]
    }
   ],
   "source": [
    "\n",
    "%pprint"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34dc834b-3cdb-4e27-bb03-b46805ddcc19",
   "metadata": {
    "tags": []
   },
   "source": [
    "\n",
    "---\n",
    "# Load needed libraries and functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a5611947-0c52-4db5-85c5-7a344b342ab3",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import sys\n",
    "\n",
    "# Insert at 1, 0 is the script path (or '' in REPL)\n",
    "sys.path.insert(1, '../py')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3ae528fa-fd9e-4beb-bb7c-c5c32e3ce4eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "%matplotlib inline\n",
    "from datetime import datetime\n",
    "from neo4j.exceptions import ServiceUnavailable\n",
    "import humanize\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "import winsound\n",
    "\n",
    "bin_count = 12\n",
    "duration = 1000  # milliseconds\n",
    "freq = 880  # Hz\n",
    "height_inches = 3.0\n",
    "width_inches = 18.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "40dc0ec7-17c9-4fee-8b18-026ace2bb185",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======== Neo4j/4.4.7 ========\n",
      "Utility libraries created in 2 seconds\n",
      "Last run on 2023-02-14 18:06:22.474526\n"
     ]
    }
   ],
   "source": [
    "\n",
    "t0 = time.time()\n",
    "try:\n",
    "    \n",
    "    # Get the Neo4j driver\n",
    "    from storage import Storage\n",
    "    s = Storage()\n",
    "\n",
    "    from ha_utils import HeaderAnalysis\n",
    "    ha = HeaderAnalysis(s=s, verbose=False)\n",
    "\n",
    "    from scrape_utils import WebScrapingUtilities\n",
    "    wsu = WebScrapingUtilities(s=s)\n",
    "    uri = wsu.secrets_json['neo4j']['connect_url']\n",
    "    user =  wsu.secrets_json['neo4j']['username']\n",
    "    password = wsu.secrets_json['neo4j']['password']\n",
    "\n",
    "    from cypher_utils import CypherUtilities\n",
    "    cu = CypherUtilities(uri=uri, user=user, password=password, driver=None, s=s, ha=ha)\n",
    "    \n",
    "    version_str = cu.driver.get_server_info().agent\n",
    "    print(f'======== {version_str} ========')\n",
    "    \n",
    "    import warnings\n",
    "    warnings.filterwarnings('ignore')\n",
    "except ServiceUnavailable as e:\n",
    "    print('You need to start Neo4j as a console')\n",
    "    raise\n",
    "except Exception as e:\n",
    "    print(f'{e.__class__}: {str(e).strip()}')\n",
    "duration_str = humanize.precisedelta(time.time() - t0, minimum_unit='seconds', format='%0.0f')\n",
    "# winsound.Beep(freq, duration)\n",
    "print(f'Utility libraries created in {duration_str}')\n",
    "print(f'Last run on {datetime.now()}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "12ff672c-08e9-44eb-be02-7931eec4ae0b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Labeled child strings found in 0 seconds\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Get all part-of-speech-labeled child string data\n",
    "t0 = time.time()\n",
    "def do_cypher_tx(tx, verbose=False):\n",
    "    cypher_str = \"\"\"\n",
    "        MATCH (pos:PartsOfSpeech)-[:SUMMARIZES]->(np:NavigableParents)\n",
    "        RETURN\n",
    "            np.navigable_parent AS navigable_parent,\n",
    "            pos.pos_id AS pos_id;\"\"\"\n",
    "    if verbose:\n",
    "        clear_output(wait=True)\n",
    "        print(cypher_str)\n",
    "    results_list = tx.run(query=cypher_str, parameters=None)\n",
    "    values_list = []\n",
    "    for record in results_list:\n",
    "        values_list.append(dict(record.items()))\n",
    "\n",
    "    return values_list\n",
    "with cu.driver.session() as session:\n",
    "    row_objs_list = session.read_transaction(do_cypher_tx, verbose=False)\n",
    "duration_str = humanize.precisedelta(time.time() - t0, minimum_unit='seconds', format='%0.0f')\n",
    "# winsound.Beep(freq, duration)\n",
    "print(f'Labeled child strings found in {duration_str}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d6b17b05-6ae2-424c-ac80-1eff22ac0290",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Packages loaded in 2 seconds\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# load packages\n",
    "t0 = time.time()\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "import torch.optim as optim\n",
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data import TensorDataset, random_split, DataLoader, RandomSampler, SequentialSampler\n",
    "import time, datetime, re, random, string\n",
    "from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score\n",
    "from collections import Counter\n",
    "from transformers import get_linear_schedule_with_warmup\n",
    "from itertools import repeat\n",
    "import optuna\n",
    "from optuna.pruners import SuccessiveHalvingPruner\n",
    "from optuna.samplers import TPESampler\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from torch.cuda.amp import autocast, GradScaler\n",
    "from transformers import get_linear_schedule_with_warmup, AdamW\n",
    "\n",
    "SEED = 15\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "duration_str = humanize.precisedelta(time.time() - t0, minimum_unit='seconds', format='%0.0f')\n",
    "# winsound.Beep(freq, duration)\n",
    "print(f'Packages loaded in {duration_str}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b68584f3-59f1-473d-91e6-90674ddb95d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tue Feb 14 18:06:26 2023       \n",
      "+-----------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 528.33       Driver Version: 528.33       CUDA Version: 12.0     |\n",
      "|-------------------------------+----------------------+----------------------+\n",
      "| GPU  Name            TCC/WDDM | Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
      "|                               |                      |               MIG M. |\n",
      "|===============================+======================+======================|\n",
      "|   0  NVIDIA GeForce ... WDDM  | 00000000:01:00.0 Off |                  N/A |\n",
      "| N/A   34C    P0    14W /  60W |      0MiB /  4096MiB |      0%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "                                                                               \n",
      "+-----------------------------------------------------------------------------+\n",
      "| Processes:                                                                  |\n",
      "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
      "|        ID   ID                                                   Usage      |\n",
      "|=============================================================================|\n",
      "|  No running processes found                                                 |\n",
      "+-----------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "\n",
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4d889bd8-930f-4b65-ae75-1723bb9a0870",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Tell pytorch to use cuda\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.cuda.amp.autocast(enabled=True)\n",
    "device = torch.device('cuda')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3806c03b-ce32-417d-a743-6177a6854985",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(10635, 2)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "df = pd.DataFrame(row_objs_list)\n",
    "df.pos_id = df.pos_id.map(lambda x: int(x))\n",
    "print(df.pos_id.unique().tolist())\n",
    "df.navigable_parent = df.navigable_parent.map(lambda x: ' '.join(ha.html_regex_tokenizer(x)))\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "185e1d6a-7953-4617-9904-b328179c2908",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Get counts of each char -- necessary for vocab\n",
    "counts = Counter(' '.join(df.navigable_parent.values.tolist()))\n",
    "\n",
    "# Build corpus vocab\n",
    "vocab = sorted(counts, key=counts.get, reverse=True)\n",
    "\n",
    "# Provide the vocab indicies\n",
    "vocab_to_int = {word: i for i, word in enumerate(counts, 1)}\n",
    "\n",
    "# Add padding\n",
    "vocab_to_int['PAD'] = 0\n",
    "\n",
    "# Vocab size\n",
    "vocab_size = len(vocab_to_int.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1c052cd0-5bd3-496e-af81-43c054757400",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10635, 74, 1014)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "max_char_length = 1014\n",
    "\n",
    "# Encode text\n",
    "def encode(text):\n",
    "    encoded = np.zeros([vocab_size, max_char_length], dtype='float32')\n",
    "    review = text.lower()[:max_char_length-1:-1]\n",
    "    i = 0\n",
    "    for letter in text:\n",
    "        if i >= max_char_length:\n",
    "            break\n",
    "        if letter in vocab_to_int:\n",
    "            encoded[vocab_to_int[letter]][i] = 1\n",
    "        i += 1\n",
    "    return encoded\n",
    "\n",
    "\n",
    "encoded_text = []\n",
    "for doc in df.navigable_parent.values:\n",
    "    encoded_text.append(encode(doc))\n",
    "\n",
    "encoded_text = np.asarray(encoded_text, dtype=np.float32)\n",
    "encoded_text.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "cb492493-552a-4237-9a57-6cf1e01af29c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10635, 1014, 74)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "encoded_text = encoded_text.reshape(len(df), max_char_length, vocab_size)\n",
    "encoded_text.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9f091fa-a66f-45e1-afa0-067121b80caf",
   "metadata": {},
   "source": [
    "\n",
    "# 5 Data Set and Data Loaders"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe44eb4f-1d37-47fb-96dc-a79443ae2ffb",
   "metadata": {},
   "source": [
    "\n",
    "Next we proceed with the usual task of preparing our `TensorDataset`, data loaders, a time helper function, and a weighted random sampler to help account for class imbalance during training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d4e2f5bc-bda7-4e1f-b5a2-c40cd5b3b02b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Prepare tensor data sets\n",
    "def prepare_dataset(padded_tokens, target):\n",
    "    \n",
    "    # Prepare target into np array\n",
    "    target = np.array(target.values, dtype=np.int64).reshape(-1, 1)\n",
    "    \n",
    "    # Create tensor data sets\n",
    "    tensor_df = TensorDataset(torch.from_numpy(padded_tokens), torch.from_numpy(target))\n",
    "    \n",
    "    # 80% of df\n",
    "    train_size = int(0.8 * len(df))\n",
    "    \n",
    "    # 20% of df\n",
    "    val_size = len(df) - train_size\n",
    "    \n",
    "    # 50% of validation\n",
    "    test_size = int(val_size - 0.5*val_size)\n",
    "    \n",
    "    # Divide the dataset by randomly selecting samples\n",
    "    train_dataset, val_dataset = random_split(tensor_df, [train_size, val_size])\n",
    "    \n",
    "    # Divide validation by randomly selecting samples\n",
    "    val_dataset, test_dataset = random_split(val_dataset, [test_size, test_size+1])\n",
    "\n",
    "    return train_dataset, val_dataset, test_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b62d4519-00dd-4643-9e0b-2c747733e237",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Create tensor data sets\n",
    "train_dataset, val_dataset, test_dataset = prepare_dataset(encoded_text, df.pos_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a10fddeb-0689-4ac2-9b55-0397698340dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Helper function to count target distribution inside tensor data sets\n",
    "def target_count(tensor_dataset):\n",
    "    \n",
    "    # Set empty count containers\n",
    "    for i in df.pos_id.unique():\n",
    "        exec(f'count{i} = 0')\n",
    "    \n",
    "    # Set total container to turn into torch tensor\n",
    "    total = []\n",
    "    \n",
    "    # For every item in the tensor data set\n",
    "    for i in tensor_dataset:\n",
    "        \n",
    "        # Add to the count of that container\n",
    "        exec(f'count{i[1].item()} += 1')\n",
    "    \n",
    "    for i in df.pos_id.unique():\n",
    "        total.append(eval(f'count{i}'))\n",
    "    \n",
    "    return torch.tensor(total)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "116904f5-3f76-4a12-95a0-3c4033176c55",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Prepare weighted sampling for imbalanced classification\n",
    "def create_sampler(target_tensor, tensor_dataset):\n",
    "    \n",
    "    # Generate class distributions [x, y]\n",
    "    class_sample_count = target_count(tensor_dataset)\n",
    "    \n",
    "    # Weight\n",
    "    weight = 1. / class_sample_count.float()\n",
    "    \n",
    "    # Produce weights for each observation in the data set\n",
    "    samples_weight = torch.tensor([weight[t[1]] for t in tensor_dataset])\n",
    "    \n",
    "    # Prepare sampler\n",
    "    sampler = torch.utils.data.WeightedRandomSampler(weights=samples_weight, num_samples=len(samples_weight), replacement=True)\n",
    "    \n",
    "    return sampler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "101cdf96-cdf7-491d-8f38-129f83a3fdf5",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Create samplers for each data set\n",
    "train_sampler = create_sampler(target_count(train_dataset), train_dataset)\n",
    "val_sampler = create_sampler(target_count(val_dataset), val_dataset)\n",
    "test_sampler = create_sampler(target_count(test_dataset), test_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "54ff820c-fa4d-4aee-a2c7-27c1050711d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Create DataLoaders with samplers\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=80, sampler=train_sampler, shuffle=False)\n",
    "valid_dataloader = DataLoader(val_dataset, batch_size=80, sampler=val_sampler, shuffle=False)\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=80, sampler=test_sampler, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "4114151c-f988-4f51-88ab-897141288d7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Time function\n",
    "def format_time(elapsed):\n",
    "    '''\n",
    "    Takes a time in seconds and returns a string hh:mm:ss\n",
    "    '''\n",
    "    \n",
    "    # Round to the nearest second\n",
    "    elapsed_rounded = int(round((elapsed)))\n",
    "    \n",
    "    # Format as hh:mm:ss\n",
    "    return str(datetime.timedelta(seconds=elapsed_rounded))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16369103-8485-4e60-9421-66d12eb9eb8c",
   "metadata": {},
   "source": [
    "\n",
    "# 6 Character-level CNN"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3adea7e3-141b-4cac-aa6a-11d7b793fcb2",
   "metadata": {},
   "source": [
    "\n",
    "Then we create our Character-level CNN class and specify its configuration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "8bcbfc0e-134d-4e65-89d9-68edbafe943b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class CharCNN(nn.Module):\n",
    "\n",
    "    def __init__(self, config, vocab_size=74):\n",
    "        super().__init__()\n",
    "        num_conv_filters = config.num_conv_filters\n",
    "        output_channel = config.output_channel\n",
    "        num_affine_neurons = config.num_affine_neurons\n",
    "        target_class = config.target_class\n",
    "        input_channel = vocab_size\n",
    "\n",
    "        self.conv1 = nn.Conv1d(input_channel, num_conv_filters, kernel_size=7)\n",
    "        self.conv2 = nn.Conv1d(num_conv_filters, num_conv_filters, kernel_size=7)\n",
    "        self.conv3 = nn.Conv1d(num_conv_filters, num_conv_filters, kernel_size=3)\n",
    "        self.conv4 = nn.Conv1d(num_conv_filters, num_conv_filters, kernel_size=3)\n",
    "        self.conv5 = nn.Conv1d(num_conv_filters, num_conv_filters, kernel_size=3)\n",
    "        self.conv6 = nn.Conv1d(num_conv_filters, output_channel, kernel_size=3)\n",
    "\n",
    "        self.dropout = nn.Dropout(config.dropout)\n",
    "        self.fc1 = nn.Linear(output_channel, num_affine_neurons)\n",
    "        self.fc2 = nn.Linear(num_affine_neurons, num_affine_neurons)\n",
    "        self.fc3 = nn.Linear(num_affine_neurons, target_class)\n",
    "\n",
    "    def forward(self, x, **kwargs):\n",
    "        x = x.transpose(1, 2).type(torch.cuda.FloatTensor)\n",
    "\n",
    "        x = F.max_pool1d(F.relu(self.conv1(x)), 3)\n",
    "        x = F.max_pool1d(F.relu(self.conv2(x)), 3)\n",
    "        x = F.relu(self.conv3(x))\n",
    "        x = F.relu(self.conv4(x))\n",
    "        x = F.relu(self.conv5(x))\n",
    "        x = F.relu(self.conv6(x))\n",
    "\n",
    "        x = F.max_pool1d(x, x.size(2)).squeeze(2)\n",
    "        x = F.relu(self.fc1(x.view(x.size(0), -1)))\n",
    "        x = self.dropout(x)\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.dropout(x)\n",
    "        \n",
    "        return self.fc3(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "301d1d0a-0573-4f24-8506-b6baf6e83dfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Instantiate model config -- set ex-post from optuna search\n",
    "class config:\n",
    "    def __init__(self):\n",
    "        config.num_conv_filters = 256\n",
    "        config.output_channel = 256\n",
    "        config.num_affine_neurons = 1024\n",
    "        config.target_class = 2\n",
    "        config.dropout = 0.4\n",
    "        \n",
    "        return None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "429fbb4d-08a8-4dec-b13a-9293b72a3344",
   "metadata": {},
   "source": [
    "\n",
    "# 7 Training Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f6c80f2-b949-4e75-9117-2b67482fd6ef",
   "metadata": {},
   "source": [
    "\n",
    "Next we create our usual training functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "a3a612bb-5f31-4a03-88db-5fe38b1ff804",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def train(model, dataloader, optimizer, criterion):\n",
    "\n",
    "    # Capture time\n",
    "    total_t0 = time.time()\n",
    "\n",
    "    # Perform one full pass over the training set\n",
    "    print('')\n",
    "    print('======== Epoch {:} / {:} ========'.format(epoch + 1, epochs))\n",
    "    print('Training...')\n",
    "\n",
    "    # Reset total loss for epoch\n",
    "    train_total_loss = 0\n",
    "    total_train_f1 = 0\n",
    "\n",
    "    # Put model into training mode\n",
    "    model.train()\n",
    "\n",
    "    # For each batch of training data...\n",
    "    for step, batch in enumerate(dataloader):\n",
    "\n",
    "        # Progress update every 40 batches\n",
    "        if step % 40 == 0 and not step == 0:\n",
    "\n",
    "            # Report progress\n",
    "            print('  Batch {:>5,}  of  {:>5,}.'.format(step, len(dataloader)))\n",
    "\n",
    "        # Unpack this training batch from our dataloader:\n",
    "        # As we unpack the batch, we'll also copy each tensor to the GPU\n",
    "        # `batch` contains two pytorch tensors:\n",
    "        #   [0]: input ids\n",
    "        #   [1]: labels\n",
    "        b_input_ids = batch[0].cuda()\n",
    "        b_labels = batch[1].cuda().long()\n",
    "\n",
    "        # Clear previously calculated gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        with autocast():\n",
    "            \n",
    "            # Forward propagation (evaluate model on training batch)\n",
    "            logits = model(b_input_ids)\n",
    "\n",
    "        # Calculate cross entropy loss\n",
    "        loss = criterion(logits.view(-1, 2), b_labels.view(-1))\n",
    "\n",
    "        # Sum the training loss over all batches for average loss at end\n",
    "        # loss is a tensor containing a single value\n",
    "        train_total_loss += loss.item()\n",
    "\n",
    "        # Scales losss: calls backward() on scaled loss to create scaled gradients\n",
    "        # Backward passes under autocast are not recommended\n",
    "        # Backward ops run in the same dtype autocast chose for corresponding forward ops\n",
    "        scaler.scale(loss).backward()\n",
    "\n",
    "        # scaler.step() first unscales the gradients of the optimizer's assigned params\n",
    "        # If these gradients do not contain infs or NaNs, optimizer.step() is then called,\n",
    "        # otherwise, optimizer.step() is skipped\n",
    "        scaler.step(optimizer)\n",
    "\n",
    "        # Updates the scale for next iteration\n",
    "        scaler.update()\n",
    "\n",
    "        # Update the learning rate\n",
    "        scheduler.step()\n",
    "\n",
    "        # Get preds\n",
    "        _, predicted = torch.max(logits, 1)\n",
    "\n",
    "        # Move logits and labels to CPU\n",
    "        predicted = predicted.detach().cpu().numpy()\n",
    "        y_true = b_labels.detach().cpu().numpy()\n",
    "\n",
    "        # Calculate f1\n",
    "        total_train_f1 += f1_score(predicted, y_true, average='weighted', labels=np.unique(predicted))\n",
    "\n",
    "    # Calculate the average loss over all of the batches\n",
    "    avg_train_loss = train_total_loss / len(dataloader)\n",
    "\n",
    "    # Calculate the average f1 over all of the batches\n",
    "    avg_train_f1 = total_train_f1 / len(dataloader)\n",
    "\n",
    "    # Record all statistics from this epoch.\n",
    "    training_stats.append(\n",
    "        {\n",
    "            'Train Loss': avg_train_loss,\n",
    "            'Train F1': avg_train_f1\n",
    "        }\n",
    "    )\n",
    "\n",
    "    # Training time end\n",
    "    training_time = format_time(time.time() - total_t0)\n",
    "\n",
    "    # Print result summaries\n",
    "    print('')\n",
    "    print('Summary Results')\n",
    "    print('epoch | trn loss | trn f1 | trn time ')\n",
    "    print(f'{epoch+1:5d} | {avg_train_loss:.5f} | {avg_train_f1:.5f} | {training_time:}')\n",
    "\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "a392ee9f-458c-41fd-904e-5f093b8ae5c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def validating(model, dataloader, criterion):\n",
    "\n",
    "    # Capture validation time\n",
    "    total_t0 = time.time()\n",
    "\n",
    "    # After the completion of each training epoch, measure our performance on\n",
    "    # our validation set\n",
    "    print('')\n",
    "    print('Running Validation...')\n",
    "\n",
    "    # Put the model in evaluation mode\n",
    "    model.eval()\n",
    "\n",
    "    # Track variables\n",
    "    total_valid_accuracy = 0\n",
    "    total_valid_loss = 0\n",
    "    total_valid_f1 = 0\n",
    "    total_valid_recall = 0\n",
    "    total_valid_precision = 0\n",
    "\n",
    "    # Evaluate data for one epoch\n",
    "    for batch in dataloader:\n",
    "\n",
    "        # Unpack batch from dataloader\n",
    "        b_input_ids = batch[0].cuda()\n",
    "        b_labels = batch[1].cuda().long()\n",
    "\n",
    "        # Tell pytorch not to bother calculating gradients\n",
    "        # as it's only necessary for training\n",
    "        with torch.no_grad():\n",
    "\n",
    "            # Forward propagation (evaluate model on training batch)\n",
    "            logits = model(b_input_ids)\n",
    "\n",
    "            # Calculate BCEWithLogitsLoss\n",
    "            loss = criterion(logits.view(-1, 2), b_labels.view(-1))\n",
    "\n",
    "            # Calculate preds\n",
    "            _, predicted = torch.max(logits, 1)\n",
    "\n",
    "        # Accumulate validation loss\n",
    "        total_valid_loss += loss.item()\n",
    "\n",
    "        # Move logits and labels to CPU\n",
    "        predicted = predicted.detach().cpu().numpy()\n",
    "        y_true = b_labels.detach().cpu().numpy()\n",
    "\n",
    "        # Calculate f1\n",
    "        total_valid_f1 += f1_score(predicted, y_true, average='weighted', labels=np.unique(predicted))\n",
    "\n",
    "        # Calculate accuracy\n",
    "        total_valid_accuracy += accuracy_score(predicted, y_true)\n",
    "\n",
    "        # Calculate precision\n",
    "        total_valid_precision += precision_score(predicted, y_true, average='weighted', labels=np.unique(predicted))\n",
    "\n",
    "        # Calculate recall\n",
    "        total_valid_recall += recall_score(predicted, y_true, average='weighted', labels=np.unique(predicted))\n",
    "\n",
    "    # Report final accuracy of validation run\n",
    "    avg_accuracy = total_valid_accuracy / len(dataloader)\n",
    "\n",
    "    # Report final f1 of validation run\n",
    "    global avg_val_f1\n",
    "    avg_val_f1 = total_valid_f1 / len(dataloader)\n",
    "\n",
    "    # Report final precision of validation run\n",
    "    avg_precision = total_valid_precision / len(dataloader)\n",
    "\n",
    "    # Report final recall of validation run\n",
    "    avg_recall = total_valid_recall / len(dataloader)\n",
    "\n",
    "    # Calculate the average loss over all of the batches.\n",
    "    avg_val_loss = total_valid_loss / len(dataloader)\n",
    "\n",
    "    # Record all statistics from this epoch.\n",
    "    valid_stats.append(\n",
    "        {\n",
    "            'Val Loss': avg_val_loss,\n",
    "            'Val Accur.': avg_accuracy,\n",
    "            'Val precision': avg_precision,\n",
    "            'Val recall': avg_recall,\n",
    "            'Val F1': avg_val_f1\n",
    "        }\n",
    "    )\n",
    "\n",
    "    # Capture end validation time\n",
    "    training_time = format_time(time.time() - total_t0)\n",
    "\n",
    "    # Print result summaries\n",
    "    print('')\n",
    "    print('summary results')\n",
    "    print('epoch | val loss | val f1 | val time')\n",
    "    print(f'{epoch+1:5d} | {avg_val_loss:.5f} | {avg_val_f1:.5f} | {training_time:}')\n",
    "\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "736cb5f5-efc6-4f24-84cd-8db7120a1da4",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def testing(model, dataloader, criterion):\n",
    "\n",
    "    print('')\n",
    "    print('Running Testing...')\n",
    "    \n",
    "    # Put the model in evaluation mode\n",
    "    model.eval()\n",
    "\n",
    "    # Track variables\n",
    "    total_test_accuracy = 0\n",
    "    total_test_loss = 0\n",
    "    total_test_f1 = 0\n",
    "    total_test_recall = 0\n",
    "    total_test_precision = 0\n",
    "\n",
    "    # Evaluate data for one epoch\n",
    "    for step, batch in enumerate(dataloader):\n",
    "        \n",
    "        # Progress update every 40 batches\n",
    "        if step % 40 == 0 and not step == 0:\n",
    "\n",
    "            # Report progress\n",
    "            print('  Batch {:>5,}  of  {:>5,}.'.format(step, len(dataloader)))\n",
    "\n",
    "        # Unpack batch from dataloader\n",
    "        b_input_ids = batch[0].cuda()\n",
    "        b_labels = batch[1].cuda().long()\n",
    "\n",
    "        # Tell pytorch not to bother calculating gradients\n",
    "        # only necessary for training\n",
    "        with torch.no_grad():\n",
    "\n",
    "            # Forward propagation (evaluate model on training batch)\n",
    "            logits = model(b_input_ids)\n",
    "\n",
    "            # Calculate cross entropy loss\n",
    "            loss = criterion(logits.view(-1, 2), b_labels.view(-1))\n",
    "\n",
    "            # Calculate preds\n",
    "            _, predicted = torch.max(logits, 1)\n",
    "\n",
    "            # Accumulate test loss\n",
    "            total_test_loss += loss.item()\n",
    "\n",
    "        # Move logits and labels to CPU\n",
    "        predicted = predicted.detach().cpu().numpy()\n",
    "        y_true = b_labels.detach().cpu().numpy()\n",
    "\n",
    "        # Calculate f1\n",
    "        total_test_f1 += f1_score(predicted, y_true, average='weighted', labels=np.unique(predicted))\n",
    "\n",
    "        # Calculate accuracy\n",
    "        total_test_accuracy += accuracy_score(predicted, y_true)\n",
    "\n",
    "        # Calculate precision\n",
    "        total_test_precision += precision_score(predicted, y_true, average='weighted', labels=np.unique(predicted))\n",
    "\n",
    "        # Calculate recall\n",
    "        total_test_recall += recall_score(predicted, y_true, average='weighted', labels=np.unique(predicted))\n",
    "\n",
    "    # Report final accuracy of test run\n",
    "    avg_accuracy = total_test_accuracy / len(dataloader)\n",
    "\n",
    "    # Report final f1 of test run\n",
    "    avg_test_f1 = total_test_f1 / len(dataloader)\n",
    "\n",
    "    # Report final precision of test run\n",
    "    avg_precision = total_test_precision / len(dataloader)\n",
    "\n",
    "    # Report final recall of test run\n",
    "    avg_recall = total_test_recall / len(dataloader)\n",
    "\n",
    "    # Calculate the average loss over all of the batches\n",
    "    avg_test_loss = total_test_loss / len(dataloader)\n",
    "\n",
    "    # Record all statistics from this epoch.\n",
    "    test_stats.append(\n",
    "        {\n",
    "            'Test Loss': avg_test_loss,\n",
    "            'Test Accur.': avg_accuracy,\n",
    "            'Test precision': avg_precision,\n",
    "            'Test recall': avg_recall,\n",
    "            'Test F1': avg_test_f1\n",
    "        }\n",
    "    )\n",
    "    \n",
    "    return None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cee5b96a-a3a0-454b-b2dc-8ac957c9f42c",
   "metadata": {},
   "source": [
    "\n",
    "Next, we instantiate a number of preparatory objects to help us train:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "9f2bd619-0fbd-47d1-b92e-008ad7344748",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "config1 = config()\n",
    "model = CharCNN(config1).cuda()\n",
    "\n",
    "# Set loss\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# Set number of epochs\n",
    "epochs = 7\n",
    "\n",
    "# Set optimizer\n",
    "optimizer = AdamW(model.parameters(), lr=0.0009978734977728082, weight_decay=0.5)\n",
    "\n",
    "# Set LR scheduler\n",
    "total_steps = len(train_dataloader) * epochs\n",
    "scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps=0, num_training_steps=total_steps)\n",
    "\n",
    "# Create gradient scaler for mixed precision\n",
    "scaler = GradScaler()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ac0ca88-ecd1-4094-8fbf-91fbd825758e",
   "metadata": {},
   "source": [
    "\n",
    "# 8 Training"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "341a45d4-09bf-4caf-80d5-24857498e187",
   "metadata": {},
   "source": [
    "\n",
    "Now we are ready to train our model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "c7f065ce-210a-49fb-ab4e-879fcae61142",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======== Epoch 1 / 7 ========\n",
      "Training...\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "CUDA error: device-side assert triggered\nCUDA kernel errors might be asynchronously reported at some other API call,so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_11000\\1641443730.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      9\u001b[0m     \u001b[1;31m# Train\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 10\u001b[1;33m     \u001b[0mtrain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrain_dataloader\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     11\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     12\u001b[0m     \u001b[1;31m# Validate\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_11000\\4251825962.py\u001b[0m in \u001b[0;36mtrain\u001b[1;34m(model, dataloader, optimizer, criterion)\u001b[0m\n\u001b[0;32m     46\u001b[0m         \u001b[1;31m# Sum the training loss over all batches for average loss at end\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     47\u001b[0m         \u001b[1;31m# loss is a tensor containing a single value\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 48\u001b[1;33m         \u001b[0mtrain_total_loss\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[0mloss\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     49\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     50\u001b[0m         \u001b[1;31m# Scales losss: calls backward() on scaled loss to create scaled gradients\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mRuntimeError\u001b[0m: CUDA error: device-side assert triggered\nCUDA kernel errors might be asynchronously reported at some other API call,so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1."
     ]
    }
   ],
   "source": [
    "\n",
    "# Create training result storage\n",
    "training_stats = []\n",
    "valid_stats = []\n",
    "best_valid_loss = float('inf')\n",
    "\n",
    "# For each epoch\n",
    "for epoch in range(epochs):\n",
    "    \n",
    "    # Train\n",
    "    train(model, train_dataloader, optimizer, criterion)\n",
    "    \n",
    "    # Validate\n",
    "    validating(model, valid_dataloader, criterion)\n",
    "    \n",
    "    # Check validation loss\n",
    "    if valid_stats[epoch]['Val Loss'] < best_valid_loss:\n",
    "        best_valid_loss = valid_stats[epoch]['Val Loss']\n",
    "        \n",
    "        # Save best model for use later\n",
    "        torch.save(model.state_dict(), '../saves/pt/char-cnn-model1.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25a9e264-b649-4c0e-88fb-4f578b8bf72a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
