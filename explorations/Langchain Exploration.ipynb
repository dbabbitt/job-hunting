{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pretty printing has been turned OFF\n"
     ]
    }
   ],
   "source": [
    "\n",
    "%pprint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "%matplotlib inline\n",
    "from IPython.display import clear_output, display\n",
    "from PIL import Image\n",
    "from cohere.error import CohereAPIError\n",
    "from datetime import datetime\n",
    "from langchain import PromptTemplate\n",
    "from langchain.chains import LLMChain\n",
    "from langchain.chains.question_answering import load_qa_chain\n",
    "from langchain.embeddings.cohere import CohereEmbeddings\n",
    "from langchain.llms import Cohere, OpenAI\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.text_splitter import CharacterTextSplitter, SpacyTextSplitter\n",
    "from langchain.vectorstores import Chroma\n",
    "from neo4j.exceptions import ServiceUnavailable\n",
    "from openai.error import RateLimitError\n",
    "from pandas import DataFrame\n",
    "from ratelimit import limits, sleep_and_retry\n",
    "from stability_sdk import client as stability_client\n",
    "from tqdm.notebook import tqdm\n",
    "import cohere\n",
    "import getpass\n",
    "import humanize\n",
    "import io\n",
    "import os\n",
    "import pandas as pd\n",
    "import random\n",
    "import re\n",
    "import stability_sdk.interfaces.gooseai.generation.generation_pb2 as stability_generation\n",
    "import sys\n",
    "import time\n",
    "import warnings\n",
    "import winsound\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "duration = 1000  # milliseconds\n",
    "freq = 880  # Hz\n",
    "\n",
    "# Insert at 1, 0 is the script path (or '' in REPL)\n",
    "sys.path.insert(1, '../py')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Get the Storage object\n",
    "from storage import Storage\n",
    "s = Storage(\n",
    "    data_folder_path=os.path.abspath('../data'),\n",
    "    saves_folder_path=os.path.abspath('../saves')\n",
    ")\n",
    "\n",
    "# Get the WebScrapingUtilities object\n",
    "from scrape_utils import WebScrapingUtilities\n",
    "wsu = WebScrapingUtilities(\n",
    "    s=s,\n",
    "    secrets_json_path=os.path.abspath('../data/secrets/jh_secrets.json')\n",
    ")\n",
    "\n",
    "os.environ['SERPAPI_API_KEY'] = wsu.secrets_json['SERPAPI_API_KEY']\n",
    "\n",
    "# Paste your API key here. Remember to not share it publicly\n",
    "co_key = wsu.secrets_json['Cohere_API_Key']\n",
    "os.environ['COHERE_API_KEY'] = co_key\n",
    "co = cohere.Client(co_key)\n",
    "\n",
    "# To get your API key, visit https://beta.dreamstudio.ai/membership\n",
    "os.environ['STABILITY_KEY'] = wsu.secrets_json['Dream_Studio_API_Key']\n",
    "stability_api = stability_client.StabilityInference(\n",
    "    key=os.environ['STABILITY_KEY'], \n",
    "    verbose=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I have 49,163 labeled parts of speech in here\n",
      "predict_single is now available\n",
      "Parts-of-speech logistic regression elements built in 9 seconds\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Get the HeaderAnalysis object\n",
    "from ha_utils import HeaderAnalysis\n",
    "ha = HeaderAnalysis(s=s, verbose=False)\n",
    "\n",
    "# Get the CypherUtilities object and Neo4j driver\n",
    "from cypher_utils import CypherUtilities\n",
    "uri = wsu.secrets_json['neo4j']['connect_url']\n",
    "user =  wsu.secrets_json['neo4j']['username']\n",
    "password = wsu.secrets_json['neo4j']['password']\n",
    "cu = CypherUtilities(\n",
    "    uri=uri, user=user, password=password, driver=None, s=s, ha=ha\n",
    ")\n",
    "\n",
    "# Get the SectionLRClassifierUtilities object\n",
    "from section_classifier_utils import SectionLRClassifierUtilities\n",
    "slrcu = SectionLRClassifierUtilities(ha=ha, cu=cu, verbose=False)\n",
    "\n",
    "# Check if the slrcu has built its parts-of-speech logistic regression elements\n",
    "t1 = time.time()\n",
    "if not hasattr(slrcu, 'pos_predict_percent_fit_dict'):\n",
    "    slrcu.build_pos_logistic_regression_elements(sampling_strategy_limit=None, verbose=True)\n",
    "if hasattr(slrcu, 'pos_predict_percent_fit_dict'):\n",
    "    print('predict_single is now available')\n",
    "else:\n",
    "    print('predict_single is not available')\n",
    "duration_str = humanize.precisedelta(time.time() - t1, minimum_unit='seconds', format='%0.0f')\n",
    "print(f'Parts-of-speech logistic regression elements built in {duration_str}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "predict_single is now available\n",
      "Parts-of-speech conditional random field elements built in 1 second\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Get the SectionCRFClassifierUtilities object\n",
    "from section_classifier_utils import SectionCRFClassifierUtilities\n",
    "scrfcu = SectionCRFClassifierUtilities(cu=cu, ha=ha, verbose=False)\n",
    "\n",
    "# Check if the scrfcu has built its parts-of-speech conditional random field elements\n",
    "t1 = time.time()\n",
    "if not hasattr(scrfcu, 'pos_symbol_crf'):\n",
    "    scrfcu.build_pos_conditional_random_field_elements(verbose=True)\n",
    "if hasattr(scrfcu, 'pos_predict_percent_fit_dict'):\n",
    "    print('predict_single is now available')\n",
    "else:\n",
    "    print('predict_single is not available')\n",
    "duration_str = humanize.precisedelta(time.time() - t1, minimum_unit='seconds', format='%0.0f')\n",
    "print(f'Parts-of-speech conditional random field elements built in {duration_str}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I have 49,163 labeled parts of speech in here\n",
      "predict_single is now available\n",
      "Parts-of-speech stochastic gradient descent elements built in 11 seconds\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Get the SectionSGDClassifierUtilities object\n",
    "from section_classifier_utils import SectionSGDClassifierUtilities\n",
    "ssgdcu = SectionSGDClassifierUtilities(ha=ha, cu=cu, verbose=False)\n",
    "\n",
    "# Check if the ssgdcu has built its parts-of-speech stochastic gradient decent elements\n",
    "t1 = time.time()\n",
    "if not hasattr(ssgdcu, 'pos_predict_percent_fit_dict'):\n",
    "    ssgdcu.build_pos_stochastic_gradient_descent_elements(sampling_strategy_limit=None, verbose=True)\n",
    "if hasattr(ssgdcu, 'pos_predict_percent_fit_dict'):\n",
    "    print('predict_single is now available')\n",
    "else:\n",
    "    print('predict_single is not available')\n",
    "duration_str = humanize.precisedelta(time.time() - t1, minimum_unit='seconds', format='%0.0f')\n",
    "print(f'Parts-of-speech stochastic gradient descent elements built in {duration_str}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def combine_adjacent(split_strs_list):\n",
    "    combined_list = []\n",
    "    for i, s in enumerate(split_strs_list):\n",
    "        if i == 0:\n",
    "            combined_list.append(s)\n",
    "        elif combined_list[-1].lower().endswith(' and'):\n",
    "            combined_list[-1] = combined_list[-1] + ' ' + s\n",
    "        else:\n",
    "            combined_list.append(s)\n",
    "    \n",
    "    return combined_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Define the Trial key rate limit - 5 API calls / minute\n",
    "@sleep_and_retry\n",
    "@limits(calls=5, period=60)\n",
    "def get_suggestion(chain, navigable_parent):\n",
    "    llm_suggestion = chain.run(navigable_parent).strip()\n",
    "    \n",
    "    return llm_suggestion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "----"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 353
    },
    "id": "5jGrF82Yghg6",
    "outputId": "493116d6-19b0-4028-d051-db1b89a3005d"
   },
   "outputs": [],
   "source": [
    "\n",
    "print(llm('Tell me a joke'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "PfXQp12lhO87"
   },
   "outputs": [],
   "source": [
    "\n",
    "template = '''\n",
    "I want you to act as a classical music historian that knows the history of a composition when given the name of it.\n",
    "\n",
    "Here are some examples of historical descriptions:\n",
    "\n",
    "The Symphony No. 3 in D minor by Gustav Mahler was written in sketch beginning in 1893, composed primarily in 1895,'''\n",
    "template += \" and took final form in 1896. Consisting of six movements, it is Mahler's longest composition and is the\"\n",
    "template += ' longest symphony in the standard repertoire, with a typical performance lasting around 95 to 110 minutes.'\n",
    "template += ' It was voted one of the ten greatest symphonies of all time in a survey of conductors carried out by'\n",
    "template += ''' the BBC Music Magazine.\n",
    "\n",
    "The answer should detailed as if written by an expert.\n",
    "\n",
    "What is the history of {composition}?\n",
    "'''\n",
    "\n",
    "prompt = PromptTemplate(\n",
    "    input_variables=['composition'],\n",
    "    template=template,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "yoG_9069Tcgx"
   },
   "outputs": [],
   "source": [
    "\n",
    "print(prompt.format(composition='colorful socks'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "XGu-G9_wtw-C"
   },
   "outputs": [],
   "source": [
    "\n",
    "chain = LLMChain(llm=llm, prompt=prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "egFoGA4MTkO8"
   },
   "outputs": [],
   "source": [
    "\n",
    "# Run the chain only specifying the input variable\n",
    "print(chain.run('Alpine Symphony'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "LQX1bzJ3TlL3"
   },
   "outputs": [],
   "source": [
    "\n",
    "llm = OpenAI(temperature=0.9)\n",
    "prompt = PromptTemplate(\n",
    "    input_variables=['product'],\n",
    "    template='What is a good name for a company that makes {product}?',\n",
    ")\n",
    "chain = LLMChain(llm=llm, prompt=prompt)\n",
    "print(chain.run('colorful socks').strip())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from langchain.agents import load_tools\n",
    "from langchain.agents import initialize_agent\n",
    "\n",
    "# First, let's load the language model we're going to use to control the agent\n",
    "llm = OpenAI(temperature=0)\n",
    "\n",
    "# Next, let's load some tools to use. Note that the `llm-math` tool uses an LLM, so we need to pass that in.\n",
    "tools = load_tools(['serpapi', 'llm-math'], llm=llm)\n",
    "\n",
    "# Finally, let's initialize an agent with the tools, the language model, and the type of agent we want to use.\n",
    "agent = initialize_agent(tools, llm, agent='zero-shot-react-description', verbose=True)\n",
    "\n",
    "# Now let's test it out!\n",
    "agent.run('What was the high temperature in SF yesterday in Fahrenheit? What is that number raised to the .023 power?')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from langchain import ConversationChain\n",
    "\n",
    "llm = OpenAI(temperature=0)\n",
    "conversation = ConversationChain(llm=llm, verbose=True)\n",
    "\n",
    "conversation.predict(input='Hi there!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "conversation.predict(input=\"I'm doing well! Just having a conversation with an AI.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.schema import (\n",
    "    AIMessage,\n",
    "    HumanMessage,\n",
    "    SystemMessage\n",
    ")\n",
    "\n",
    "chat = ChatOpenAI(temperature=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "chat([HumanMessage(content='Translate this sentence from English to French. I love programming.')])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "messages = [\n",
    "    SystemMessage(content='You are a helpful assistant that translates English to French.'),\n",
    "    HumanMessage(content='Translate this sentence from English to French. I love programming.')\n",
    "]\n",
    "chat(messages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "batch_messages = [\n",
    "    [\n",
    "        SystemMessage(content='You are a helpful assistant that translates English to French.'),\n",
    "        HumanMessage(content='Translate this sentence from English to French. I love programming.')\n",
    "    ],\n",
    "    [\n",
    "        SystemMessage(content='You are a helpful assistant that translates English to French.'),\n",
    "        HumanMessage(content='Translate this sentence from English to French. I love artificial intelligence.')\n",
    "    ],\n",
    "]\n",
    "result = chat.generate(batch_messages)\n",
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from langchain.prompts.chat import (\n",
    "    ChatPromptTemplate,\n",
    "    SystemMessagePromptTemplate,\n",
    "    HumanMessagePromptTemplate,\n",
    ")\n",
    "\n",
    "chat = ChatOpenAI(temperature=0)\n",
    "\n",
    "template = 'You are a helpful assistant that translates {input_language} to {output_language}.'\n",
    "system_message_prompt = SystemMessagePromptTemplate.from_template(template)\n",
    "human_template='{text}'\n",
    "human_message_prompt = HumanMessagePromptTemplate.from_template(human_template)\n",
    "\n",
    "chat_prompt = ChatPromptTemplate.from_messages([system_message_prompt, human_message_prompt])\n",
    "\n",
    "# Get a chat completion from the formatted messages\n",
    "chat(chat_prompt.format_prompt(input_language='English', output_language='Navajo', text='I love programming.').to_messages())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "chat = ChatOpenAI(temperature=0)\n",
    "\n",
    "template='You are a helpful assistant that translates {input_language} to {output_language}.'\n",
    "system_message_prompt = SystemMessagePromptTemplate.from_template(template)\n",
    "human_template='{text}'\n",
    "human_message_prompt = HumanMessagePromptTemplate.from_template(human_template)\n",
    "chat_prompt = ChatPromptTemplate.from_messages([system_message_prompt, human_message_prompt])\n",
    "\n",
    "chain = LLMChain(llm=chat, prompt=chat_prompt)\n",
    "chain.run(input_language='English', output_language='Navajo', text='I love programming.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# First, let's load the language model we're going to use to control the agent\n",
    "chat = ChatOpenAI(temperature=0)\n",
    "\n",
    "# Next, let's load some tools to use. Note that the `llm-math` tool uses an LLM, so we need to pass that in.\n",
    "llm = OpenAI(temperature=0)\n",
    "tools = load_tools(['serpapi', 'llm-math'], llm=llm)\n",
    "\n",
    "\n",
    "# Finally, let's initialize an agent with the tools, the language model, and the type of agent we want to use.\n",
    "agent = initialize_agent(tools, chat, agent='chat-zero-shot-react-description', verbose=True)\n",
    "\n",
    "# Now let's test it out!\n",
    "agent.run(\"Who is Olivia Wilde's boyfriend? What is his current age raised to the 0.23 power?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from langchain.prompts import MessagesPlaceholder\n",
    "from langchain.chains import ConversationChain\n",
    "from langchain.memory import ConversationBufferMemory\n",
    "\n",
    "template_str = 'The following is a friendly conversation between a human and an AI. The AI is talkative and provides'\n",
    "template_str += ' lots of specific details from its context. If the AI does not know the answer to a question,'\n",
    "template_str += ' it truthfully says it does not know.'\n",
    "prompt = ChatPromptTemplate.from_messages([\n",
    "    SystemMessagePromptTemplate.from_template(template_str),\n",
    "    MessagesPlaceholder(variable_name='history'),\n",
    "    HumanMessagePromptTemplate.from_template('{input}')\n",
    "])\n",
    "\n",
    "llm = ChatOpenAI(temperature=0)\n",
    "memory = ConversationBufferMemory(return_messages=True)\n",
    "conversation = ConversationChain(memory=memory, prompt=prompt, llm=llm)\n",
    "\n",
    "conversation.predict(input='Hi there!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "conversation.predict(input=\"I'm doing well! Just having a conversation with an AI.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "conversation.predict(input='Tell me about yourself.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from langchain.utilities import PythonREPL\n",
    "\n",
    "python_repl = PythonREPL()\n",
    "print(python_repl.run(\"\"\"from langchain.utilities import PythonREPL;pr=PythonREPL();print(pr.run('print(1+1)'))\"\"\").strip())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import textwrap\n",
    "\n",
    "llm = OpenAI(model_name='text-davinci-003', temperature=0)\n",
    "print('\\n'.join(textwrap.wrap(llm('What is LangChain?').strip())))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "toplevel = 'https://langchain.readthedocs.io/en/latest'\n",
    "soup = wsu.get_page_soup(toplevel)\n",
    "print(soup.prettify())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "anchors_attrs = [anchor.attrs for anchor in soup.find_all('a')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "paths = []\n",
    "for anchor_attrs in anchors_attrs:\n",
    "    try:\n",
    "        classes = anchor_attrs['class']\n",
    "        link = anchor_attrs['href']\n",
    "        if 'reference' in classes:\n",
    "            if 'internal' in classes:\n",
    "                paths.append(link)\n",
    "            elif 'external' in classes:\n",
    "                if link.startswith('./'):\n",
    "                    paths.append(link[len('./'):])\n",
    "                else:\n",
    "                    pass # Not a link to docs\n",
    "            else:\n",
    "                pass # I didn't understand that reference\n",
    "        else:\n",
    "            pass # Not a reference\n",
    "    except KeyError:\n",
    "        print('No classes or href:', anchor_attrs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import requests\n",
    "\n",
    "paths = ['index.html'] + paths\n",
    "pages = []\n",
    "for path in paths:\n",
    "    try:\n",
    "        url = '/'.join([toplevel, path])\n",
    "        resp = requests.get(url)\n",
    "        resp.raise_for_status()\n",
    "    except Exception:\n",
    "        print(url)\n",
    "    finally:\n",
    "        pages.append({'content': resp.content, 'url': url})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import nltk\n",
    "\n",
    "nltk.download('punkt')\n",
    "nltk.download('averaged_perceptron_tagger')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from unstructured.partition.html import partition_html\n",
    "\n",
    "parsed_docs = [partition_html(text=page['content']) for page in pages]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "texts = []\n",
    "for doc in parsed_docs:\n",
    "    texts.append('\\n\\n'.join(\n",
    "        [str(el).strip() for el in doc]\n",
    "    ).strip().replace('\\\\n', '').replace(r'\\xe2\\x80\\x99', \"'\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "print(*textwrap.wrap(texts[0]), sep='\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "for page, text in zip(pages, texts):\n",
    "    page['text'] = text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "pages[0].keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import pandas as pd\n",
    "\n",
    "pd.DataFrame(pages).sample(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Chunk the text for use inside LLM prompts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from langchain.text_splitter import CharacterTextSplitter\n",
    "\n",
    "text_splitter = CharacterTextSplitter.from_tiktoken_encoder(\n",
    "    chunk_size=1024, chunk_overlap=128, separator=' '\n",
    ")\n",
    "documents = text_splitter.create_documents(\n",
    "    [page['text'] for page in pages], metadatas=[{'source': page['url']} for page in pages]\n",
    ")\n",
    "print(documents[0].metadata['source'], *textwrap.wrap(documents[0].page_content), sep='\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from langchain.embeddings.openai import OpenAIEmbeddings\n",
    "\n",
    "embeddings = OpenAIEmbeddings()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from langchain.vectorstores import FAISS\n",
    "\n",
    "docsearch = FAISS.from_documents(documents, embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from langchain.chains.qa_with_sources import load_qa_with_sources_chain\n",
    "\n",
    "chain = load_qa_with_sources_chain(llm, chain_type='stuff')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import langchain\n",
    "\n",
    "dir(langchain.chains)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from langchain.chains import sequential\n",
    "\n",
    "dir(sequential)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "query = 'What is LangChain?'\n",
    "docs = docsearch.similarity_search(query)\n",
    "result = chain({'input_documents': docs, 'question': query})\n",
    "text = '\\n'.join(textwrap.wrap(result['output_text']))\n",
    "text = '\\n\\nSOURCES:\\n'.join(map(lambda s: s.strip(), text.split('SOURCES:')))\n",
    "print(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "for doc in docs:\n",
    "    print()\n",
    "    print(doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "print(chain.llm_chain.prompt.template)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "assert scrfcu.pos_predict_percent_fit_dict['O-RQ']('*') == 0.0, \"You need to rerun this\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "t1 = time.time()\n",
    "cu.populate_pos_relationships(verbose=False)\n",
    "duration_str = humanize.precisedelta(time.time() - t1, minimum_unit='seconds', format='%0.0f')\n",
    "print(f'Parts-of-speech relationships repopulated in {duration_str}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "idx = split_orqs_df[mask_series].sort_values('orq_score').head(1).index[0]\n",
    "child_str = split_orqs_df.iloc[idx].tag_name + split_orqs_df.iloc[idx].split_str + split_orqs_df.iloc[idx].tag_name.replace('<', '</')\n",
    "child_str"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def do_cypher_tx(tx, navigable_parent, verbose=False):\n",
    "    cypher_str = '''\n",
    "        MATCH (np:NavigableParents)\n",
    "        WHERE np.navigable_parent CONTAINS $navigable_parent\n",
    "        SET\n",
    "            np.is_header = 'False',\n",
    "            np.is_task_scope = 'True',\n",
    "            np.is_minimum_qualification = 'False',\n",
    "            np.is_preferred_qualification = 'False',\n",
    "            np.is_educational_requirement = 'False',\n",
    "            np.is_legal_notification = 'False',\n",
    "            np.is_other = 'False',\n",
    "            np.is_corporate_scope = 'False',\n",
    "            np.is_job_title = 'False',\n",
    "            np.is_office_location = 'False',\n",
    "            np.is_job_duration = 'False',\n",
    "            np.is_supplemental_pay = 'False',\n",
    "            np.is_interview_procedure = 'False',\n",
    "            np.is_posting_date = 'False'\n",
    "        ''' + cu.return_everything_str + ';'\n",
    "    results_list = tx.run(query=cypher_str, parameters={'navigable_parent': navigable_parent})\n",
    "    \n",
    "    return [dict(record.items()) for record in results_list]\n",
    "with cu.driver.session() as session:\n",
    "    row_objs_list = session.write_transaction(do_cypher_tx, navigable_parent=child_str, verbose=False)\n",
    "row_objs_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "child_str"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "template = 'Given the following HTML strings, create a MINIMUM REQUIREMENTS section which parses out only the minimum requirements.\\n\\n'\n",
    "template += 'HTML STRING: \"'\n",
    "template += html_strs_list[0]\n",
    "template += '\"\\n=========\\n'\n",
    "template += 'MINIMUM REQUIREMENTS:'\n",
    "template += \"\\nMust possess a Bachelor's degree or foreign equivalent in Statistics, Mathematics, or a closely related field\"\n",
    "template += \"\\nAcademic coursework or professional experience must include descriptive and predictive statistics\"\n",
    "template += \"\\nAcademic coursework or professional experience must include experience working with statistical programming tools,\"\n",
    "template += \" such as R or Python\"\n",
    "template += \"\\nAcademic coursework or professional experience must include experience with Jupyter Notebooks, Matplotlib, Seaborn,\"\n",
    "template += \" Dask, Docker, or BigQuery\"\n",
    "template += \"\\nAcademic coursework or professional experience must include experience with Google Cloud Platform\"\n",
    "template += \"\\nAcademic coursework or professional experience must include development and deployment of code (version control and CI/CD)\"\n",
    "template += \"\\nBachelor's / Master's degree in computer science or equivalent\\n\"\n",
    "template += \"\\n2+ years of experience as a data analyst / engineer in a data focused environment and role\\n\"\n",
    "template += \"\\nKnowledge of best practices with data wrangling/mapping/cleansing,\"\n",
    "template += \" and other state-of-the-art relevant computational techniques and processes\"\n",
    "template += \"\\nStrong Python data science skill set\"\n",
    "template += \"\\nStrong SQL skills\"\n",
    "template += \"\\nStrong knowledge of engineering practices for development and deployment of code (version control + CI/CD)\"\n",
    "template += \"\\nSolid foundational knowledge of descriptive and predictive statistics\"\n",
    "template += \"\\nPassionate about data quality and able to clearly articulate its importance to internal and external partners in specific\"\n",
    "template += \" contexts\"\n",
    "template += \"\\nCollaborative self-starter\"\n",
    "template += \"\\nClear and strong written and verbal communication skills\"\n",
    "template += \"\\nClear and strong presentation skills\"\n",
    "template += \"\\nClear and strong documentation (technical and non-technical) skills\"\n",
    "template += \"\\nAbility to collaborate with other peers and senior / principal engineers\"\n",
    "template += \"\\nEffective time management based on priorities dictated by the business\"\n",
    "template += \"\\nSelf-assessment and situational assessment skills\"\n",
    "template += \"\\nService orientation toward customers (both internal and external)\"\n",
    "template += \"\\nResilient in the face of adversity\"\n",
    "template += \"\\nAbility to solve technical and business problems with help from other team members collaboratively\"\n",
    "template += \"\\n\\nHTML STRING: \" + html_strs_list[1] + \"\\n=========\\nMINIMUM REQUIREMENTS:\"\n",
    "template += \"\\nBachelors degree in Computer Science, Engineering (any) or a related field\"\n",
    "template += \"\\nOne (1) year of related work experience involving working across all phases of the Agile methodology\"\n",
    "template += \" project delivery lifecycle for designing, developing, testing and implementing big data solutions,\"\n",
    "template += \" data analytics, data visualization, data management, and OFSAA solutions for enterprise-level clients\"\n",
    "template += \"\\nOne (1) year of related work experience involving interacting directly with stakeholders in gathering\"\n",
    "template += \" functional and technical requirements, analyzing client requirements, and translating requirements\"\n",
    "template += \" into project designs to deliver solutions that satisfy business needs using Confluence wiki, Atlassian\"\n",
    "template += \" Jira, Git, Microsoft (MS) Powerpoint, Visio And Sharepoint\"\n",
    "template += \"\\nOne (1) year of related work experience involving utilizing Cloudera Hadoop, Confluent KAFKA, OFSAA,\"\n",
    "template += \" Arcadia Data, Unix, procedural language (PL/SQL) and SQL for the design and development of data solutions\"\n",
    "template += \"\\nOne (1) year of related work experience involving tracking and reporting on project and milestone\"\n",
    "template += \" deliverable status to ensure timely project delivery and communicating updates to internal and external\"\n",
    "template += \" stakeholders using Atlassian Jira, Confluence, Sharepoint, Powerpoint And Visio\"\n",
    "template += \"\\nOne (1) year of related work experience involving utilizing Bash shell, Intellij integrated development\"\n",
    "template += \" environment (IDE), Eclipse integrated development environment (IDE), Toad and SQL developer in the\"\n",
    "template += \" process of project-related responsibilities that include writing code in SQL and debugging code to solve\"\n",
    "template += \" defects\"\n",
    "template += \"\\nOne (1) year of related work experience involving analyzing and documenting current-state systems,\"\n",
    "template += \" processes, and environments to support project design of future-state systems, processes, and\"\n",
    "template += \" environments using Confluent KAFKA, Cloudera Hadoop, Ofsaa, Arcadia Data, Python, Unix, PL/SQL, and SQL\"\n",
    "template += \"\\nOne (1) year of related work experience involving supporting and performing unit testing to ensure\"\n",
    "template += \" successful design and implementation of project solutions\"\n",
    "template += \"\\nOne (1) year of related work experience involving designing and delivering user manuals and training\"\n",
    "template += \" for solutions to ensure successful implementation, adoption, and maintenance of data solutions in big\"\n",
    "template += \" data/Hive, OFSAA and using concepts in data warehouse, data lake, and data visualization\"\n",
    "template += \"\\nOne (1) year of related work experience involving supporting business development activities, including\"\n",
    "template += \" proposal development and responses to requests for proposals\"\n",
    "template += \"\\nOne (1) year of related work experience involving utilizing Confluent (KAFKA) KSQL/SQL, Tableau,\"\n",
    "template += \" Jupyter Notebook, Oracle and Apache Tools Impala, Spark, Livy, Zookeeper and Scoop for the completion\"\n",
    "template += \" of tasks that include writing code in query language to build data pipelines, creating test cases to\"\n",
    "template += \" evaluate products, data analysis, data migration, data modeling, data mapping and data reporting\"\n",
    "template += \"\\n\\nHTML STRING: \" + html_strs_list[2] + \"\\n=========\\nMINIMUM REQUIREMENTS:\"\n",
    "template += \"\\nStrong Python experience especially in data engineering/ML for ML based product development\"\n",
    "template += \"\\nKnowledge on different algorithms and corresponding Python packages e.g. fuzzy match of strings,\"\n",
    "template += \" graph algorithm to create connected lists, etc.\"\n",
    "template += \"\\nStrong coding skills in Pandas, Numpy\"\n",
    "template += \"\\nGood understanding of Pandas groupby, sort, merge, append, assignment, filters, map, apply\"\n",
    "template += \"\\nStrong knowledge in Python objects, tuples, list, dict, generators, lambda, etc.\"\n",
    "# template += \"\\n\\nHTML STRING: \" + html_strs_list[3] + \"\\n=========\\nMINIMUM REQUIREMENTS:\"\n",
    "# template += \"\\nEnthusiasm for troubleshooting, analyzing,and resolving complex problems\"\n",
    "# template += \"\\nDemonstrable strong problem-solving and communication skills\"\n",
    "# template += \"\\nPrepared to be an expert performance engineering resource on multiple initiatives of diverse scopes\"\n",
    "# template += \"\\nHands-on experience in designing, developing and implementing state of the art test simulation, analysis\"\n",
    "# template += \" tools and technologies to ensure platforms deliver industry-leading performance for high availability and\"\n",
    "# template += \" great performance for achieving targeting revenues to the clients\"\n",
    "# template += \"\\nExperience with load testing using JMeter, API, and Microservice testing using RestAssure\"\n",
    "# template += \"\\nDemonstrable ability to design and delivered performance Testing and Engineering frameworks for complex\"\n",
    "# template += \" enterprise applications.\"\n",
    "# template += \"\\nHas played an architect-level role in handling end-to-end (frontend, Middleware and backend systems)\"\n",
    "# template += \" performance tuning and optimization of the platform for at least 2 to 3 large engagements\"\n",
    "template += \"\\n\\nHTML STRING: {html_str}\\n=========\\nMINIMUM REQUIREMENTS:\"\n",
    "prompt = PromptTemplate(\n",
    "    input_variables=['html_str'],\n",
    "    template=template,\n",
    ")\n",
    "chain = LLMChain(llm=llm, prompt=prompt)\n",
    "print(f'\"{html_strs_list[4]}\"')\n",
    "reqs_list = chain.run(html_strs_list[4]).split('\\n')\n",
    "for req_str in reqs_list:\n",
    "    if req_str: print('\\n'+req_str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "print()\n",
    "for html_str in html_strs_list[3].split(' * '):\n",
    "    print(f'template += \"\\\\n{html_str}\"')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Job Hunting (Python 3.10.9)",
   "language": "python",
   "name": "jh_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
