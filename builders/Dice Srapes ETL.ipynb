{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pretty printing has been turned OFF\n"
     ]
    }
   ],
   "source": [
    "\n",
    "%pprint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import sys\n",
    "\n",
    "# Insert at 1, 0 is the script path (or '' in REPL)\n",
    "sys.path.insert(1, '../py')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from pandas import DataFrame\n",
    "\n",
    "from storage import Storage\n",
    "s = Storage()\n",
    "\n",
    "from ha_utils import HeaderAnalysis\n",
    "ha = HeaderAnalysis(verbose=False)\n",
    "\n",
    "from scrape_utils import WebScrapingUtilities\n",
    "wsu = WebScrapingUtilities(s=s)\n",
    "uri = wsu.secrets_json['neo4j']['connect_url']\n",
    "user =  wsu.secrets_json['neo4j']['username']\n",
    "password = wsu.secrets_json['neo4j']['password']\n",
    "\n",
    "from cypher_utils import CypherUtilities\n",
    "cu = CypherUtilities(uri=uri, user=user, password=password, driver=None, s=s, ha=ha)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from neo4j.exceptions import ServiceUnavailable\n",
    "\n",
    "try:\n",
    "    version_str = cu.driver.verify_connectivity()\n",
    "    \n",
    "    from hc_utils import HeaderCategories\n",
    "    hc = HeaderCategories(cu=cu, verbose=False)\n",
    "\n",
    "    from section_utils import SectionUtilities\n",
    "    su = SectionUtilities(s=s, ha=ha, cu=cu, verbose=False)\n",
    "\n",
    "    from lr_utils import LrUtilities\n",
    "    lru = LrUtilities(ha=ha, cu=cu, hc=hc, verbose=False)\n",
    "    lru.build_isheader_logistic_regression_elements()\n",
    "    lru.build_pos_logistic_regression_elements()\n",
    "\n",
    "    import warnings\n",
    "    warnings.filterwarnings('ignore')\n",
    "except ServiceUnavailable as e:\n",
    "    # print(str(e).strip())\n",
    "    raise ServiceUnavailable('You need to start Neo4j as a console')\n",
    "except Exception as e:\n",
    "    print(e.__class__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Last run on 2022-07-25 08:25:16.183121\n"
     ]
    }
   ],
   "source": [
    "\n",
    "%run ../load_magic/dataframes.py\n",
    "from datetime import datetime\n",
    "from urllib.parse import urlparse, parse_qs\n",
    "from urllib.error import HTTPError, URLError\n",
    "from IPython.display import clear_output\n",
    "import random\n",
    "import requests\n",
    "import time\n",
    "print(f'Last run on {datetime.now()}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "----"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "file_path = '../data/html/dice_email.html'\n",
    "page_soup = wsu.get_page_soup(file_path)\n",
    "tags_list = ['#jobDescriptionText', 'table', 'tbody', 'tr', 'td', 'div', 'div', 'div', 'div', 'div', 'div',\n",
    "             'div', 'div', 'div', 'table', 'tbody', 'tr:nth-child(1)', 'td', 'table',\n",
    "             'tbody', 'tr:nth-child(1)', 'td', 'a']\n",
    "css_selector = ' > '.join(tags_list)\n",
    "link_soups_list = page_soup.select(css_selector)\n",
    "len(link_soups_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "url_strs_list = []\n",
    "rows_list = []\n",
    "for link_soup in link_soups_list:\n",
    "    url_str = link_soup['href']\n",
    "    row_dict = {k: v[0] for k, v in parse_qs(urlparse(url_str).query).items()}\n",
    "    row_dict['url_str'] = url_str\n",
    "    rows_list.append(row_dict)\n",
    "df = pd.DataFrame(rows_list)\n",
    "url_strs_list = df.url_str.unique().tolist()\n",
    "len(url_strs_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Getting the FireFox driver\n"
     ]
    }
   ],
   "source": [
    "\n",
    "driver = wsu.get_driver()\n",
    "wsu.log_into_dice(driver, verbose=False)\n",
    "sorry_css = 'h1.pull-left'\n",
    "article_css = '#jobdescSec'\n",
    "job_title_css = '#jt'\n",
    "job_org_css = '#hiringOrganizationName'\n",
    "job_location_css = '.location'\n",
    "ascii_regex = re.compile('[^A-Za-z0-9]+')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving to ../saves/html\\DataiKu_Avista_Tech_Remote.html\n",
      "Saving to ../saves/html\\Data_Scientist_InfoSmart_Technologies_Inc_Remote_or_Atlanta_GA.html\n",
      "Saving to ../saves/html\\OPB_Data_Scientist_Codeforce_360_Remote_or_100_Remote_GA.html\n",
      "Saving to ../saves/html\\Senior_Data_Scientist_Esteem_IT_Remote.html\n",
      "Fileing 4 postings complete\n"
     ]
    }
   ],
   "source": [
    "\n",
    "files_list = []\n",
    "for url_str in url_strs_list:\n",
    "    wsu.driver_get_url(driver, url_str, verbose=False)\n",
    "    time.sleep(3)\n",
    "    tags_list = driver.find_elements_by_css_selector(sorry_css)\n",
    "    if not tags_list:\n",
    "        job_title_str = driver.find_elements_by_css_selector(job_title_css)[0].text\n",
    "        job_org_str = driver.find_elements_by_css_selector(job_org_css)[0].text\n",
    "        job_location_str = driver.find_elements_by_css_selector(job_location_css)[0].text\n",
    "        page_title = f'{job_title_str} {job_org_str} {job_location_str}'\n",
    "        file_name = ascii_regex.sub(' ', page_title).strip().replace(' ', '_')\n",
    "        file_name = f'{file_name}.html'\n",
    "        cu.ensure_filename(file_name, verbose=False)\n",
    "        cu.set_posting_url(file_name, url_str, verbose=False)\n",
    "        file_path = os.path.join(cu.SAVES_HTML_FOLDER, file_name)\n",
    "        if os.path.isfile(file_path):\n",
    "            file_name = datetime.now().strftime('%Y%m%d%H%M%S%f') + f'_{file_name}'\n",
    "        if not os.path.isfile(file_path):\n",
    "            with open(file_path, 'w', encoding=s.encoding_type) as f:\n",
    "                print(f'Saving to {file_path}')\n",
    "                f.write('<html><head><title>')\n",
    "                f.write(page_title)\n",
    "                f.write('</title></head><body><div id=\"jobDescriptionText\">')\n",
    "                web_obj = driver.find_elements_by_css_selector(article_css)[0]\n",
    "                article_str = web_obj.get_attribute('innerHTML').strip()\n",
    "                f.write(article_str)\n",
    "                f.write('</div></body></html>')\n",
    "            files_list.append(file_name)\n",
    "print(f'Fileing {len(files_list)} postings complete')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['DataiKu_Avista_Tech_Remote.html', 'Data_Scientist_InfoSmart_Technologies_Inc_Remote_or_Atlanta_GA.html', 'OPB_Data_Scientist_Codeforce_360_Remote_or_100_Remote_GA.html', 'Senior_Data_Scientist_Esteem_IT_Remote.html']"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "driver.close()\n",
    "cu.ensure_navigableparent('END', verbose=False)\n",
    "for file_name in files_list:\n",
    "    file_path = os.path.join(cu.SAVES_HTML_FOLDER, file_name)\n",
    "    if os.path.isfile(file_path):\n",
    "        page_soup = wsu.get_page_soup(file_path)\n",
    "        for div_soup in page_soup.find_all(name='div', attrs={'id': 'jobDescriptionText'}):\n",
    "            child_strs_list = ha.get_navigable_children(div_soup, [])\n",
    "            assert child_strs_list, f'Something is wrong with {file_name}'\n",
    "            cu.populate_from_child_strings(child_strs_list, file_name, verbose=False)\n",
    "files_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
