{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pretty printing has been turned OFF\n"
     ]
    }
   ],
   "source": [
    "\n",
    "%pprint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import sys\n",
    "\n",
    "# Insert at 1, 0 is the script path (or '' in REPL)\n",
    "sys.path.insert(1, '../py')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: total: 2 s\n",
      "Wall time: 2.66 s\n"
     ]
    }
   ],
   "source": [
    "\n",
    "%%time\n",
    "from scrape_utils import WebScrapingUtilities\n",
    "%run ../load_magic/storage.py\n",
    "from html_analysis import HeaderAnalysis\n",
    "from cypher_utils import CypherUtilities\n",
    "\n",
    "wsu = WebScrapingUtilities()\n",
    "uri = wsu.secrets_json['neo4j']['connect_url']\n",
    "user =  wsu.secrets_json['neo4j']['username']\n",
    "password = wsu.secrets_json['neo4j']['password']\n",
    "s = Storage()\n",
    "ha = HeaderAnalysis()\n",
    "cu = CypherUtilities(uri=uri, user=user, password=password, driver=None, s=s, ha=ha)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def update_remaining_child_strs_list_dictionary(child_strs_list_dict=None, navigable_parent_is_header_dict=None):\n",
    "    if navigable_parent_is_header_dict is None:\n",
    "        navigable_parent_is_header_dict = ha.NAVIGABLE_PARENT_IS_HEADER_DICT\n",
    "    if child_strs_list_dict is None:\n",
    "        files_list = os.listdir(ha.SAVES_HTML_FOLDER)\n",
    "        child_strs_list_dict = {}\n",
    "        for file_name in files_list:\n",
    "            child_strs_list = ha.get_child_strs_from_file(file_name)\n",
    "            if not len(child_strs_list):\n",
    "                os.remove(file_path)\n",
    "            child_strs_list = [child_str for child_str in child_strs_list if child_str not in navigable_parent_is_header_dict]\n",
    "            child_strs_list_dict[file_name] = child_strs_list\n",
    "        \n",
    "        return child_strs_list_dict\n",
    "    \n",
    "    for file_name, child_strs_list in child_strs_list_dict.items():\n",
    "        child_strs_list = [child_str for child_str in child_strs_list if child_str not in navigable_parent_is_header_dict]\n",
    "        if not len(child_strs_list):\n",
    "            child_strs_list = child_strs_list_dict.pop(file_name)\n",
    "            break\n",
    "        else:\n",
    "            child_strs_list_dict[file_name] = child_strs_list\n",
    "    \n",
    "    return child_strs_list_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "----\n",
    "'TASK_SCOPE_HEADERS_LIST', 'REQ_QUALS_HEADERS_LIST', 'PREFF_QUALS_HEADERS_LIST', 'LEGAL_NOTIFS_HEADERS_LIST', 'JOB_TITLE_HEADERS_LIST', 'OFFICE_LOC_HEADERS_LIST', 'JOB_DURATION_HEADERS_LIST', 'SUPP_PAY_HEADERS_LIST', 'EDUC_REQS_HEADERS_LIST', 'INTERV_PROC_HEADERS_LIST', 'CORP_SCOPE_HEADERS_LIST', 'POST_DATE_HEADERS_LIST', 'OTHER_HEADERS_LIST'"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "\n",
    "print(tag_str)\n",
    "ha.store_unique_list('REQ_QUALS_HEADERS_LIST', tag_str)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "\n",
    "for file_name in os.listdir(cu.SAVES_HTML_FOLDER):\n",
    "    cypher_str = f'''\n",
    "        MATCH (fn:FileNames {{file_name: \"{file_name}\"}})\n",
    "        WITH count(*) as count\n",
    "        CALL apoc.when (\n",
    "            count > 0,\n",
    "            'RETURN true AS bool',\n",
    "            'RETURN false AS bool',\n",
    "            {{count:count}}\n",
    "            ) YIELD value\n",
    "        RETURN value.bool AS filename_exists;'''\n",
    "    row_objs_list = cu.get_execution_results(cypher_str, verbose=False)\n",
    "    if not row_objs_list[0]['filename_exists']:"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "\n",
    "files_list = sorted([fn for fn in os.listdir(cu.SAVES_HTML_FOLDER) if fn.endswith('.html')])\n",
    "for file_name in files_list:\n",
    "    cypher_str = f'''MERGE (fn:FileNames {{file_name: \"{file_name}\"}})\n",
    "        RETURN fn.file_name_id'''\n",
    "    # print(cypher_str)\n",
    "    with cu.driver.session() as session:\n",
    "        row_objs_list = session.write_transaction(cu.do_cypher_tx, cypher_str)\n",
    "    file_name_id = row_objs_list[0]['fn.file_name_id']\n",
    "    if file_name_id is None:\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "----\n",
    "'TASK_SCOPE_HEADERS_LIST', 'REQ_QUALS_HEADERS_LIST', 'PREFF_QUALS_HEADERS_LIST', 'LEGAL_NOTIFS_HEADERS_LIST', 'JOB_TITLE_HEADERS_LIST', 'OFFICE_LOC_HEADERS_LIST', 'JOB_DURATION_HEADERS_LIST', 'SUPP_PAY_HEADERS_LIST', 'EDUC_REQS_HEADERS_LIST', 'INTERV_PROC_HEADERS_LIST', 'CORP_SCOPE_HEADERS_LIST', 'POST_DATE_HEADERS_LIST', 'OTHER_HEADERS_LIST'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from datetime import datetime\n",
    "from urllib.parse import urlparse, parse_qs\n",
    "%run ../load_magic/dataframes.py\n",
    "from html_analysis import ElementAnalysis\n",
    "\n",
    "ea = ElementAnalysis()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def generate_child_strs(verbose=False):\n",
    "    file_path = '../data/html/indeed_email.html'\n",
    "    page_soup = get_page_soup(file_path)\n",
    "    css_selector = 'body > table > tbody > tr > td > a > table > tbody > tr > td > a'\n",
    "    link_soups_list = page_soup.select(css_selector)\n",
    "    for link_soup in link_soups_list:\n",
    "        url_str = link_soup['href']\n",
    "        page_soup = get_page_soup(url_str)\n",
    "        page_title = page_soup.find('title').string.strip()\n",
    "        file_name = re.sub(r'\\W+', ' ', page_title).strip().replace(' ', '_')\n",
    "        jk = parse_qs(urlparse(url_str).query).get('jk', [''])[0]\n",
    "        if len(jk):\n",
    "            file_name = f'{jk}_{file_name}.html'\n",
    "        else:\n",
    "            # file_name = datetime.now().strftime('%Y%m%d%H%M%S%f') + f'_{file_name}.html'\n",
    "            file_name = f'{file_name}.html'\n",
    "        file_path = os.path.join(cu.SAVES_HTML_FOLDER, file_name)\n",
    "        if not os.path.isfile(file_path):\n",
    "            with open(file_path, 'w', encoding=s.encoding_type) as f:\n",
    "                if verbose:\n",
    "                    print(f'Saving to {file_path}')\n",
    "                f.write('<html><head><title>')\n",
    "                f.write(page_title)\n",
    "                f.write('</title></head><body>')\n",
    "                row_div_list = page_soup.find_all(name='div', attrs={'class': ['jobsearch-JobComponent-description']})\n",
    "                for div_soup in row_div_list:\n",
    "                    f.write(str(div_soup))\n",
    "                f.write('</body></html>')\n",
    "        file_name_id = cu.get_filename_id(file_name, verbose=verbose)\n",
    "        row_div_list = page_soup.find_all(name='div', id='jobDescriptionText')\n",
    "        for div_soup in row_div_list:\n",
    "            child_strs_list = ea.ha.get_navigable_children(div_soup, [])\n",
    "            child_tags_list = ea.ha.get_child_tags_list(child_strs_list)\n",
    "            for sequence_order, header_tag in enumerate(child_tags_list):\n",
    "                header_tag_id = cu.get_headertag_id(header_tag, verbose=verbose)\n",
    "                header_tag_sequence_id = cu.get_headertagsequence_id(file_name_id, header_tag_id, sequence_order, verbose=verbose)\n",
    "                cu.ensure_headertagsequence_filename_relationship(file_name_id, verbose=verbose)\n",
    "                cu.ensure_headertagsequence_headertag_relationship(header_tag_id, verbose=verbose)\n",
    "            for sequence_order, navigable_parent in enumerate(child_strs_list):\n",
    "                navigable_parent_id = cu.get_navigableparent_id(navigable_parent, verbose=verbose)\n",
    "                navigable_parent_sequence_id = cu.get_navigableparentsequence_id(file_name_id, navigable_parent_id, sequence_order, verbose=verbose)\n",
    "                cu.ensure_navigableparentsequence_filename_relationship(file_name_id, verbose=verbose)\n",
    "                cu.ensure_navigableparentsequence_navigableparent_relationship(navigable_parent_id, verbose=verbose)\n",
    "                \n",
    "                yield navigable_parent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "CHILD_STRS_LIST = list(generate_child_strs(verbose=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "ZMQInteractiveShell_obj = get_ipython()\n",
    "NAVIGABLE_PARENT_IS_HEADER_DICT = s.load_object('NAVIGABLE_PARENT_IS_HEADER_DICT')\n",
    "def get_dictionary_code():\n",
    "    output_str = ''\n",
    "    tag_str = CHILD_STRS_LIST.pop()\n",
    "    while tag_str in NAVIGABLE_PARENT_IS_HEADER_DICT:\n",
    "        tag_str = CHILD_STRS_LIST.pop()\n",
    "    output_str += f'\\n# {len(CHILD_STRS_LIST)} to go\\n'\n",
    "    if \"'\" in tag_str:\n",
    "        tag_str = tag_str.replace('\"', '\\\\\"')\n",
    "        output_str += f'tag_str = \"{tag_str}\"\\n'\n",
    "    else:\n",
    "        output_str += f\"tag_str = '{tag_str}'\\n\"\n",
    "    output_str += 'NAVIGABLE_PARENT_IS_HEADER_DICT[tag_str] = False\\n'\n",
    "    output_str += 'print(len(NAVIGABLE_PARENT_IS_HEADER_DICT.keys()))\\n'\n",
    "    output_str += 's.store_objects(NAVIGABLE_PARENT_IS_HEADER_DICT=NAVIGABLE_PARENT_IS_HEADER_DICT)\\n'\n",
    "    output_str += 'navigable_parent = cu.clean_text(tag_str)\\n'\n",
    "    output_str += 'if NAVIGABLE_PARENT_IS_HEADER_DICT[tag_str]:\\n'\n",
    "    output_str += '    cypher_str = cu.set_is_header1_cypher_str.format(navigable_parent)\\n'\n",
    "    output_str += 'else:\\n'\n",
    "    output_str += '    cypher_str = cu.set_is_header0_cypher_str.format(navigable_parent)\\n'\n",
    "    output_str += 'print(cypher_str)\\n'\n",
    "    output_str += 'with cu.driver.session() as session:\\n'\n",
    "    output_str += '    session.write_transaction(cu.do_cypher_tx, cypher_str)'\n",
    "    \n",
    "    return output_str"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "ZMQInteractiveShell_obj.set_next_input(text=get_dictionary_code(), replace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 368,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11891\n",
      "Pickling to C:\\Users\\daveb\\OneDrive\\Documents\\GitHub\\job-hunting\\saves\\pkl\\NAVIGABLE_PARENT_IS_HEADER_DICT.pkl\n",
      "\n",
      "            MATCH (np:NavigableParents {navigable_parent: '<p>As part of our small but mighty team, you will influence the design and implementation of our data management strategy. Day to day work will include:</p>'})\n",
      "            SET np.is_header = 'False';\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# 397 to go\n",
    "tag_str = '<p>As part of our small but mighty team, you will influence the design and implementation of our data management strategy. Day to day work will include:</p>'\n",
    "NAVIGABLE_PARENT_IS_HEADER_DICT[tag_str] = False\n",
    "print(len(NAVIGABLE_PARENT_IS_HEADER_DICT.keys()))\n",
    "s.store_objects(NAVIGABLE_PARENT_IS_HEADER_DICT=NAVIGABLE_PARENT_IS_HEADER_DICT)\n",
    "navigable_parent = cu.clean_text(tag_str)\n",
    "if NAVIGABLE_PARENT_IS_HEADER_DICT[tag_str]:\n",
    "    cypher_str = cu.set_is_header1_cypher_str.format(navigable_parent)\n",
    "else:\n",
    "    cypher_str = cu.set_is_header0_cypher_str.format(navigable_parent)\n",
    "print(cypher_str)\n",
    "with cu.driver.session() as session:\n",
    "    session.write_transaction(cu.do_cypher_tx, cypher_str)"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
