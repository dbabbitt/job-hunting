{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pretty printing has been turned OFF\n"
     ]
    }
   ],
   "source": [
    "\n",
    "%pprint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import sys\n",
    "\n",
    "# Insert at 1, 0 is the script path (or '' in REPL)\n",
    "sys.path.insert(1, '../py')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from storage import Storage\n",
    "s = Storage()\n",
    "\n",
    "from ha_utils import HeaderAnalysis\n",
    "ha = HeaderAnalysis(verbose=False)\n",
    "\n",
    "from scrape_utils import WebScrapingUtilities\n",
    "wsu = WebScrapingUtilities()\n",
    "uri = wsu.secrets_json['neo4j']['connect_url']\n",
    "user =  wsu.secrets_json['neo4j']['username']\n",
    "password = wsu.secrets_json['neo4j']['password']\n",
    "\n",
    "from cypher_utils import CypherUtilities\n",
    "cu = CypherUtilities(uri=uri, user=user, password=password, driver=None, s=s, ha=ha)\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Last run on 2023-02-26 19:53:20.076462\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from IPython.display import clear_output\n",
    "from datetime import datetime\n",
    "import humanize\n",
    "import os\n",
    "import re\n",
    "import time\n",
    "import winsound\n",
    "\n",
    "duration = 1000  # milliseconds\n",
    "freq = 880  # Hz\n",
    "print(f'Last run on {datetime.now()}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "----"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'navigable_parent': '•\\xa0\\xa0 \\xa0Work closely with our media agency to marry the front end media performance to back end, Client owned measurements including site analytics, conversions and sentiment.', 'is_header': 'False', 'is_task_scope': 'True', 'is_qualification': None, 'is_minimum_qualification': 'False', 'is_preferred_qualification': 'False', 'is_legal_notification': 'False', 'is_job_title': 'False', 'is_office_location': 'False', 'is_job_duration': 'False', 'is_supplemental_pay': 'False', 'is_educational_requirement': 'False', 'is_interview_procedure': 'False', 'is_corporate_scope': 'False', 'is_posting_date': 'False', 'is_other': 'False'}, {'navigable_parent': '•\\xa0\\xa0 \\xa0Maintain and further optimize a set of dashboards that provide data-informed insights as it pertains to keywords, audiences, lengths and investment levels.', 'is_header': 'False', 'is_task_scope': 'True', 'is_qualification': None, 'is_minimum_qualification': 'False', 'is_preferred_qualification': 'False', 'is_legal_notification': 'False', 'is_job_title': 'False', 'is_office_location': 'False', 'is_job_duration': 'False', 'is_supplemental_pay': 'False', 'is_educational_requirement': 'False', 'is_interview_procedure': 'False', 'is_corporate_scope': 'False', 'is_posting_date': 'False', 'is_other': 'False'}, {'navigable_parent': '•\\xa0\\xa0 \\xa0Conduct ad hoc reporting with insightful recommendations', 'is_header': 'False', 'is_task_scope': 'True', 'is_qualification': None, 'is_minimum_qualification': 'False', 'is_preferred_qualification': 'False', 'is_legal_notification': 'False', 'is_job_title': 'False', 'is_office_location': 'False', 'is_job_duration': 'False', 'is_supplemental_pay': 'False', 'is_educational_requirement': 'False', 'is_interview_procedure': 'False', 'is_corporate_scope': 'False', 'is_posting_date': 'False', 'is_other': 'False'}]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "# Show what's in the database already for this html string\n",
    "def do_cypher_tx(tx):\n",
    "    cypher_str = '''\n",
    "        MATCH (np:NavigableParents)\n",
    "        WHERE\n",
    "            (np.is_header = 'False')\n",
    "            AND (np.is_task_scope = 'True')\n",
    "        ''' + cu.return_everything_str + ';'\n",
    "    results_list = tx.run(query=cypher_str)\n",
    "\n",
    "    return [dict(record.items()) for record in results_list]\n",
    "with cu.driver.session() as session:\n",
    "    row_objs_list = session.write_transaction(do_cypher_tx)\n",
    "df = pd.DataFrame(row_objs_list[:3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "----"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from nltk.tokenize import sent_tokenize\n",
    "\n",
    "roles_df = s.load_csv(csv_name='Accenture_Technology_Open_Roles_02.21.2023', folder_path='../saves').rename(columns={'Client': 'Client Name'})\n",
    "roles_df.columns = [re.sub(r'[^A-Za-z0-9]+', ' ', cn).strip().replace(' ', '_').lower() for cn in roles_df.columns]\n",
    "mask_series = (roles_df.role_is_sold == 'Yes')\n",
    "base_dict = {\n",
    "    'header': 'False',\n",
    "    'task_scope': 'False',\n",
    "    'minimum_qualification': 'False',\n",
    "    'preferred_qualification': 'False',\n",
    "    'educational_requirement': 'False',\n",
    "    'legal_notification': 'False',\n",
    "    'other': 'False',\n",
    "    'corporate_scope': 'False',\n",
    "    'job_title': 'False',\n",
    "    'office_location': 'False',\n",
    "    'job_duration': 'False',\n",
    "    'supplemental_pay': 'False',\n",
    "    'interview_procedure': 'False',\n",
    "    'posting_date': 'False'\n",
    "}\n",
    "fake_stops_list = ['e.g.', 'etc.', 'M.S.', 'B.S.', 'Ph.D.', '(ex.', '(Ex.', 'U.S.',\n",
    "                   'i.e.', '&amp;']\n",
    "replacements_list = ['eg', 'etc', 'MS', 'BS', 'PhD', '(eg', '(eg', 'US', 'ie', '&']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def create_accenture_file(file_path, row, verbose=False):\n",
    "    with open(file_path, 'w', encoding='utf-8') as f:\n",
    "        print('<html><body><div id=\"jobDescriptionText\" class=\"jobsearch-jobDescriptionText\">', file=f)\n",
    "\n",
    "        # Get dual-roled descriptions list\n",
    "        role_description = row.role_description.strip()\n",
    "        paragraph_str = re.sub(r'\\s+', ' ', role_description).strip()\n",
    "        for fake_stop, replacement in zip(fake_stops_list, replacements_list):\n",
    "            paragraph_str = paragraph_str.replace(fake_stop, replacement)\n",
    "        child_strs_list = sent_tokenize(paragraph_str)\n",
    "        \n",
    "        # Get parts-of-speech list\n",
    "        child_tags_list = ha.get_child_tags_list(child_strs_list)\n",
    "        is_header_list = []\n",
    "        for is_header, child_str in zip(ha.get_is_header_list(child_strs_list), child_strs_list):\n",
    "            if is_header is None:\n",
    "                probs_list = lru.ISHEADER_PREDICT_PERCENT_FIT(child_str)\n",
    "                idx = probs_list.index(max(probs_list))\n",
    "                is_header = [True, False][idx]\n",
    "            is_header_list.append(is_header)\n",
    "        feature_dict_list = hc.get_feature_dict_list(child_tags_list, is_header_list, child_strs_list)\n",
    "        feature_tuple_list = []\n",
    "        for feature_dict in feature_dict_list:\n",
    "            feature_tuple_list.append(hc.get_feature_tuple(feature_dict, lru.pos_lr_predict_single))\n",
    "        crf_list = crf.CRF.predict_single(crf.sent2features(feature_tuple_list))\n",
    "        db_pos_list = []\n",
    "        for navigable_parent in child_strs_list:\n",
    "            db_pos_list = cu.append_parts_of_speech_list(navigable_parent, pos_list=db_pos_list)\n",
    "        for crf_symbol, db_symbol in zip(crf_list, db_pos_list):\n",
    "            if db_symbol in [None, 'O', 'H']:\n",
    "                pos_list.append(crf_symbol)\n",
    "            else:\n",
    "                pos_list.append(db_symbol)\n",
    "        if verbose:\n",
    "            print(pos_list)\n",
    "            raise\n",
    "\n",
    "        # Role Primary Skill\n",
    "        role_primary_skill = row.role_primary_skill.strip()\n",
    "        print('<hrq>Role Primary Skill:</hrq>', file=f)\n",
    "        for role_str in child_strs_list:\n",
    "            if (' plus' in role_str) or ('preferred' in role_str):\n",
    "                print(f'<opq>{role_str}</opq>', file=f)\n",
    "            else:\n",
    "                role_str = role_str.lstrip('·*- ')\n",
    "                if role_str:\n",
    "                    print(f'<orq>Ability to {role_str[0].lower()}{role_str[1:]}</orq>', file=f)\n",
    "        skills_list = [s.strip() for s in re.split('[|/]', role_primary_skill, 0) if s.strip()]\n",
    "        for skill_str in skills_list:\n",
    "            print(f'<orq>{skill_str}</orq>', file=f)\n",
    "\n",
    "        # Role Description\n",
    "        print('<hts>Role Description:</hts>', file=f)\n",
    "        for role_str in child_strs_list:\n",
    "            print(f'<ots>{role_str}</ots>', file=f)\n",
    "\n",
    "        # Role ID\n",
    "        print(f'<ojt>Role ID: {role_id}</ojt>', file=f)\n",
    "\n",
    "        # Client\n",
    "        print(f'<ocs>Client: {client_name}</ocs>', file=f)\n",
    "\n",
    "        # Role Title\n",
    "        print(f'<ojt>Role Title: {role_title}</ojt>', file=f)\n",
    "\n",
    "        # Assigned Role\n",
    "        assigned_role = row.assigned_role.strip()\n",
    "        print(f'<ots>Assigned Role: {assigned_role}</ots>', file=f)\n",
    "\n",
    "        # Project Metro City\n",
    "        project_metro_city = row.project_metro_city.strip()\n",
    "        print(f'<ool>Project Metro City: {project_metro_city}</ool>', file=f)\n",
    "\n",
    "        # Career Level From - To\n",
    "        career_level_from_to = row.career_level_from_to.strip()\n",
    "        print(f'<osp>Career Level From - To: {career_level_from_to}</osp>', file=f)\n",
    "\n",
    "        # Role Start Date\n",
    "        role_start_date = row.role_start_date.strip()\n",
    "        print(f'<ojd>Role Start Date: {role_start_date}</ojd>', file=f)\n",
    "\n",
    "        # Role End Date\n",
    "        role_end_date = row.role_end_date.strip()\n",
    "        print(f'<ojd>Role End Date: {role_end_date}</ojd>', file=f)\n",
    "\n",
    "        # Role Client Supply Contact\n",
    "        role_client_supply_contact = row.role_client_supply_contact.strip()\n",
    "        print(f'<oip>Role Client Supply Contact: {role_client_supply_contact}</oip>', file=f)\n",
    "\n",
    "        # Role Primary Contact\n",
    "        role_primary_contact = row.role_primary_contact.strip()\n",
    "        print(f'<oip>Role Primary Contact: {role_primary_contact}</oip>', file=f)\n",
    "\n",
    "        # Role Primary Contact (Email ID)\n",
    "        role_primary_contact_email_id = row.role_primary_contact_email_id.strip()\n",
    "        print(f'<oip>Role Primary Contact (Email ID): {role_primary_contact_email_id}</oip>', file=f)\n",
    "\n",
    "        # Role Is Sold\n",
    "        role_is_sold = row.role_is_sold.strip()\n",
    "        print(f'<oip>Role Is Sold: {role_is_sold}</oip>', file=f)\n",
    "\n",
    "        print('</div></body></html>', file=f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "STARTSWITH_STR = '''\n",
    "    MATCH (np:NavigableParents)\n",
    "    WHERE (np.navigable_parent STARTS WITH \"<{html_prefix}{html_suffix}>\")\n",
    "    SET\n",
    "        '''\n",
    "COMMA_STR = ''',\n",
    "        '''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "for row in tqdm(roles_df[mask_series].itertuples(index=False), total=len(roles_df[mask_series])):\n",
    "    \n",
    "    # Create the file path\n",
    "    role_id = str(row.role_id).strip()\n",
    "    client_name = row.client_name.strip()\n",
    "    role_title = row.role_title.strip()\n",
    "    file_name = re.sub(r'[^A-Za-z0-9]+', ' ', f'{role_id} {client_name} {role_title}').strip().replace(' ', '_') + '.html'\n",
    "    file_path = os.path.join(cu.SAVES_HTML_FOLDER, file_name)\n",
    "    # if os.path.exists(file_path):\n",
    "    #     cu.delete_filename_node(file_name, verbose=False)\n",
    "    if not os.path.exists(file_path):\n",
    "        \n",
    "        # Create the file using HTML\n",
    "        create_accenture_file(file_path, row, verbose=True)\n",
    "        \n",
    "        # Populate the database with the file info\n",
    "        page_soup = wsu.get_page_soup(file_path)\n",
    "        div_soup = page_soup.find_all(name='div', id='jobDescriptionText')[0]\n",
    "        child_strs_list = ha.get_navigable_children(div_soup, [])\n",
    "        cu.ensure_filename(file_name, verbose=False)\n",
    "        cu.populate_from_child_strings(child_strs_list, file_name, verbose=False)\n",
    "        cu.set_accenture_data(\n",
    "            file_name, assigned_role, career_level_from_to, client_name, project_metro_city, role_client_supply_contact, role_end_date,\n",
    "            role_id, role_is_sold, role_primary_contact, role_primary_contact_email_id, role_start_date, role_title, verbose=False\n",
    "        )\n",
    "        for is_header, html_prefix in zip(['True', 'False'], ['h', 'o']):\n",
    "            for cypher_suffix, html_suffix in zip(\n",
    "                ['corporate_scope', 'interview_procedure', 'job_duration', 'job_title', 'office_location', 'posting_date', 'preferred_qualification', 'minimum_qualification', 'supplemental_pay', 'task_scope'],\n",
    "                ['cs', 'ip', 'jd', 'jt', 'ol', 'pd', 'pq', 'rq', 'sp', 'ts']\n",
    "            ):\n",
    "                working_dict = base_dict.copy()\n",
    "                working_dict.update({'header': is_header})\n",
    "                working_dict.update({cypher_suffix: 'True'})\n",
    "                \n",
    "                def do_cypher_tx(tx, verbose=False):\n",
    "                    cypher_str = STARTSWITH_STR.format(html_prefix, html_suffix)\n",
    "                    attrs_list = [f\"np.is_{k} = '{v}'\" for k, v in working_dict.items()]\n",
    "                    cypher_str += COMMA_STR.join(attrs_list)\n",
    "                    if verbose:\n",
    "                        clear_output(wait=True)\n",
    "                        print(cypher_str)\n",
    "                    parameter_dict = {}\n",
    "                    results_list = tx.run(query=cypher_str, parameters=parameter_dict)\n",
    "\n",
    "                with cu.driver.session() as session:\n",
    "                    session.write_transaction(do_cypher_tx, verbose=False)\n",
    "        clear_output(wait=True)\n",
    "        print(file_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "----"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "roles_df['Project Metro City'].map(lambda x: 'remote' in x.lower()).any()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "mask_series = roles_df['Role Description'].map(lambda x: 'remote' in x.lower())\n",
    "roles_df[mask_series]['Role Description'].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "print()\n",
    "for column_name in roles_df.columns:\n",
    "    variable_name = column_name.lower().replace(' ', '_')\n",
    "    print(f\"\"\"        {variable_name} = row_series['{column_name}'].strip()\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "----"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "files_list = sorted([fn for fn in os.listdir(cu.SAVES_HTML_FOLDER) if fn.endswith('.html')])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "t0 = time.time()\n",
    "for file_name in files_list:\n",
    "    cu.ensure_filename(file_name, verbose=True)\n",
    "    file_path = os.path.join(cu.SAVES_HTML_FOLDER, file_name)\n",
    "    page_soup = wsu.get_page_soup(file_path)\n",
    "    row_div_list = page_soup.find_all(name='div', id='jobDescriptionText')\n",
    "    for div_soup in row_div_list:\n",
    "        child_strs_list = ha.get_navigable_children(div_soup, [])\n",
    "        cu.populate_from_child_strings(child_strs_list, file_name, verbose=False)\n",
    "duration_str = humanize.precisedelta(time.time() - t0, minimum_unit='seconds', format='%0.0f')\n",
    "winsound.Beep(freq, duration)\n",
    "print(f'{len(files_list):,} file names reinserted in {duration_str}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "----\n",
    "## Unless you have written consent from the Generative AI and LLM CoE, you may not use generative AI tools while coding and cannot upload Accenture, ecosystem or client content or data to these tools."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "from selenium.common.exceptions import TimeoutException\n",
    "\n",
    "PREFIX_STR = '<orq>Ability to '\n",
    "strip_count = len(PREFIX_STR)\n",
    "def do_cypher_tx(tx, verbose=False):\n",
    "    cypher_str = f'''\n",
    "    MATCH (np:NavigableParents)\n",
    "    WHERE\n",
    "        (np.navigable_parent STARTS WITH \"{PREFIX_STR}\")\n",
    "        //AND (np.navigable_parent CONTAINS \"preferred\")\n",
    "    RETURN np.navigable_parent AS navigable_parent; '''\n",
    "    if verbose:\n",
    "        clear_output(wait=True)\n",
    "        print(cypher_str)\n",
    "    parameter_dict = {}\n",
    "    results_list = tx.run(query=cypher_str, parameters={})\n",
    "    values_list = []\n",
    "    for record in results_list:\n",
    "        values_list.append(dict(record.items()))\n",
    "\n",
    "    return values_list\n",
    "\n",
    "with cu.driver.session() as session:\n",
    "    rows_list = session.write_transaction(do_cypher_tx, verbose=True)\n",
    "df = pd.DataFrame(rows_list)\n",
    "ability2_list = df.navigable_parent.tolist()\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "driver = wsu.get_driver(verbose=False)\n",
    "for navigable_parent in tqdm(ability2_list):\n",
    "    orq_str = navigable_parent[strip_count:]\n",
    "    orq_str = orq_str[0].upper() + orq_str[1:]\n",
    "    orq_str = orq_str.replace('</orq>', '')\n",
    "    try:\n",
    "        youchat_text = wsu.get_chatgpt_rephrasing(driver, orq_str, part_of_speech='minimum requirement', verbose=False)\n",
    "    except TimeoutException as e:\n",
    "        clear_output(wait=True)\n",
    "        driver.close()\n",
    "        wsu.wait_for(1000, verbose=False)\n",
    "        driver = wsu.get_driver(verbose=False)\n",
    "        youchat_text = wsu.get_chatgpt_rephrasing(driver, orq_str, part_of_speech='minimum requirement', verbose=False)\n",
    "    if youchat_text:\n",
    "        youchat_text = '<orq>' + youchat_text + '</orq>'\n",
    "        def do_cypher_tx(tx, navigable_parent, youchat_html, verbose=False):\n",
    "            cypher_str = '''\n",
    "                MATCH (np:NavigableParents {navigable_parent: $navigable_parent})\n",
    "                SET np.navigable_parent = $youchat_html;'''\n",
    "            if verbose:\n",
    "                clear_output(wait=True)\n",
    "                print(cypher_str.replace('$navigable_parent', f'\"{navigable_parent}\"').replace('$youchat_html', f'\"{youchat_html}\"'))\n",
    "            parameter_dict = {'navigable_parent': navigable_parent, 'youchat_html': youchat_html}\n",
    "            tx.run(query=cypher_str, parameters=parameter_dict)\n",
    "\n",
    "        with cu.driver.session() as session:\n",
    "            session.write_transaction(\n",
    "                do_cypher_tx, navigable_parent=navigable_parent, youchat_html=youchat_text, verbose=False\n",
    "            )\n",
    "driver.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "driver.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Job Hunting (Python 3.10.9)",
   "language": "python",
   "name": "jh_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
