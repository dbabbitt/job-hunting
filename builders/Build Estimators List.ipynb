{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pretty printing has been turned OFF\n"
     ]
    }
   ],
   "source": [
    "\n",
    "%pprint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from sklearn.calibration import CalibratedClassifierCV\n",
    "from sklearn.dummy import DummyClassifier\n",
    "from sklearn.linear_model import LogisticRegressionCV\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.naive_bayes import BernoulliNB\n",
    "from sklearn.naive_bayes import ComplementNB\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.neighbors import RadiusNeighborsClassifier\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.tree import ExtraTreeClassifier\n",
    "\n",
    "estimators_list = [BernoulliNB(), CalibratedClassifierCV(), ComplementNB(), DecisionTreeClassifier(), DummyClassifier(), KNeighborsClassifier(),\n",
    "                   LogisticRegressionCV(), MLPClassifier(), MultinomialNB(), RadiusNeighborsClassifier(), SGDClassifier()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from inspect import signature\n",
    "\n",
    "method_name_list = []\n",
    "for clf in estimators_list:\n",
    "    method_name = type(clf).__name__\n",
    "    module_name = type(clf).__module__\n",
    "    import_exec_str = f'from {module_name} import {method_name}'\n",
    "    try:\n",
    "        exec(import_exec_str)\n",
    "        #print(import_exec_str)\n",
    "        method_name_list.append(method_name)\n",
    "    except Exception as e:\n",
    "        print(f'# The executed string \"{import_exec_str}\" gets this error: {str(e).strip()}', file=f)\n",
    "function_str_list = []\n",
    "for method_name in sorted(method_name_list):\n",
    "    sig_eval_str = f'signature({method_name})'\n",
    "    function_str = f'{method_name}('\n",
    "    signature_list = []\n",
    "    try:\n",
    "        sig = eval(sig_eval_str)\n",
    "        params_dict = dict(sig.parameters)\n",
    "        for parameter_obj in params_dict.values():\n",
    "            name = parameter_obj.name\n",
    "            default = parameter_obj.default\n",
    "            kind = parameter_obj.kind\n",
    "            if default is None:\n",
    "                signature_list.append(f'{name}=None')\n",
    "            elif str(default) == 'nan':\n",
    "                signature_list.append(f'{name}=np.nan')\n",
    "            elif type(default) is str:\n",
    "                signature_list.append(f\"{name}='{default}'\")\n",
    "            elif kind is parameter_obj.VAR_KEYWORD:\n",
    "                pass\n",
    "            elif type(default) is type:\n",
    "                signature_list.append(f'{name}={name}_obj')\n",
    "            else:\n",
    "                signature_list.append(f'{name}={default}')\n",
    "        function_str += ', '.join(signature_list)\n",
    "        function_str += ')'\n",
    "        function_str_list.append(function_str)\n",
    "    except Exception as e:\n",
    "        print(f'The evaluated string \"{sig_eval_str}\" gets this error: {str(e).strip()}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "estimators_list = [\n",
      "                   # done in xxxx\n",
      "                   # Best score: xxxx\n",
      "                    BernoulliNB(alpha=1.0, binarize=0.0, fit_prior=True, class_prior=None),\n",
      "\n",
      "                   # done in xxxx\n",
      "                   # Best score: xxxx\n",
      "                   CalibratedClassifierCV(base_estimator=None, method='sigmoid', cv=None),\n",
      "\n",
      "                   # done in xxxx\n",
      "                   # Best score: xxxx\n",
      "                   ComplementNB(alpha=1.0, fit_prior=True, class_prior=None, norm=False),\n",
      "\n",
      "                   # done in xxxx\n",
      "                   # Best score: xxxx\n",
      "                   DecisionTreeClassifier(criterion='gini', splitter='best', max_depth=None, min_samples_split=2, min_samples_leaf=1, min_weight_fraction_leaf=0.0, max_features=None, random_state=None, max_leaf_nodes=None, min_impurity_decrease=0.0, min_impurity_split=None, class_weight=None, presort='deprecated', ccp_alpha=0.0),\n",
      "\n",
      "                   # done in xxxx\n",
      "                   # Best score: xxxx\n",
      "                   DummyClassifier(strategy='warn', random_state=None, constant=None),\n",
      "\n",
      "                   # done in xxxx\n",
      "                   # Best score: xxxx\n",
      "                   KNeighborsClassifier(n_neighbors=5, weights='uniform', algorithm='auto', leaf_size=30, p=2, metric='minkowski', metric_params=None, n_jobs=None),\n",
      "\n",
      "                   # done in xxxx\n",
      "                   # Best score: xxxx\n",
      "                   LogisticRegressionCV(Cs=10, fit_intercept=True, cv=None, dual=False, penalty='l2', scoring=None, solver='lbfgs', tol=0.0001, max_iter=100, class_weight=None, n_jobs=None, verbose=0, refit=True, intercept_scaling=1.0, multi_class='auto', random_state=None, l1_ratios=None),\n",
      "\n",
      "                   # done in xxxx\n",
      "                   # Best score: xxxx\n",
      "                   MLPClassifier(hidden_layer_sizes=(100,), activation='relu', solver='adam', alpha=0.0001, batch_size='auto', learning_rate='constant', learning_rate_init=0.001, power_t=0.5, max_iter=200, shuffle=True, random_state=None, tol=0.0001, verbose=False, warm_start=False, momentum=0.9, nesterovs_momentum=True, early_stopping=False, validation_fraction=0.1, beta_1=0.9, beta_2=0.999, epsilon=1e-08, n_iter_no_change=10, max_fun=15000),\n",
      "\n",
      "                   # done in xxxx\n",
      "                   # Best score: xxxx\n",
      "                   MultinomialNB(alpha=1.0, fit_prior=True, class_prior=None),\n",
      "\n",
      "                   # done in xxxx\n",
      "                   # Best score: xxxx\n",
      "                   RadiusNeighborsClassifier(radius=1.0, weights='uniform', algorithm='auto', leaf_size=30, p=2, metric='minkowski', outlier_label=None, metric_params=None, n_jobs=None),\n",
      "\n",
      "                   # done in xxxx\n",
      "                   # Best score: xxxx\n",
      "                   SGDClassifier(loss='hinge', penalty='l2', alpha=0.0001, l1_ratio=0.15, fit_intercept=True, max_iter=1000, tol=0.001, shuffle=True, verbose=0, epsilon=0.1, n_jobs=None, random_state=None, learning_rate='optimal', eta0=0.0, power_t=0.5, early_stopping=False, validation_fraction=0.1, n_iter_no_change=5, class_weight=None, warm_start=False, average=False) \n",
      "                  ]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print('''\n",
    "estimators_list = [\n",
    "                   # done in xxxx\n",
    "                   # Best score: xxxx\n",
    "                   ''', ''',\n",
    "\n",
    "                   # done in xxxx\n",
    "                   # Best score: xxxx\n",
    "                   '''.join(function_str_list), '''\n",
    "                  ]''')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "function_str_list = []\n",
    "for method_name in sorted(method_name_list):\n",
    "    sig_eval_str = f'signature({method_name})'\n",
    "    function_str = f'{method_name}('\n",
    "    signature_list = []\n",
    "    try:\n",
    "        sig = eval(sig_eval_str)\n",
    "        params_dict = dict(sig.parameters)\n",
    "        print(f\"clf_params_dict['{method_name}'] = {{\", file=f)\n",
    "        clf_str_list = []\n",
    "        for parameter_obj in params_dict.values():\n",
    "            name = parameter_obj.name\n",
    "            default = parameter_obj.default\n",
    "            kind = parameter_obj.kind\n",
    "            if default is None:\n",
    "                clf_str_list.append(f\"        'clf__{name}': (None,),\")\n",
    "                #signature_list.append(f'{name}=None')\n",
    "            elif str(default) == 'nan':\n",
    "                clf_str_list.append(f\"        'clf__{name}': (np.nan,),\")\n",
    "                #signature_list.append(f'{name}=np.nan')\n",
    "            elif type(default) is str:\n",
    "                clf_str_list.append(f\"        'clf__{name}': ('{default}',),\")\n",
    "                #signature_list.append(f\"{name}='{default}'\")\n",
    "            elif kind is parameter_obj.VAR_KEYWORD:\n",
    "                pass\n",
    "            elif type(default) is type:\n",
    "                #clf_str_list.append(f\"        'clf__{name}': ({name}_obj,),\")\n",
    "                signature_list.append(f'{name}={name}_obj')\n",
    "            else:\n",
    "                clf_str_list.append(f\"        'clf__{name}': ({default},),\")\n",
    "                #signature_list.append(f'{name}={default}')\n",
    "        for clf_str in sorted(clf_str_list):\n",
    "            print(clf_str, file=f)\n",
    "        print('    }', file=f)\n",
    "        function_str += ', '.join(signature_list)\n",
    "        function_str += ')'\n",
    "        function_str_list.append(function_str)\n",
    "    except Exception as e:\n",
    "        print(f'        # The evaluated string \"{sig_eval_str}\" gets this error: {str(e).strip()}', file=f)\n",
    "        print('    }', file=f)\n",
    "print(estimators_list_str.format(estimators_infix_str.join(function_str_list)), file=f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "file_path = '../py/grid_search_params.py'\n",
    "with open(file_path, 'a') as f:\n",
    "    print('''\n",
    "for estimator in estimators_list:\n",
    "    steps_list = [\n",
    "        ('vect', CountVectorizer()),\n",
    "        ('tfidf', TfidfTransformer()),\n",
    "        ('clf', estimator),\n",
    "    ]\n",
    "    pipeline = Pipeline(steps_list)\n",
    "    clf_name = str(estimator.__class__).split('.')[-1].split(\"'\")[0]\n",
    "    params_dict = clf_params_dict[clf_name]\n",
    "    params_dict.update({\n",
    "        'tfidf__norm': ('l1',),\n",
    "        'tfidf__smooth_idf': (True,),\n",
    "        'tfidf__sublinear_tf': (False,),\n",
    "        'tfidf__use_idf': (True,),\n",
    "        'vect__analyzer': ('word',),\n",
    "        'vect__binary': (False,),\n",
    "        'vect__decode_error': ('strict',),\n",
    "        'vect__lowercase': (False,),\n",
    "        'vect__max_df': (1.0,),\n",
    "        'vect__max_features': (None,),\n",
    "        'vect__min_df': (0.0,),\n",
    "        'vect__ngram_range': ((1, 5),),\n",
    "        'vect__stop_words': (None,),\n",
    "        'vect__strip_accents': ('ascii',),\n",
    "        'vect__tokenizer': (regex_tokenizer,),\n",
    "    })\n",
    "    \n",
    "    \n",
    "    \n",
    "    # Find the best parameters for both the feature extraction and the\n",
    "    # classifier\n",
    "    grid_search = GridSearchCV(pipeline, params_dict, n_jobs=-1, verbose=1, scoring='f1')\n",
    "    \n",
    "    # Add inital pipeline parameters\n",
    "    with open(score_file_path, 'a') as f:\n",
    "        print('', file=f)\n",
    "        print('_'*len(clf_name), file=f)\n",
    "        print(clf_name, file=f)\n",
    "        print('^'*len(clf_name), file=f)\n",
    "        print(\"Performing grid search...\", file=f)\n",
    "        print(\"pipeline:\", [name for name, _ in pipeline.steps], file=f)\n",
    "        print(\"parameters:\", file=f)\n",
    "        print('{', file=f)\n",
    "        for key, value in params_dict.items():\n",
    "            print(f\"\"\"        '{key}': {str(value)},\"\"\", file=f)\n",
    "        print('}', file=f)\n",
    "    \n",
    "    # Add scores\n",
    "    t0 = time.time()\n",
    "    print()\n",
    "    print(time.ctime(t0))\n",
    "    print(clf_name)\n",
    "    grid_search.fit(data.data, data.target)\n",
    "    with open(score_file_path, 'a') as f:\n",
    "        print(\"done in %0.3fs\" % (time.time() - t0), file=f)\n",
    "        print('', file=f)\n",
    "        print(\"Best %s score: %0.3f\" % clf_name, grid_search.best_score_, file=f)\n",
    "        print(\"Best parameters set:\", file=f)\n",
    "    best_parameters_dict = grid_search.best_estimator_.get_params()\n",
    "    for param_name in sorted(params_dict.keys()):\n",
    "        with open(score_file_path, 'a') as f:\n",
    "            #print(\"    %s: %r\" % (param_name, best_parameters_dict[param_name]), file=f)\n",
    "            value_str = get_value_str(best_parameters_dict, params_dict, param_name)\n",
    "            print(f\"        '{param_name}': ({value_str},),\", file=f)\n",
    "    with open(score_file_path, 'a') as f:\n",
    "        print('', file=f)\n",
    "    \n",
    "    # Add function signatures\n",
    "    for step_tuple in steps_list:\n",
    "        step_prefix = step_tuple[0]\n",
    "        step_name = str(step_tuple[1].__class__).split('.')[-1].split(\"'\")[0]\n",
    "        function_str = f'{step_name}('\n",
    "        signature_list = []\n",
    "        for param_name in sorted(params_dict.keys()):\n",
    "            if param_name.startswith(step_prefix) and (param_name in best_parameters_dict):\n",
    "                value_str = get_value_str(best_parameters_dict, params_dict, param_name)\n",
    "                signature_list.append(f'{param_name[len(step_prefix)+2:]}={value_str}')\n",
    "        function_str += ', '.join(signature_list)\n",
    "        function_str += '),'\n",
    "        with open(score_file_path, 'a') as f:\n",
    "            print(function_str, file=f)''', file=f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
