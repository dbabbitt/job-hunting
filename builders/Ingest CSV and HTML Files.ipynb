{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pretty printing has been turned OFF\n"
     ]
    }
   ],
   "source": [
    "\n",
    "%pprint"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "\n",
    "---\n",
    "# Load needed libraries and functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from IPython.display import clear_output\n",
    "from datetime import datetime\n",
    "from nltk.tokenize import sent_tokenize\n",
    "from tqdm import tqdm\n",
    "import humanize\n",
    "import os\n",
    "import re\n",
    "import sys\n",
    "import time\n",
    "import winsound\n",
    "\n",
    "# Insert at 1, 0 is the script path (or '' in REPL)\n",
    "if (osp.join(os.pardir, 'py') not in sys.path): sys.path.insert(1, osp.join(os.pardir, 'py'))\n",
    "\n",
    "duration = 1000  # milliseconds\n",
    "freq = 880  # Hz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Utility libraries created in 0 seconds\n",
      "Last run on 2023-03-06 10:25:57.358185\n"
     ]
    }
   ],
   "source": [
    "\n",
    "t0 = time.time()\n",
    "\n",
    "# Get the Neo4j driver\n",
    "from storage import Storage\n",
    "s = Storage()\n",
    "\n",
    "from scrape_utils import WebScrapingUtilities\n",
    "wsu = WebScrapingUtilities(s=s)\n",
    "uri = wsu.secrets_json['neo4j']['connect_url']\n",
    "user =  wsu.secrets_json['neo4j']['username']\n",
    "password = wsu.secrets_json['neo4j']['password']\n",
    "\n",
    "from ha_utils import HeaderAnalysis\n",
    "ha = HeaderAnalysis(s=s, verbose=False)\n",
    "\n",
    "# Get the neo4j object\n",
    "from cypher_utils import CypherUtilities\n",
    "cu = CypherUtilities(uri=uri, user=user, password=password, driver=None, s=s, ha=ha)\n",
    "\n",
    "duration_str = humanize.precisedelta(time.time() - t0, minimum_unit='seconds', format='%0.0f')\n",
    "wsu.beep(freq, duration)\n",
    "print(f'Utility libraries created in {duration_str}')\n",
    "print(f'Last run on {datetime.now()}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def create_accenture_file(file_path, row, verbose=False):\n",
    "    with open(file_path, 'w', encoding='utf-8') as f:\n",
    "        print('<html><body><div id=\"jobDescriptionText\" class=\"jobsearch-jobDescriptionText\">', file=f)\n",
    "\n",
    "        # Get dual-roled descriptions list\n",
    "        role_description = row.role_description.strip()\n",
    "        paragraph_str = re.sub('[·\\*\\-]?\\t([^\\r\\n]+)', r'<li>\\g<1></li>', role_description)\n",
    "        paragraph_str = re.sub('([^\\r\\n]+):\\s*[\\r\\n]+', r'<b>\\g<1>:</b>\\n', paragraph_str)\n",
    "        paragraph_str = re.sub(r'\\s+', ' ', paragraph_str).strip()\n",
    "        paragraph_str = re.sub(r'>\\s*<', '>\\n<', paragraph_str).strip()\n",
    "        for fake_stop, replacement in zip(fake_stops_list, replacements_list):\n",
    "            paragraph_str = paragraph_str.replace(fake_stop, replacement)\n",
    "        child_strs_list = [string for sublist in sent_tokenize(paragraph_str) for string in sublist.split('\\n')]\n",
    "        # child_tags_list = ha.construct_child_tags_list(child_strs_list)\n",
    "\n",
    "        # Role Primary Skill\n",
    "        role_primary_skill = row.role_primary_skill.strip()\n",
    "        print('<hrq>Role Primary Skill:</hrq>', file=f)\n",
    "        for role_str in child_strs_list:\n",
    "            if ('<b>' in role_str) and ('</b>' in role_str):\n",
    "                role_str = role_str.replace('<b>', '').replace('</b>', '')\n",
    "                print(f'<hrq>{role_str}</hrq>', file=f)\n",
    "            elif (' plus' in role_str) or ('preferred' in role_str):\n",
    "                print(f'<opq>{role_str}</opq>', file=f)\n",
    "            elif ('<li>' in role_str) and ('</li>' in role_str):\n",
    "                role_str = role_str.replace('<li>', '').replace('</li>', '').lstrip('·*- \\t')\n",
    "                if role_str:\n",
    "                    print(f'<orq>Ability to {role_str[0].lower()}{role_str[1:]}</orq>', file=f)\n",
    "            else:\n",
    "                print(f'<orq>Ability to {role_str[0].lower()}{role_str[1:]}</orq>', file=f)\n",
    "        skills_list = [s.strip() for s in re.split('[|/]', role_primary_skill, 0) if s.strip()]\n",
    "        for skill_str in skills_list:\n",
    "            print(f'<orq>{skill_str}</orq>', file=f)\n",
    "\n",
    "        # Role Description\n",
    "        print('<hts>Role Description:</hts>', file=f)\n",
    "        for role_str in child_strs_list:\n",
    "            role_str = role_str.replace('<li>', '').replace('</li>', '').lstrip('·*- \\t')\n",
    "            if role_str:\n",
    "                print(f'<ots>{role_str}</ots>', file=f)\n",
    "\n",
    "        # Role ID\n",
    "        print(f'<ojt>Role ID: {role_id}</ojt>', file=f)\n",
    "\n",
    "        # Client\n",
    "        print(f'<ocs>Client: {client_name}</ocs>', file=f)\n",
    "\n",
    "        # Role Title\n",
    "        print(f'<ojt>Role Title: {role_title}</ojt>', file=f)\n",
    "\n",
    "        # Assigned Role\n",
    "        assigned_role = row.assigned_role.strip()\n",
    "        print(f'<ots>Assigned Role: {assigned_role}</ots>', file=f)\n",
    "\n",
    "        # Project Metro City\n",
    "        project_metro_city = row.project_metro_city.strip()\n",
    "        print(f'<ool>Project Metro City: {project_metro_city}</ool>', file=f)\n",
    "\n",
    "        # Career Level From - To\n",
    "        career_level_from_to = row.career_level_from_to.strip()\n",
    "        print(f'<osp>Career Level From - To: {career_level_from_to}</osp>', file=f)\n",
    "\n",
    "        # Role Start Date\n",
    "        role_start_date = row.role_start_date.strip()\n",
    "        print(f'<ojd>Role Start Date: {role_start_date}</ojd>', file=f)\n",
    "\n",
    "        # Role End Date\n",
    "        role_end_date = row.role_end_date.strip()\n",
    "        print(f'<ojd>Role End Date: {role_end_date}</ojd>', file=f)\n",
    "\n",
    "        # Role Client Supply Contact\n",
    "        role_client_supply_contact = row.role_client_supply_contact.strip()\n",
    "        print(f'<oip>Role Client Supply Contact: {role_client_supply_contact}</oip>', file=f)\n",
    "\n",
    "        # Role Primary Contact\n",
    "        role_primary_contact = row.role_primary_contact.strip()\n",
    "        print(f'<oip>Role Primary Contact: {role_primary_contact}</oip>', file=f)\n",
    "\n",
    "        # Role Primary Contact (Email ID)\n",
    "        role_primary_contact_email_id = row.role_primary_contact_email_id.strip()\n",
    "        print(f'<oip>Role Primary Contact (Email ID): {role_primary_contact_email_id}</oip>', file=f)\n",
    "\n",
    "        print('</div></body></html>', file=f)\n",
    "        \n",
    "        return (\n",
    "            assigned_role, career_level_from_to, project_metro_city, role_client_supply_contact, role_end_date, role_primary_contact,\n",
    "            role_primary_contact_email_id, role_start_date\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def reset_parts_of_speech(verbose=False):\n",
    "    for is_header, html_prefix in zip([true, false], ['h', 'o']):\n",
    "        for cypher_suffix, html_suffix in zip(\n",
    "            ['corporate_scope', 'interview_procedure', 'job_duration', 'job_title', 'office_location', 'posting_date',\n",
    "             'preferred_qualification', 'minimum_qualification', 'supplemental_pay', 'task_scope'],\n",
    "            ['cs', 'ip', 'jd', 'jt', 'ol', 'pd', 'pq', 'rq', 'sp', 'ts']\n",
    "        ):\n",
    "            working_dict = base_dict.copy()\n",
    "            working_dict.update({'header': is_header})\n",
    "            working_dict.update({cypher_suffix: true})\n",
    "\n",
    "            def do_cypher_tx(tx, verbose=False):\n",
    "                cypher_str = STARTSWITH_STR.format(html_prefix, html_suffix)\n",
    "                attrs_list = [f\"np.is_{k} = '{v}'\" for k, v in working_dict.items()]\n",
    "                cypher_str += COMMA_STR.join(attrs_list)\n",
    "                if verbose:\n",
    "                    clear_output(wait=True)\n",
    "                    print(cypher_str)\n",
    "                parameter_dict = {}\n",
    "                results_list = tx.run(query=cypher_str, parameters=parameter_dict)\n",
    "\n",
    "            with cu.driver.session() as session:\n",
    "                session.write_transaction(do_cypher_tx, verbose=verbose)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1232, 13)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "roles_df = s.load_csv(csv_name='Accenture_Technology_Open_Roles_03.06.2023', folder_path='../saves').rename(columns={'Client': 'Client Name'})\n",
    "print(roles_df.shape)\n",
    "roles_df.columns = [re.sub(r'[^A-Za-z0-9]+', ' ', cn).strip().replace(' ', '_').lower() for cn in roles_df.columns]\n",
    "if 'role_is_sold' in roles_df.columns:\n",
    "    mask_series = (roles_df.role_is_sold == 'Yes')\n",
    "    df = roles_df[mask_series]\n",
    "else:\n",
    "    df = roles_df\n",
    "base_dict = {\n",
    "    'header': false,\n",
    "    'task_scope': false,\n",
    "    'minimum_qualification': false,\n",
    "    'preferred_qualification': false,\n",
    "    'educational_requirement': false,\n",
    "    'legal_notification': false,\n",
    "    'other': false,\n",
    "    'corporate_scope': false,\n",
    "    'job_title': false,\n",
    "    'office_location': false,\n",
    "    'job_duration': false,\n",
    "    'supplemental_pay': false,\n",
    "    'interview_procedure': false,\n",
    "    'posting_date': false\n",
    "}\n",
    "fake_stops_list = ['e.g.', 'etc.', 'M.S.', 'B.S.', 'Ph.D.', '(ex.', '(Ex.', 'U.S.',\n",
    "                   'i.e.', '&amp;', 'E.g.']\n",
    "replacements_list = ['eg', 'etc', 'MS', 'BS', 'PhD', '(eg', '(eg', 'US', 'ie', '&', 'eg']\n",
    "STARTSWITH_STR = '''\n",
    "    MATCH (np:NavigableParents)\n",
    "    WHERE (np.navigable_parent STARTS WITH \"<{}{}>\")\n",
    "    SET\n",
    "        '''\n",
    "COMMA_STR = ''',\n",
    "        '''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████| 1232/1232 [1:44:30<00:00,  5.09s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4729451_ULTIMATE_KRONOS_GROUP_Application_Developer.html\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "for row in tqdm(df.itertuples(index=False), total=df.shape[0]):\n",
    "    if str(row.role_id) != 'nan':\n",
    "\n",
    "        # Create the file path\n",
    "        role_id = str(row.role_id).strip()\n",
    "        client_name = row.client_name.strip()\n",
    "        role_title = row.role_title.strip()\n",
    "        file_name = re.sub(r'[^A-Za-z0-9]+', ' ', f'{role_id} {client_name} {role_title}').strip().replace(' ', '_') + '.html'\n",
    "        file_path = os.path.join(cu.SAVES_HTML_FOLDER, file_name)\n",
    "        if not os.path.exists(file_path):\n",
    "\n",
    "            # Create the file using HTML\n",
    "            (\n",
    "                assigned_role, career_level_from_to, project_metro_city, role_client_supply_contact, role_end_date, role_primary_contact, role_primary_contact_email_id, role_start_date\n",
    "            ) = create_accenture_file(file_path, row, verbose=True)\n",
    "\n",
    "            # Populate the database with the file info\n",
    "            page_soup = wsu.get_page_soup(file_path)\n",
    "            div_soup = page_soup.find_all(name='div', id='jobDescriptionText')[0]\n",
    "            child_strs_list = ha.get_navigable_children(div_soup, [])\n",
    "            cu.ensure_filename(file_name, verbose=False)\n",
    "            cu.populate_from_child_strings(child_strs_list, file_name, verbose=False)\n",
    "            cu.set_accenture_data(\n",
    "                file_name=file_name, assigned_role=assigned_role, career_level_from_to=career_level_from_to, client_name=client_name, project_metro_city=project_metro_city,\n",
    "                role_client_supply_contact=role_client_supply_contact, role_end_date=role_end_date, role_id=role_id, role_primary_contact=role_primary_contact,\n",
    "                role_primary_contact_email_id=role_primary_contact_email_id, role_start_date=role_start_date, role_title=role_title, verbose=False\n",
    "            )\n",
    "            reset_parts_of_speech(verbose=False)\n",
    "            clear_output(wait=True)\n",
    "            print(file_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raise"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "----"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "files_list = sorted([fn for fn in os.listdir(cu.SAVES_HTML_FOLDER) if fn.endswith('.html')])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "t0 = time.time()\n",
    "for file_name in files_list:\n",
    "    cu.ensure_filename(file_name, verbose=True)\n",
    "    file_path = os.path.join(cu.SAVES_HTML_FOLDER, file_name)\n",
    "    page_soup = wsu.get_page_soup(file_path)\n",
    "    row_div_list = page_soup.find_all(name='div', id='jobDescriptionText')\n",
    "    for div_soup in row_div_list:\n",
    "        child_strs_list = ha.get_navigable_children(div_soup, [])\n",
    "        cu.populate_from_child_strings(child_strs_list, file_name, verbose=False)\n",
    "duration_str = humanize.precisedelta(time.time() - t0, minimum_unit='seconds', format='%0.0f')\n",
    "wsu.beep(freq, duration)\n",
    "print(f'{len(files_list):,} file names reinserted in {duration_str}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "----\n",
    "## Unless you have written consent from the Generative AI and LLM CoE, you may not use generative AI tools while coding and cannot upload Accenture, ecosystem or client content or data to these tools."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import pandas as pd\n",
    "from selenium.common.exceptions import TimeoutException\n",
    "\n",
    "PREFIX_STR = '<orq>Ability to '\n",
    "strip_count = len(PREFIX_STR)\n",
    "def do_cypher_tx(tx, verbose=False):\n",
    "    cypher_str = f'''\n",
    "    MATCH (np:NavigableParents)\n",
    "    WHERE\n",
    "        (np.navigable_parent STARTS WITH \"{PREFIX_STR}\")\n",
    "        //AND (np.navigable_parent CONTAINS \"preferred\")\n",
    "    RETURN np.navigable_parent AS navigable_parent; '''\n",
    "    if verbose:\n",
    "        clear_output(wait=True)\n",
    "        print(cypher_str)\n",
    "    parameter_dict = {}\n",
    "    results_list = tx.run(query=cypher_str, parameters={})\n",
    "    values_list = []\n",
    "    for record in results_list:\n",
    "        values_list.append(dict(record.items()))\n",
    "\n",
    "    return values_list\n",
    "\n",
    "with cu.driver.session() as session:\n",
    "    rows_list = session.write_transaction(do_cypher_tx, verbose=True)\n",
    "df = pd.DataFrame(rows_list)\n",
    "ability2_list = df.navigable_parent.tolist()\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "driver = wsu.get_driver(verbose=False)\n",
    "for navigable_parent in tqdm(ability2_list):\n",
    "    orq_str = navigable_parent[strip_count:]\n",
    "    orq_str = orq_str[0].upper() + orq_str[1:]\n",
    "    orq_str = orq_str.replace('</orq>', '')\n",
    "    try:\n",
    "        youchat_text = wsu.get_chatgpt_rephrasing(driver, orq_str, part_of_speech='minimum requirement', verbose=False)\n",
    "    except TimeoutException as e:\n",
    "        clear_output(wait=True)\n",
    "        driver.close()\n",
    "        wsu.wait_for(1000, verbose=False)\n",
    "        driver = wsu.get_driver(verbose=False)\n",
    "        youchat_text = wsu.get_chatgpt_rephrasing(driver, orq_str, part_of_speech='minimum requirement', verbose=False)\n",
    "    if youchat_text:\n",
    "        youchat_text = '<orq>' + youchat_text + '</orq>'\n",
    "        def do_cypher_tx(tx, navigable_parent, youchat_html, verbose=False):\n",
    "            cypher_str = '''\n",
    "                MATCH (np:NavigableParents {navigable_parent: $navigable_parent})\n",
    "                SET np.navigable_parent = $youchat_html;'''\n",
    "            if verbose:\n",
    "                clear_output(wait=True)\n",
    "                print(cypher_str.replace('$navigable_parent', f'\"{navigable_parent}\"').replace('$youchat_html', f'\"{youchat_html}\"'))\n",
    "            parameter_dict = {'navigable_parent': navigable_parent, 'youchat_html': youchat_html}\n",
    "            tx.run(query=cypher_str, parameters=parameter_dict)\n",
    "\n",
    "        with cu.driver.session() as session:\n",
    "            session.write_transaction(\n",
    "                do_cypher_tx, navigable_parent=navigable_parent, youchat_html=youchat_text, verbose=False\n",
    "            )\n",
    "driver.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "driver.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "----"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Prescient Edge is seeking a Backend Distributed ML Engineer to support a Federal government client. As a Backend Distributed ML Engineer, you will contribute to the development of innovative breakthroughs in machine learning, generating insights from private data silos faster than ever before while preserving the privacy of the underlying data. The technology is developed by a team of covert intelligence operatives as well as government and financial services executives all working together to build a platform that is ready to cope with the agility, efficiency, and relentless push required to plow through the regulatory and compliance that these organizations require.\n",
      "We are seeking a Backend Distributed ML Engineer with the ability to contribute to the development of breakthroughs in machine learning and generate insights from private data silos. The ideal candidate must have experience in developing platforms that can handle the agility, efficiency, and compliance required by government and financial services organizations.\n",
      "\n",
      "Build up a strong understanding and expert knowledge of the various data sources and methodologies brought together for LiveRamp data sharing solutions retail transaction data, 1st-party CRM data, ad exposure and response data, and 3rd-party data provider attributes (demographics, psychographics, lifestyle segments)\n",
      "A strong understanding and expert knowledge of LiveRamp data sharing solutions retail transaction data, 1st-party CRM data, ad exposure and response data, and 3rd-party data provider attributes (demographics, psychographics, lifestyle segments) is a minimum requirement.\n",
      "\n",
      "Enable smarter business processes and implement analytics for meaningful insights\n",
      "The minimum requirement is the ability to enable smarter business processes and use analytics to gain meaningful insights.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Show what's in the database already for this html string\n",
    "import pandas as pd\n",
    "\n",
    "def do_cypher_tx(tx):\n",
    "    cypher_str = '''\n",
    "        MATCH (np:NavigableParents)\n",
    "        WHERE\n",
    "            (np.navigable_parent =~ \"^<([^><]+)>.+</\\\\1>$\")\n",
    "            AND (np.is_header = false)\n",
    "            AND (np.is_task_scope = true)\n",
    "        ''' + cu.return_every_np_str + '''\n",
    "        LIMIT 3;'''\n",
    "    results_list = tx.run(query=cypher_str)\n",
    "\n",
    "    return [dict(record.items()) for record in results_list]\n",
    "with cu.driver.session() as session:\n",
    "    row_objs_list = session.write_transaction(do_cypher_tx)\n",
    "df = pd.DataFrame(row_objs_list[:3])\n",
    "task_regex = re.compile(r'^<([^><]+)>([^><]+)</\\1>$')\n",
    "driver = wsu.get_driver(verbose=False)\n",
    "for html_str in df.navigable_parent.to_list():\n",
    "    for match_obj in task_regex.finditer(html_str):\n",
    "        task_str = match_obj.group(2)\n",
    "        youchat_text = wsu.get_chatgpt_rephrasing(driver, 'Ability to ' + task_str, part_of_speech='minimum requirement', verbose=False)\n",
    "        print()\n",
    "        print(task_str)\n",
    "        print(youchat_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "driver.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Job Hunting (Python 3.10.9)",
   "language": "python",
   "name": "jh_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
