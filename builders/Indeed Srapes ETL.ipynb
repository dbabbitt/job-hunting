{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pretty printing has been turned OFF\n"
     ]
    }
   ],
   "source": [
    "\n",
    "%pprint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import sys\n",
    "\n",
    "# Insert at 1, 0 is the script path (or '' in REPL)\n",
    "sys.path.insert(1, '../py')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: total: 35.8 s\n",
      "Wall time: 41.4 s\n"
     ]
    }
   ],
   "source": [
    "\n",
    "%%time\n",
    "from pandas import DataFrame\n",
    "\n",
    "from storage import Storage\n",
    "s = Storage()\n",
    "\n",
    "from ha_utils import HeaderAnalysis\n",
    "ha = HeaderAnalysis(verbose=False)\n",
    "\n",
    "from scrape_utils import WebScrapingUtilities\n",
    "wsu = WebScrapingUtilities()\n",
    "uri = wsu.secrets_json['neo4j']['connect_url']\n",
    "user =  wsu.secrets_json['neo4j']['username']\n",
    "password = wsu.secrets_json['neo4j']['password']\n",
    "\n",
    "from cypher_utils import CypherUtilities\n",
    "cu = CypherUtilities(uri=uri, user=user, password=password, driver=None, s=s, ha=ha)\n",
    "\n",
    "from hc_utils import HeaderCategories\n",
    "hc = HeaderCategories(cu=cu, verbose=False)\n",
    "\n",
    "from section_utils import SectionUtilities\n",
    "su = SectionUtilities(s=s, ha=ha, cu=cu, verbose=False)\n",
    "\n",
    "from lr_utils import LrUtilities\n",
    "lru = LrUtilities(ha=ha, cu=cu, hc=hc, verbose=False)\n",
    "lru.build_isheader_logistic_regression_elements()\n",
    "lru.build_pos_logistic_regression_elements()\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "%run ../load_magic/dataframes.py\n",
    "from datetime import datetime\n",
    "from urllib.parse import urlparse, parse_qs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "----"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Got an HTTPError with https://www.indeed.com/rc/clk/dl?jk=e6ea2963f2bf7038&from=ja&qd=RnZhMybXSk4M3QtTVGXWoZj-R_bxcYib5xeGNtZ7GZZCz9IDoAfw9Pn-C-g9mkpdHI4neQgiQceQG2P-Q_YyG8TLUsj70s3Sqe1WWe88NIQ&rd=AbLHrm6GRQMtYBMzyEs2L1_MKnaSAFGAsD6kfERFt3g&tk=1g5fg2ha2g78g800&alid=6254377b33b425113af40aa2: HTTP Error 404:\n",
      "Got an HTTPError with https://www.indeed.com/rc/clk/dl?jk=c29833f9f645ad3e&from=ja&qd=RnZhMybXSk4M3QtTVGXWoZj-R_bxcYib5xeGNtZ7GZZCz9IDoAfw9Pn-C-g9mkpdHI4neQgiQceQG2P-Q_YyG8TLUsj70s3Sqe1WWe88NIQ&rd=F-EaRGHMzpZKxB2tk3D1Ml_MKnaSAFGAsD6kfERFt3g&tk=1g5fg2ha2g78g800&alid=6254377b33b425113af40aa2: HTTP Error 404:\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from urllib.error import HTTPError\n",
    "\n",
    "file_path = '../data/html/indeed_email.html'\n",
    "page_soup = get_page_soup(file_path)\n",
    "css_selector = 'body > table > tbody > tr > td > a > table > tbody > tr > td > a'\n",
    "link_soups_list = page_soup.select(css_selector)\n",
    "files_list = []\n",
    "for link_soup in link_soups_list:\n",
    "    url_str = link_soup['href']\n",
    "    try:\n",
    "        page_soup = get_page_soup(url_str)\n",
    "        page_title = page_soup.find('title').string.strip()\n",
    "        file_name = re.sub(r'\\W+', ' ', page_title).strip().replace(' ', '_')\n",
    "        jk = parse_qs(urlparse(url_str).query).get('jk', [''])[0]\n",
    "        if len(jk):\n",
    "            file_name = f'{jk}_{file_name}.html'\n",
    "        else:\n",
    "            # file_name = datetime.now().strftime('%Y%m%d%H%M%S%f') + f'_{file_name}.html'\n",
    "            file_name = f'{file_name}.html'\n",
    "        file_path = os.path.join(cu.SAVES_HTML_FOLDER, file_name)\n",
    "        if not os.path.isfile(file_path):\n",
    "            with open(file_path, 'w', encoding=s.encoding_type) as f:\n",
    "                print(f'Saving to {file_path}')\n",
    "                f.write('<html><head><title>')\n",
    "                f.write(page_title)\n",
    "                f.write('</title></head><body>')\n",
    "                row_div_list = page_soup.find_all(name='div', attrs={'class': ['jobsearch-JobComponent-description']})\n",
    "                for div_soup in row_div_list:\n",
    "                    f.write(str(div_soup))\n",
    "                f.write('</body></html>')\n",
    "            files_list.append(file_name)\n",
    "        cu.ensure_filename(file_name, verbose=False)\n",
    "        file_name = cu.clean_text(file_name)\n",
    "        url_str = cu.clean_text(url_str)\n",
    "        cypher_str = f'''\n",
    "            MATCH (fn:FileNames {{file_name: \"{file_name}\"}})\n",
    "            SET fn.posting_url = \"{url_str}\"\n",
    "            RETURN fn;'''\n",
    "        with cu.driver.session() as session:\n",
    "            row_objs_list = session.write_transaction(cu.do_cypher_tx, cypher_str)\n",
    "            # print(row_objs_list)\n",
    "    except HTTPError as e:\n",
    "        print(f'Got an HTTPError with {url_str}: {str(e).strip()}')\n",
    "    except Exception as e:\n",
    "        print(f'Got an {e.__class__} error with {url_str}: {str(e).strip()}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "verbose = True\n",
    "cu.ensure_navigableparent('END', verbose=False)\n",
    "for file_name in files_list:\n",
    "    file_path = os.path.join(cu.SAVES_HTML_FOLDER, file_name)\n",
    "    page_soup = get_page_soup(file_path)\n",
    "    row_div_list = page_soup.find_all(name='div', id='jobDescriptionText')\n",
    "    for div_soup in row_div_list:\n",
    "        child_strs_list = ha.get_navigable_children(div_soup, [])\n",
    "        cu.populate_from_child_strings(child_strs_list, file_name, verbose=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
