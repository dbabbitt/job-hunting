{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pretty printing has been turned OFF\n"
     ]
    }
   ],
   "source": [
    "\n",
    "%pprint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import sys\n",
    "\n",
    "# Insert at 1, 0 is the script path (or '' in REPL)\n",
    "sys.path.insert(1, '../py')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Get the Neo4j driver\n",
    "from storage import Storage\n",
    "s = Storage()\n",
    "\n",
    "from ha_utils import HeaderAnalysis\n",
    "ha = HeaderAnalysis(verbose=False)\n",
    "\n",
    "from scrape_utils import WebScrapingUtilities\n",
    "wsu = WebScrapingUtilities()\n",
    "uri = wsu.secrets_json['neo4j']['connect_url']\n",
    "user =  wsu.secrets_json['neo4j']['username']\n",
    "password = wsu.secrets_json['neo4j']['password']\n",
    "\n",
    "from cypher_utils import CypherUtilities\n",
    "cu = CypherUtilities(uri=uri, user=user, password=password, driver=None, s=s, ha=ha)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======== None ========\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from neo4j.exceptions import ServiceUnavailable\n",
    "\n",
    "try:\n",
    "    version_str = cu.driver.verify_connectivity()\n",
    "    print(f'======== {version_str} ========')\n",
    "    \n",
    "    from hc_utils import HeaderCategories\n",
    "    hc = HeaderCategories(cu=cu, verbose=False)\n",
    "    \n",
    "    from section_utils import SectionUtilities\n",
    "    su = SectionUtilities(s=s, ha=ha, cu=cu, verbose=False)\n",
    "    \n",
    "    from lr_utils import LrUtilities\n",
    "    lru = LrUtilities(ha=ha, cu=cu, hc=hc, verbose=False)\n",
    "    \n",
    "    from crf_utils import CrfUtilities\n",
    "    crf = CrfUtilities(ha=ha, hc=hc, cu=cu, verbose=False)\n",
    "    \n",
    "    import warnings\n",
    "    warnings.filterwarnings('ignore')\n",
    "except ServiceUnavailable as e:\n",
    "    # print(str(e).strip())\n",
    "    raise ServiceUnavailable('You need to start Neo4j as a console')\n",
    "except Exception as e:\n",
    "    print(e.__class__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from datetime import datetime\n",
    "from urllib.parse import urlparse, parse_qs\n",
    "from IPython.display import clear_output\n",
    "import shutil\n",
    "import time\n",
    "import humanize\n",
    "%run ../load_magic/dataframes.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "navigable_parent_cypher_str = '''\n",
    "    MATCH (np:NavigableParents {{navigable_parent: '{}'}})\n",
    "    ''' + cu.return_everything_str + ';'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "---\n",
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Is-header classifier retrained in 12 seconds\n"
     ]
    }
   ],
   "source": [
    "\n",
    "t0 = time.time()\n",
    "lru.build_isheader_logistic_regression_elements(verbose=False)\n",
    "lru.retrain_isheader_classifier(verbose=False)\n",
    "duration_str = humanize.precisedelta(time.time() - t0, minimum_unit='seconds', format='%0.0f')\n",
    "print(f'Is-header classifier retrained in {duration_str}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parts-of-speech classifier rebuilt in 1 minute and 1 second\n"
     ]
    }
   ],
   "source": [
    "\n",
    "t0 = time.time()\n",
    "lru.build_pos_logistic_regression_elements(verbose=False)\n",
    "duration_str = humanize.precisedelta(time.time() - t0, minimum_unit='seconds', format='%0.0f')\n",
    "print(f'Parts-of-speech classifier rebuilt in {duration_str}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Is-qualified classifer retrained in 42 seconds\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Rebuild the classifer from the quals dictionary\n",
    "t0 = time.time()\n",
    "lru.build_isqualified_logistic_regression_elements(verbose=False)\n",
    "lru.retrain_isqualified_classifier(verbose=False)\n",
    "duration_str = humanize.precisedelta(time.time() - t0, minimum_unit='seconds', format='%0.0f')\n",
    "print(f'Is-qualified classifer retrained in {duration_str}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "----"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lead_Data_Scientist_Boston_MA_Hybrid_Long_term_Contract.html\n"
     ]
    }
   ],
   "source": [
    "\n",
    "file_path = r'C:\\Users\\daveb\\OneDrive\\Documents\\GitHub\\job-hunting\\data\\html\\other_email.html'\n",
    "file_name = re.sub(r'[^A-Za-z0-9]+', ' ',\n",
    "                   '''\n",
    "Lead Data Scientist\n",
    "Boston, MA (Hybrid)\n",
    "Long-term Contract\n",
    "''').strip().replace(' ', '_') + '.html'\n",
    "new_file_path = os.path.join(cu.SAVES_HTML_FOLDER, file_name)\n",
    "if os.path.isfile(new_file_path):\n",
    "    file_name = datetime.now().strftime('%Y%m%d%H%M%S%f') + f'_{file_name}'\n",
    "    new_file_path = os.path.join(cu.SAVES_HTML_FOLDER, file_name)\n",
    "if not os.path.isfile(new_file_path):\n",
    "    shutil.copy(file_path, os.path.join(cu.SAVES_HTML_FOLDER, file_name))\n",
    "    page_soup = get_page_soup(file_path)\n",
    "    div_soup = page_soup.find_all(name='div', id='jobDescriptionText')[0]\n",
    "    child_strs_list = ha.get_navigable_children(div_soup, [])\n",
    "    cu.ensure_filename(file_name, verbose=False)\n",
    "    cu.populate_from_child_strings(child_strs_list, file_name, verbose=False)\n",
    "    print(file_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 381,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "    MATCH (fn:FileNames {file_name: \"Sr_Data_Scientist_UMass_Memorial_Health_Worcester_MA_01608.html\"})\n",
      "    SET fn.posting_url = \"https://www.indeed.com/viewjob?jk=78378484a2e75fd6&tk=1g9nsupd7v4et808&from=jobi2a&advn=1593641562529740&adid=395461290&ad=-6NYlbfkN0AMaXa3HsIVUHhGOm8qWa3jps87ppVspeqLYuXaXfwUvTtXN3pcVmE_N7alNJEpzZj3mgotD54zrWPVlfmCXff-woQlMoXgzqETYa5F5ct05VMXgjmEyf0XKSWbSLlLvVRfNZXI1NYWuGTowU6iNq2fuZNaxJ96oipbVurd7ncAqCBr4ieYUdGK1fbMfxkusHVqRvDlgxnEkEPWZlKANlZMvQGDWgrlbgxgSTFQzxfjS1fLytJ3LEsJGhmdJmuAPvQhRMQxlURfFkQfZ9cMknDpbehyipfNja7AWsR8j5hTTaAgLiRIvGv8JCIVdSlmJUb3E2Qw1IosQMCjoZvEGalh2VWjp9_S7U8eihLCKDW0kaQUsgPrPddEH5gdk3q9UrO8EjohgMkCgktl0zKuAeCUxMD5voENPl7wu9WQR5ppAGbaPZZoxsh4Hh8d_gQkrRM_ZaQ0y2ObsEIOhrAEJFlf&pub=21d85ca573e478f5e659e48885c828920cace3277f6b99df&i2af=jobi2a_smbrez_posting_email&xkcb=SoD5-_M3b9pFxlg0WL0PbzkdCdPP\"\n",
      "    RETURN fn;\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'fn': <Node id=852753 labels=frozenset({'FileNames'}) properties={'file_name': 'Sr_Data_Scientist_UMass_Memorial_Health_Worcester_MA_01608.html', 'posting_url': 'https://www.indeed.com/viewjob?jk=78378484a2e75fd6&tk=1g9nsupd7v4et808&from=jobi2a&advn=1593641562529740&adid=395461290&ad=-6NYlbfkN0AMaXa3HsIVUHhGOm8qWa3jps87ppVspeqLYuXaXfwUvTtXN3pcVmE_N7alNJEpzZj3mgotD54zrWPVlfmCXff-woQlMoXgzqETYa5F5ct05VMXgjmEyf0XKSWbSLlLvVRfNZXI1NYWuGTowU6iNq2fuZNaxJ96oipbVurd7ncAqCBr4ieYUdGK1fbMfxkusHVqRvDlgxnEkEPWZlKANlZMvQGDWgrlbgxgSTFQzxfjS1fLytJ3LEsJGhmdJmuAPvQhRMQxlURfFkQfZ9cMknDpbehyipfNja7AWsR8j5hTTaAgLiRIvGv8JCIVdSlmJUb3E2Qw1IosQMCjoZvEGalh2VWjp9_S7U8eihLCKDW0kaQUsgPrPddEH5gdk3q9UrO8EjohgMkCgktl0zKuAeCUxMD5voENPl7wu9WQR5ppAGbaPZZoxsh4Hh8d_gQkrRM_ZaQ0y2ObsEIOhrAEJFlf&pub=21d85ca573e478f5e659e48885c828920cace3277f6b99df&i2af=jobi2a_smbrez_posting_email&xkcb=SoD5-_M3b9pFxlg0WL0PbzkdCdPP'}>}]"
      ]
     },
     "execution_count": 381,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "# Add the posting URL to the file name\n",
    "cypher_str = f'''\n",
    "    MATCH (fn:FileNames {{file_name: \"{file_name}\"}})\n",
    "    SET fn.posting_url = \"https://www.indeed.com/viewjob?jk=78378484a2e75fd6&tk=1g9nsupd7v4et808&from=jobi2a&advn=1593641562529740&adid=395461290&ad=-6NYlbfkN0AMaXa3HsIVUHhGOm8qWa3jps87ppVspeqLYuXaXfwUvTtXN3pcVmE_N7alNJEpzZj3mgotD54zrWPVlfmCXff-woQlMoXgzqETYa5F5ct05VMXgjmEyf0XKSWbSLlLvVRfNZXI1NYWuGTowU6iNq2fuZNaxJ96oipbVurd7ncAqCBr4ieYUdGK1fbMfxkusHVqRvDlgxnEkEPWZlKANlZMvQGDWgrlbgxgSTFQzxfjS1fLytJ3LEsJGhmdJmuAPvQhRMQxlURfFkQfZ9cMknDpbehyipfNja7AWsR8j5hTTaAgLiRIvGv8JCIVdSlmJUb3E2Qw1IosQMCjoZvEGalh2VWjp9_S7U8eihLCKDW0kaQUsgPrPddEH5gdk3q9UrO8EjohgMkCgktl0zKuAeCUxMD5voENPl7wu9WQR5ppAGbaPZZoxsh4Hh8d_gQkrRM_ZaQ0y2ObsEIOhrAEJFlf&pub=21d85ca573e478f5e659e48885c828920cace3277f6b99df&i2af=jobi2a_smbrez_posting_email&xkcb=SoD5-_M3b9pFxlg0WL0PbzkdCdPP\"\n",
    "    RETURN fn;'''\n",
    "print(cypher_str)\n",
    "with cu.driver.session() as session:\n",
    "    row_objs_list = session.write_transaction(cu.do_cypher_tx, cypher_str)\n",
    "row_objs_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['O-O', 'O-TS', 'O-O', 'O-CS', 'O-O', 'O-CS', 'H-TS', 'O-ER', 'O-RQ', 'O-RQ', 'O-RQ', 'O-PQ', 'O-CS', 'O-TS', 'O-TS', 'O-CS', 'O-RQ', 'O-RQ', 'O-RQ']\n",
      "[7, 8, 9, 10, 16, 17, 18]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "0 O-O) <span style=\"color:#8c564b80;\"><span lang=\"EN-US\" style=\"color:black;background:yellow\">Job Title: (O-O Other Non-header)</span></span><br />1 O-TS) <span style=\"color:#9edae580;\"><span style=\"color:black;background:yellow\">Lead Data Scientist (O-TS Task Scope Non-header)</span></span><br />2 O-O) <span style=\"color:#8c564b80;\"><span lang=\"EN-US\" style=\"color:black;background:yellow\">Location: (O-O Other Non-header)</span></span><br />3 O-CS) <span style=\"color:#1f77b480;\"><span style=\"color:black;background:yellow\">Boston, MA (Hybrid) (O-CS Corporate Scope Non-header)</span></span><br />4 O-O) <span style=\"color:#8c564b80;\"><span lang=\"EN-US\" style=\"color:black;background:yellow\">Duration: (O-O Other Non-header)</span></span><br />5 O-CS) <span style=\"color:#1f77b480;\"><b>Long-term Contract (O-CS Corporate Scope Non-header)</b></span><br />6 H-TS) <span style=\"color:#9edae5ff;\">Job Description: (H-TS Task Scope Header)</span><br /><hr />7 O-ER) <span style=\"color:#aec7e880;\">Has Bachelor’s or Master’s Degree in a technology related field (e.g. Engineering, Computer Science, etc.). (O-ER Education Requirements Non-header)</span><br />8 O-RQ) <span style=\"color:#bcbd2280;\">8+ years of proven experience in implementing Big data solutions in data analytics space. (O-RQ Required Qualifications Non-header)</span><br />9 O-RQ) <span style=\"color:#bcbd2280;\">2+ years of experience in developing ML infrastructure and MLOps in the Cloud using AWS Sagemaker. (O-RQ Required Qualifications Non-header)</span><br />10 O-RQ) <span style=\"color:#bcbd2280;\">Extensive experience working with machine learning models with respect to deployment, inference, tuning, and measurement required. (O-RQ Required Qualifications Non-header)</span><br />11 O-PQ) <span style=\"color:#c7c7c780;\">Experience in Object Oriented Programming (Java, Scala, Python), SQL, Unix scripting or related programming languages and exposure to some of Python’s ML ecosystem (numpy, panda, (O-PQ Preferred Qualifications Non-header)</span><br />12 O-CS) <span style=\"color:#1f77b480;\">sklearn, tensorflow, etc.). (O-CS Corporate Scope Non-header)</span><br />13 O-TS) <span style=\"color:#9edae580;\">Experience with building data pipelines in getting the data required to build and evaluate ML models, using tools like Apache Spark or other distributed data processing frameworks. (O-TS Task Scope Non-header)</span><br />14 O-TS) <span style=\"color:#9edae580;\">Data movement technologies (ETL/ELT), Messaging/Streaming Technologies (AWS SQS, Kinesis/Kafka), Relational and NoSQL databases (DynamoDB, EKS, Graph database), API and in-memory (O-TS Task Scope Non-header)</span><br />15 O-CS) <span style=\"color:#1f77b480;\">technologies. (O-CS Corporate Scope Non-header)</span><br />16 O-RQ) <span style=\"color:#bcbd2280;\">Strong knowledge of developing highly scalable distributed systems using Open-source technologies. (O-RQ Required Qualifications Non-header)</span><br />17 O-RQ) <span style=\"color:#bcbd2280;\">Experience with CI/CD tools (e.g., Jenkins or equivalent), version control (Git), orchestration/DAGs tools (AWS Step Functions, Airflow, Luigi, Kubeflow, or equivalent). (O-RQ Required Qualifications Non-header)</span><br />18 O-RQ) <span style=\"color:#bcbd2280;\">Solid experience in Agile methodologies (Kanban and SCRUM). (O-RQ Required Qualifications Non-header)</span><br /><hr />"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[7, 8, 9, 10, 16, 17, 18]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "child_tags_list = ha.get_child_tags_list(child_strs_list)\n",
    "is_header_list = []\n",
    "for is_header, child_str in zip(ha.get_is_header_list(child_strs_list), child_strs_list):\n",
    "    if is_header is None:\n",
    "        probs_list = lru.ISHEADER_PREDICT_PERCENT_FIT(child_str)\n",
    "        idx = probs_list.index(max(probs_list))\n",
    "        is_header = [True, False][idx]\n",
    "    is_header_list.append(is_header)\n",
    "feature_dict_list = hc.get_feature_dict_list(child_tags_list, is_header_list, child_strs_list)\n",
    "feature_tuple_list = []\n",
    "for feature_dict in feature_dict_list:\n",
    "    feature_tuple_list.append(hc.get_feature_tuple(feature_dict, lru.pos_lr_predict_single))\n",
    "crf_list = crf.CRF.predict_single(crf.sent2features(feature_tuple_list))\n",
    "db_pos_list = []\n",
    "for navigable_parent in child_strs_list:\n",
    "    db_pos_list = cu.append_parts_of_speech_list(navigable_parent, pos_list=db_pos_list)\n",
    "pos_list, indices_list = su.visualize_basic_quals_section(crf_list, child_strs_list, db_pos_list=db_pos_list, verbose=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "----"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[7, 8, 9, 10, 16, 17, 18]\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "list index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_18844\\3791259926.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m# Display the context of an individual child string\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[0midx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m19\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mindices_list\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m;\u001b[0m \u001b[0mchild_str\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mchild_strs_list\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0midx\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m;\u001b[0m \u001b[0mpos_symbol\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpos_list\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0midx\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m;\u001b[0m \u001b[0mbasic_quals_dict\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0ms\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mload_object\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'basic_quals_dict'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m;\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbasic_quals_dict\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mchild_str\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;34m'\\n'\u001b[0m \u001b[1;32mif\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mchild_str\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mbasic_quals_dict\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32melse\u001b[0m \u001b[1;34m''\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mend\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m''\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m;\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34mf'{idx} {pos_symbol}) {child_str}'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m: list index out of range"
     ]
    }
   ],
   "source": [
    "\n",
    "# Display the context of an individual child string\n",
    "idx = 19\n",
    "print(indices_list); child_str = child_strs_list[idx]; pos_symbol = pos_list[idx]; basic_quals_dict = s.load_object('basic_quals_dict'); print(str(basic_quals_dict[child_str]) + '\\n' if(child_str in basic_quals_dict) else '', end=''); print(f'{idx} {pos_symbol}) {child_str}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 438,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pickling to C:\\Users\\daveb\\OneDrive\\Documents\\GitHub\\job-hunting\\saves\\pkl\\basic_quals_dict.pkl\n",
      "\"<li>Experience working with Optum Performance Analytics, medical claims data.</li>\" in basic_quals_dict: 0\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Hand-label this particular child string in the quals dictionary\n",
    "basic_quals_dict = s.load_object('basic_quals_dict')\n",
    "basic_quals_dict[child_str] = 0\n",
    "s.store_objects(basic_quals_dict=basic_quals_dict); print(f'\"{child_str}\" in basic_quals_dict: {basic_quals_dict[child_str]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'navigable_parent': 'Solid experience in Agile methodologies (Kanban and SCRUM).', 'is_header': 'False', 'is_task_scope': 'False', 'is_qualification': None, 'is_minimum_qualification': 'True', 'is_preferred_qualification': 'False', 'is_legal_notification': 'False', 'is_job_title': 'False', 'is_office_location': 'False', 'is_job_duration': 'False', 'is_supplemental_pay': 'False', 'is_educational_requirement': 'False', 'is_interview_procedure': 'False', 'is_corporate_scope': 'False', 'is_posting_date': 'False', 'is_other': 'False'}]"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "def do_cypher_tx(tx, navigable_parent, verbose=False):\n",
    "    cypher_str = \"\"\"MATCH (np:NavigableParents {navigable_parent: $navigable_parent})\n",
    "        SET\n",
    "            np.is_header = 'False',\n",
    "            np.is_task_scope = 'False',\n",
    "            np.is_minimum_qualification = 'True',\n",
    "            np.is_preferred_qualification = 'False',\n",
    "            np.is_educational_requirement = 'False',\n",
    "            np.is_legal_notification = 'False',\n",
    "            np.is_other = 'False',\n",
    "            np.is_corporate_scope = 'False',\n",
    "            np.is_job_title = 'False',\n",
    "            np.is_office_location = 'False',\n",
    "            np.is_job_duration = 'False',\n",
    "            np.is_supplemental_pay = 'False',\n",
    "            np.is_interview_procedure = 'False',\n",
    "            np.is_posting_date = 'False'\n",
    "        \"\"\" + cu.return_everything_str + ';'\n",
    "    results_list = tx.run(query=cypher_str, parameters={'navigable_parent': navigable_parent})\n",
    "\n",
    "    return [dict(record.items()) for record in results_list]\n",
    "with cu.driver.session() as session:\n",
    "    row_objs_list = session.write_transaction(do_cypher_tx, navigable_parent=child_str)\n",
    "row_objs_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pickling to C:\\Users\\daveb\\OneDrive\\Documents\\GitHub\\job-hunting\\saves\\pkl\\basic_quals_dict.pkl\n",
      "\"a larger business audience\" in basic_quals_dict: False\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Remove this particular child string from the quals dictionary\n",
    "basic_quals_dict = s.load_object('basic_quals_dict')\n",
    "child_str = child_strs_list[idx]\n",
    "basic_quals_dict.pop(child_str)\n",
    "# basic_quals_dict[child_str] = 0\n",
    "s.store_objects(basic_quals_dict=basic_quals_dict)\n",
    "print(f'\"{child_str}\" in basic_quals_dict: {child_str in basic_quals_dict}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
