{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "174c9bb2-7d08-4fa2-aa2b-e3be342e52ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pretty printing has been turned OFF\n"
     ]
    }
   ],
   "source": [
    "\n",
    "%pprint"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fb4f695-4003-423d-989a-efdaa0757778",
   "metadata": {
    "tags": []
   },
   "source": [
    "\n",
    "---\n",
    "# Load needed libraries and functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "528c4de8-98a4-4896-862e-03c72e6ce192",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "%matplotlib inline\n",
    "from datetime import datetime\n",
    "from neo4j.exceptions import ServiceUnavailable\n",
    "import humanize\n",
    "import os\n",
    "import sys\n",
    "import time\n",
    "import warnings\n",
    "import winsound\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "duration = 1000  # milliseconds\n",
    "freq = 880  # Hz\n",
    "\n",
    "# Insert at 1, 0 is the script path (or '' in REPL)\n",
    "sys.path.insert(1, '../py')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ff7aa81f-314d-4d0d-a238-355d5210f157",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======== Neo4j/4.4.7 ========\n",
      "Utility libraries created in 8 seconds\n"
     ]
    }
   ],
   "source": [
    "\n",
    "t0 = t1 = time.time()\n",
    "\n",
    "# Get the Storage object\n",
    "from storage import Storage\n",
    "s = Storage(\n",
    "    data_folder_path=os.path.abspath('../data'),\n",
    "    saves_folder_path=os.path.abspath('../saves')\n",
    ")\n",
    "\n",
    "# Get the WebScrapingUtilities object\n",
    "from scrape_utils import WebScrapingUtilities\n",
    "wsu = WebScrapingUtilities(\n",
    "    s=s,\n",
    "    secrets_json_path=os.path.abspath('../data/secrets/jh_secrets.json')\n",
    ")\n",
    "uri = wsu.secrets_json['neo4j']['connect_url']\n",
    "user =  wsu.secrets_json['neo4j']['username']\n",
    "password = wsu.secrets_json['neo4j']['password']\n",
    "\n",
    "# Get the HeaderAnalysis object\n",
    "from ha_utils import HeaderAnalysis\n",
    "ha = HeaderAnalysis(s=s, verbose=False)\n",
    "\n",
    "# Get the CypherUtilities object and Neo4j driver\n",
    "from cypher_utils import CypherUtilities\n",
    "cu = CypherUtilities(\n",
    "    uri=uri, user=user, password=password, driver=None, s=s, ha=ha\n",
    ")\n",
    "\n",
    "try:\n",
    "    version_str = cu.driver.get_server_info().agent\n",
    "    print(f'======== {version_str} ========')\n",
    "except ServiceUnavailable as e:\n",
    "    print(f'You need to start Neo4j as a console')\n",
    "    raise\n",
    "except Exception as e:\n",
    "    print(f'{e.__class__}: {str(e).strip()}')\n",
    "\n",
    "# Get the IsHeaderSgdClassifier object\n",
    "from is_header_sgd_classifier import IsHeaderSgdClassifier\n",
    "ihu = IsHeaderSgdClassifier(ha=ha, cu=cu, verbose=False)\n",
    "\n",
    "# Get the HeaderCategories object\n",
    "from hc_utils import HeaderCategories\n",
    "hc = HeaderCategories(cu=cu, verbose=False)\n",
    "\n",
    "# Get the LrUtilities object\n",
    "from lr_utils import LrUtilities\n",
    "lru = LrUtilities(ha=ha, cu=cu, hc=hc, verbose=False)\n",
    "\n",
    "# Get the SectionLRClassifierUtilities object\n",
    "from section_classifier_utils import SectionLRClassifierUtilities\n",
    "slrcu = SectionLRClassifierUtilities(ha=ha, cu=cu, verbose=False)\n",
    "\n",
    "# Get the SectionSGDClassifierUtilities object\n",
    "from section_classifier_utils import SectionSGDClassifierUtilities\n",
    "ssgdcu = SectionSGDClassifierUtilities(ha=ha, cu=cu, verbose=False)\n",
    "\n",
    "# Get the SectionCRFClassifierUtilities object\n",
    "from section_classifier_utils import SectionCRFClassifierUtilities\n",
    "scrfcu = SectionCRFClassifierUtilities(cu=cu, ha=ha, verbose=False)\n",
    "\n",
    "# Get the CrfUtilities object\n",
    "from crf_utils import CrfUtilities\n",
    "crf = CrfUtilities(\n",
    "    ha=ha, hc=hc, cu=cu, lru=lru, slrcu=slrcu, scrfcu=scrfcu, ssgdcu=ssgdcu, verbose=True\n",
    ")\n",
    "\n",
    "# Get the SectionUtilities object\n",
    "from section_utils import SectionUtilities\n",
    "su = SectionUtilities(\n",
    "    wsu=wsu, ihu=ihu, hc=hc, crf=crf, slrcu=slrcu, scrfcu=scrfcu, ssgdcu=ssgdcu, verbose=False\n",
    ")\n",
    "\n",
    "duration_str = humanize.precisedelta(time.time() - t1, minimum_unit='seconds', format='%0.0f')\n",
    "winsound.Beep(freq, duration)\n",
    "print(f'Utility libraries created in {duration_str}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fd3f331b-c66a-4c65-906b-22bc6fc034c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I have 49,124 labeled parts of speech in here\n",
      "predict_single is available\n",
      "Parts-of-speech logistic regression elements built in 1 hour, 43 minutes and 42 seconds\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Check if the slrcu has built its parts-of-speech logistic regression elements\n",
    "# Parts-of-speech logistic regression elements is normally built in 1 hour, 27 minutes and 21 seconds\n",
    "t1 = time.time()\n",
    "if not hasattr(slrcu, 'pos_predict_percent_fit_dict'):\n",
    "    slrcu.build_pos_logistic_regression_elements(sampling_strategy_limit=None, verbose=True)\n",
    "if hasattr(slrcu, 'pos_predict_percent_fit_dict'):\n",
    "    print('predict_single is available')\n",
    "else:\n",
    "    print('predict_single is not available')\n",
    "duration_str = humanize.precisedelta(time.time() - t1, minimum_unit='seconds', format='%0.0f')\n",
    "print(f'Parts-of-speech logistic regression elements built in {duration_str}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3fe80356-c69a-4579-b298-31e40e871b31",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "predict_single is now available\n",
      "Parts-of-speech conditional random field elements built in 4 seconds\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Check if the scrfcu has built its parts-of-speech conditional random field elements\n",
    "# Parts-of-speech CRF elements normally built in 29 minutes and 57 seconds\n",
    "t1 = time.time()\n",
    "if not hasattr(scrfcu, 'pos_symbol_crf'):\n",
    "    scrfcu.build_pos_conditional_random_field_elements(verbose=True)\n",
    "if hasattr(scrfcu, 'pos_predict_percent_fit_dict'):\n",
    "    print('predict_single is now available')\n",
    "else:\n",
    "    print('predict_single is not available')\n",
    "duration_str = humanize.precisedelta(time.time() - t1, minimum_unit='seconds', format='%0.0f')\n",
    "print(f'Parts-of-speech conditional random field elements built in {duration_str}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3453a69d-7080-4ebf-96a8-528e9541ea19",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I have 49,124 labeled parts of speech in here\n",
      "predict_single is now available\n",
      "Parts-of-speech stochastic gradient descent elements built in 9 seconds\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Check if the ssgdcu has built its parts-of-speech stochastic gradient decent elements\n",
    "t1 = time.time()\n",
    "if not hasattr(ssgdcu, 'pos_predict_percent_fit_dict'):\n",
    "    ssgdcu.build_pos_stochastic_gradient_descent_elements(sampling_strategy_limit=None, verbose=True)\n",
    "if hasattr(ssgdcu, 'pos_predict_percent_fit_dict'):\n",
    "    print('predict_single is now available')\n",
    "else:\n",
    "    print('predict_single is not available')\n",
    "duration_str = humanize.precisedelta(time.time() - t1, minimum_unit='seconds', format='%0.0f')\n",
    "print(f'Parts-of-speech stochastic gradient descent elements built in {duration_str}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "01e7d031-531c-4024-b9f4-03485a91a943",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "predict_single is now available\n",
      "POS classifier trained in 0 seconds\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Check if the crf has built its parts-of-speech classifier\n",
    "# POS classifier normally trained in 15 hours, 42 minutes and 41 seconds\n",
    "t1 = time.time()\n",
    "if not hasattr(crf, 'CRF'):\n",
    "    crf.build_pos_conditional_random_field_elements(verbose=True)\n",
    "if hasattr(crf, 'CRF'):\n",
    "    print('predict_single is now available')\n",
    "else:\n",
    "    print('predict_single is not available')\n",
    "duration_str = humanize.precisedelta(time.time() - t1, minimum_unit='seconds', format='%0.0f')\n",
    "print(f'POS classifier trained in {duration_str}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d259dffb-c212-4493-a879-1ae5d503f8ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I have 424,879 is-qualified vocabulary tokens in here\n",
      "Is-qualified LR elements built in 5 seconds\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Check if the lru has built its is-qualified classifier\n",
    "t1 = time.time()\n",
    "if not (hasattr(lru, 'ISQUALIFIED_LR') and hasattr(lru, 'ISQUALIFIED_CV')):\n",
    "    lru.build_isqualified_logistic_regression_elements(sampling_strategy_limit=None, verbose=True)\n",
    "duration_str = humanize.precisedelta(time.time() - t1, minimum_unit='seconds', format='%0.0f')\n",
    "print(f'Is-qualified LR elements built in {duration_str}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a9d67a34-e133-45dc-9bf3-a830af359d77",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I have 48,960 hand-labeled header htmls prepared\n",
      "7 iterations seen during training fit for a total of 48,960 records trained\n",
      "Is-header classifier trained in 6 seconds\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Train the isheader classifier\n",
    "t1 = time.time()\n",
    "ihu.build_pos_stochastic_gradient_descent_elements(verbose=True)\n",
    "duration_str = humanize.precisedelta(time.time() - t1, minimum_unit='seconds', format='%0.0f')\n",
    "print(f'Is-header classifier trained in {duration_str}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "18453504-f05c-4d52-86e9-050bdfca2040",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def display_file_in_text_editor(file_name):\n",
    "    text_editor_path = r\"C:\\Program Files\\Notepad++\\notepad++.exe\"\n",
    "    file_path = os.path.join(ha.SAVES_HTML_FOLDER, file_name)\n",
    "    !\"{text_editor_path}\" \"{os.path.abspath(file_path)}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "dcef1d94-4466-4100-86cd-0d66f23d0d40",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Last run on 2023-04-12 21:43:45.601842\n"
     ]
    }
   ],
   "source": [
    "\n",
    "winsound.Beep(freq, duration)\n",
    "print(f'Last run on {datetime.now()}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f438163-0f14-4a74-978c-769c9dba95c8",
   "metadata": {},
   "source": [
    "\n",
    "---\n",
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f02bfeac-67b7-4df6-a4a0-32ec3dec0022",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I have 13,268 hand-labeled qualification strings in here\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# You need to run this again if you changed the\n",
    "# qualification dictionary below or in another notebook\n",
    "t1 = time.time()\n",
    "\n",
    "# Keep the total retraining time to less than two minutes by adjusting the sampling strategy limit\n",
    "lru.sync_basic_quals_dict(sampling_strategy_limit=None, verbose=False)\n",
    "\n",
    "lru.retrain_isqualified_classifier(verbose=True)\n",
    "duration_str = humanize.precisedelta(time.time() - t1, minimum_unit='seconds', format='%0.0f')\n",
    "print(f'Is-qualified classifer retrained in {duration_str}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52ad3579-ebc3-4d21-a7c3-f9bd168ac6ca",
   "metadata": {},
   "source": [
    "\n",
    "---\n",
    "# Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d334d815-e43b-474e-9dbb-db509176f6e5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "t1 = time.time()\n",
    "\n",
    "# Loop through all the unset %fit values, set them if you can, break for help if you can't\n",
    "quals_list, file_name = lru.infer_from_hunting_dataframe(su=su, verbose=True)\n",
    "\n",
    "# Display as histogram\n",
    "lru.display_hunting_dataframe_as_histogram()\n",
    "\n",
    "# Show how long it took\n",
    "duration_str = humanize.precisedelta(time.time() - t1, minimum_unit='seconds', format='%0.0f')\n",
    "winsound.Beep(freq, duration)\n",
    "print(f'Minimum-requirements-met percentages estimated in {duration_str}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adde7906-399d-4a84-a2cb-ed327f307ebe",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Restarting the kernel and getting to this cell took 1 hour, 45 minutes and 13 seconds\n",
    "duration_str = humanize.precisedelta(time.time() - t0, minimum_unit='seconds', format='%0.0f')\n",
    "print(f'Restarting the kernel and getting to this cell took {duration_str}'); raise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 299,
   "id": "47bd03b8-c0a3-404c-a16d-eb4ce95c6561",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<li>Experience using Matlab/Simulink</li>\n",
      "Pickling to C:\\Users\\daveb\\OneDrive\\Documents\\GitHub\\job-hunting\\saves\\pkl\\basic_quals_dict.pkl\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Manually label the unscored qual\n",
    "qualification_str = quals_list[10]\n",
    "print(qualification_str); basic_quals_dict = s.load_object('basic_quals_dict')\n",
    "print(str(basic_quals_dict[qualification_str]) + '\\n' if(qualification_str in basic_quals_dict) else '', end='')\n",
    "basic_quals_dict[qualification_str] = 1\n",
    "s.store_objects(basic_quals_dict=basic_quals_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50146b21-cb1d-4475-9465-2a383f002533",
   "metadata": {},
   "source": [
    "\n",
    "----\n",
    "## Fix Parts-of-Speech and Quals for this posting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 247,
   "id": "8ac6efb8-6b56-43dd-9079-d6a87491ecc5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WC15DnKnMNbfDHcP2Ze72g_Senior_Data_Scientist_NielsenIQ_Warsaw_Mazowieckie_Poland_Remote.html\n",
      "CRF and child strings list recreated in 41 seconds\n"
     ]
    }
   ],
   "source": [
    "\n",
    "t1 = time.time()\n",
    "file_path = os.path.join(cu.SAVES_HTML_FOLDER, file_name)\n",
    "if os.path.isfile(file_path):\n",
    "    child_strs_list = ha.get_child_strs_from_file(file_name=file_name)\n",
    "    cu.ensure_filename(file_name, verbose=False)\n",
    "    cu.populate_from_child_strings(child_strs_list, file_name, verbose=False)\n",
    "    print(file_name)\n",
    "    child_tags_list = ha.get_child_tags_list(child_strs_list)\n",
    "    feature_dict_list = cu.get_feature_dict_list(child_tags_list, child_strs_list)\n",
    "    feature_tuple_list = []\n",
    "    for feature_dict in feature_dict_list:\n",
    "        feature_tuple_list.append(hc.get_feature_tuple(\n",
    "            feature_dict, pos_lr_predict_single=slrcu.predict_single, pos_crf_predict_single=scrfcu.predict_single,\n",
    "            pos_sgd_predict_single=ssgdcu.predict_single\n",
    "        ))\n",
    "    crf_list = crf.CRF.predict_single(crf.sent2features(feature_tuple_list))\n",
    "duration_str = humanize.precisedelta(time.time() - t1, minimum_unit='seconds', format='%0.0f')\n",
    "print(f'CRF and child strings list recreated in {duration_str}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 281,
   "id": "affcb06a-3165-4349-a936-037af549d417",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['H-TS', 'H-CS', 'O-JT', 'O-TS', 'H-TS', 'O-PQ', 'O-RQ', 'H-TS', 'O-TS', 'O-TS', 'O-TS', 'O-TS', 'O-TS', 'O-TS', 'H-RQ', 'O-ER', 'O-RQ', 'O-RQ', 'O-RQ', 'O-RQ', 'O-RQ', 'O-RQ', 'O-RQ', 'O-PQ', 'H-O', 'H-CS', 'O-CS', 'O-IP', 'H-CS', 'H-LN', 'O-LN', 'O-LN', 'H-CS']\n",
      "[6, 15, 16, 17, 18, 19, 20, 21, 22]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "0 H-TS) <span style=\"color:#9edae5ff;\"><h2 class=\"text-heading-large mb4\">About the job (H-TS Task Scope Header)</h2></span><br />1 H-CS) <span style=\"color:#1f77b4ff;\"><strong>Company Description (H-CS Corporate Scope Header)</strong></span><br />2 O-JT) <span style=\"color:#d6272880;\">REF25770R (O-JT Job Title Non-header)</span><br />3 O-TS) <span style=\"color:#9edae580;\">You will work with our global NIQ team dedicated to the development of E-Commerce products, a new way to measure how people shop online. Supporting our recent Fox Intelligence acquisition, we are looking for someone to develop our E-Commerce measurement offerings in EU5 countries and then deliver other E-Commerce initiatives in Western Europe. (O-TS Task Scope Non-header)</span><br />4 H-TS) <span style=\"color:#9edae5ff;\"><strong>Job Description (H-TS Task Scope Header)</strong></span><br />5 O-PQ) <span style=\"color:#c7c7c780;\">Ideally, you should have familiarity with consumer sourced data, exposure to business intelligence or market research, and have experience with weighting and projection algorithms. (O-PQ Preferred Qualifications Non-header)</span><br /><hr />6 O-RQ) <span style=\"color:#bcbd2280;\">You should have a passion for challenging data opportunities! (O-RQ Required Qualifications Non-header)</span><br />7 H-TS) <span style=\"color:#9edae5ff;\"><strong>Responsibilities (H-TS Task Scope Header)</strong></span><br />8 O-TS) <span style=\"color:#9edae580;\"><li>Utilize Fox Intelligence’s datasets to deliver best-in-class E-Commerce measurement to Western European markets – evaluating and integrating internal and external truth sets to serve as weighting targets (O-TS Task Scope Non-header)</li></span><br />9 O-TS) <span style=\"color:#9edae580;\"><li>Execute weighting and projection on consumer sourced data, evaluating quality of results against available truth sets and algorithm KPIs (O-TS Task Scope Non-header)</li></span><br />10 O-TS) <span style=\"color:#9edae580;\"><li>Evaluate current E-Commerce measurement methodologies to identify opportunities for enhancement (O-TS Task Scope Non-header)</li></span><br />11 O-TS) <span style=\"color:#9edae580;\"><li>Deliver on methodology enhancements to data collection methodologies, improving overall quality from client perspective (O-TS Task Scope Non-header)</li></span><br />12 O-TS) <span style=\"color:#9edae580;\"><li>Deliver codebase and documentation to local Data Science, Technical and Operations teams (O-TS Task Scope Non-header)</li></span><br />13 O-TS) <span style=\"color:#9edae580;\"><li>Prototype as well as support pilot programs to drive innovation. (O-TS Task Scope Non-header)</li></span><br />14 H-RQ) <span style=\"color:#bcbd22ff;\"><strong>We are looking for people with (H-RQ Required Qualifications Header)</strong></span><br />15 O-ER) <span style=\"color:#aec7e880;\"><li>Bachelor’s or Master’s Degree or Doctorate Degree (O-ER Education Requirements Non-header)</li></span><br />16 O-RQ) <span style=\"color:#bcbd2280;\"><li>Experienced in high-level programming languages (Python or R) (O-RQ Required Qualifications Non-header)</li></span><br />17 O-RQ) <span style=\"color:#bcbd2280;\"><li>Knowledge in SQL, working with queries and large-scale databases (O-RQ Required Qualifications Non-header)</li></span><br />18 O-RQ) <span style=\"color:#bcbd2280;\"><li>Excellent statistical and logic skills. Experience in trend analysis, multivariate statistics (parametric/ non-parametric), sampling, bias reduction, indirect estimation, data aggregation techniques, weighting and projection, and model validation techniques (O-RQ Required Qualifications Non-header)</li></span><br />19 O-RQ) <span style=\"color:#bcbd2280;\"><li>Strong communication and collaboration skills (O-RQ Required Qualifications Non-header)</li></span><br />20 O-RQ) <span style=\"color:#bcbd2280;\"><li>Ability to effectively convey complex concepts to non-experts (O-RQ Required Qualifications Non-header)</li></span><br />21 O-RQ) <span style=\"color:#bcbd2280;\"><li>Business acumen, the ability to link client's needs with the business (O-RQ Required Qualifications Non-header)</li></span><br />22 O-RQ) <span style=\"color:#bcbd2280;\"><li>Ability to manage multiple projects simultaneously (O-RQ Required Qualifications Non-header)</li></span><br /><hr />23 O-PQ) <span style=\"color:#c7c7c780;\"><li>Fluency in French, German or Italian is a plus! (O-PQ Preferred Qualifications Non-header)</li></span><br />24 H-O) <span style=\"color:#8c564bff;\"><strong>Additional Information (H-O Other Header)</strong></span><br />25 H-CS) <span style=\"color:#1f77b4ff;\"><strong>About NielsenIQ (H-CS Corporate Scope Header)</strong></span><br />26 O-CS) <span style=\"color:#1f77b480;\">NielsenIQ is a global measurement and data analytics company providing the most complete and trusted view of consumers and markets in 90 countries covering 90% of the world’s population. Focusing on consumer-packaged goods manufacturers and FMCG and retailers, we enable customers to defy what’s possible. How? We combine unparalleled datasets, pioneering technology, and the industry’s top talent to create insights that unlock innovation. Join us and change the landscape. (O-CS Corporate Scope Non-header)</span><br />27 O-IP) <span style=\"color:#ffbb7880;\">Learn more at: www.niq.com (O-IP Interview Procedures Non-header)</span><br />28 H-CS) <span style=\"color:#1f77b4ff;\">Want to keep up with our latest updates? Follow us on: LinkedIn | Instagram | Twitter | Facebook (H-CS Corporate Scope Header)</span><br />29 H-LN) <span style=\"color:#9467bdff;\"><strong>Our commitment to Diversity, Equity, and Inclusion (H-LN Legal Notifications Header)</strong></span><br />30 O-LN) <span style=\"color:#9467bd80;\">NielsenIQ is committed to reflecting the diversity of the clients, communities, and markets we measure within our own workforce. We exist to count everyone and are on a mission to systematically embed inclusion and diversity into all aspects of our workforce, measurement, and products. We enthusiastically invite candidates who share that mission to join us. (O-LN Legal Notifications Non-header)</span><br />31 O-LN) <span style=\"color:#9467bd80;\">We are proud to be an Equal Opportunity/Affirmative Action-Employer, making decisions without regard to race, color, religion, gender, gender identity or expression, sexual orientation, national origin, genetics, disability status, age, marital status, protected veteran status or any other protected class. Our global non-discrimination policy covers these protected classes in every market in which we do business worldwide. (O-LN Legal Notifications Non-header)</span><br />32 H-CS) <span style=\"color:#1f77b4ff;\">Learn more about how we are driving diversity and inclusion in everything we do by visiting the NielsenIQ News Center: https://nielseniq.com/global/en/news-center/diversity-inclusion/ (H-CS Corporate Scope Header)</span><br />"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[6, 15, 16, 17, 18, 19, 20, 21, 22]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "db_pos_list = []\n",
    "for navigable_parent in child_strs_list:\n",
    "    db_pos_list = cu.append_parts_of_speech_list(navigable_parent, pos_list=db_pos_list)\n",
    "pos_list, indices_list = su.visualize_basic_quals_section(crf_list, child_strs_list, db_pos_list=db_pos_list, verbose=True)\n",
    "winsound.Beep(freq, duration)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91379a3d-1a77-4d8b-9ef6-a76f12204ac2",
   "metadata": {},
   "outputs": [],
   "source": [
    "raise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 278,
   "id": "1d97acb2-2b60-4a56-9b3b-ecfc2b6582e6",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[15, 16, 17, 18, 19, 20, 21, 22]\n",
      "6 H-RQ) You should have a passion for challenging data opportunities!\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Display the context of an individual child string\n",
    "idx = 6\n",
    "print(indices_list); child_str = child_strs_list[idx]; pos_symbol = pos_list[idx]; basic_quals_dict = s.load_object('basic_quals_dict')\n",
    "print(str(basic_quals_dict[child_str]) + '\\n' if(child_str in basic_quals_dict) else '', end='')\n",
    "print(f'{idx} {pos_symbol}) {child_str}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 279,
   "id": "b2f46926-880b-4aed-bc99-4a6bd9ed7903",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pickling to C:\\Users\\daveb\\OneDrive\\Documents\\GitHub\\job-hunting\\saves\\pkl\\basic_quals_dict.pkl\n",
      "\"You should have a passion for challenging data opportunities!\" in basic_quals_dict: 1\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Hand-label this particular child string in the quals dictionary\n",
    "basic_quals_dict = s.load_object('basic_quals_dict')\n",
    "basic_quals_dict[child_str] = 1\n",
    "s.store_objects(basic_quals_dict=basic_quals_dict); print(f'\"{child_str}\" in basic_quals_dict: {basic_quals_dict[child_str]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 280,
   "id": "4005f4a8-f07c-4538-8d1b-210d32dd3199",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'navigable_parent': 'You should have a passion for challenging data opportunities!', 'is_header': 'False', 'is_task_scope': 'False', 'is_minimum_qualification': 'True', 'is_preferred_qualification': 'False', 'is_legal_notification': 'False', 'is_job_title': 'False', 'is_office_location': 'False', 'is_job_duration': 'False', 'is_supplemental_pay': 'False', 'is_educational_requirement': 'False', 'is_interview_procedure': 'False', 'is_corporate_scope': 'False', 'is_posting_date': 'False', 'is_other': 'False'}]"
      ]
     },
     "execution_count": 280,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "def do_cypher_tx(tx, navigable_parent, verbose=False):\n",
    "    cypher_str = '''\n",
    "        MATCH (np:NavigableParents {navigable_parent: $navigable_parent})\n",
    "        SET\n",
    "            np.is_header = 'False',\n",
    "            np.is_task_scope = 'False',\n",
    "            np.is_minimum_qualification = 'True',\n",
    "            np.is_preferred_qualification = 'False',\n",
    "            np.is_educational_requirement = 'False',\n",
    "            np.is_legal_notification = 'False',\n",
    "            np.is_other = 'False',\n",
    "            np.is_corporate_scope = 'False',\n",
    "            np.is_job_title = 'False',\n",
    "            np.is_office_location = 'False',\n",
    "            np.is_job_duration = 'False',\n",
    "            np.is_supplemental_pay = 'False',\n",
    "            np.is_interview_procedure = 'False',\n",
    "            np.is_posting_date = 'False'\n",
    "        ''' + cu.return_everything_str + ';'\n",
    "    return [dict(record.items()) for record in tx.run(query=cypher_str, parameters={'navigable_parent': navigable_parent})]\n",
    "with cu.driver.session() as session: row_objs_list = session.write_transaction(do_cypher_tx, navigable_parent=child_str, verbose=False)\n",
    "ihu.retrain_classifier(row_objs_list[0]['navigable_parent'], row_objs_list[0]['is_header'], verbose=False); row_objs_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 274,
   "id": "56d17dac-be9e-4bc3-aa01-58eed1baa9af",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'navigable_parent': '<strong>Job Description</strong>', 'is_header': 'True', 'is_task_scope': 'True', 'is_minimum_qualification': 'False', 'is_preferred_qualification': 'False', 'is_legal_notification': 'False', 'is_job_title': 'False', 'is_office_location': 'False', 'is_job_duration': 'False', 'is_supplemental_pay': 'False', 'is_educational_requirement': 'False', 'is_interview_procedure': 'False', 'is_corporate_scope': 'False', 'is_posting_date': 'False', 'is_other': 'False'}]"
      ]
     },
     "execution_count": 274,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "# Show what's in the database already for this html string\n",
    "def do_cypher_tx(tx, navigable_parent, verbose=False):\n",
    "    cypher_str = '''\n",
    "        MATCH (np:NavigableParents {navigable_parent: $navigable_parent})\n",
    "        ''' + cu.return_everything_str + ';'\n",
    "    results_list = tx.run(query=cypher_str, parameters={'navigable_parent': navigable_parent})\n",
    "    return [dict(record.items()) for record in results_list]\n",
    "with cu.driver.session() as session: row_objs_list = session.write_transaction(do_cypher_tx, navigable_parent=child_str, verbose=False)\n",
    "row_objs_list"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "181dfa2e-4e09-4da7-b046-98f5c170ec67",
   "metadata": {},
   "source": [
    "\n",
    "----\n",
    "## Maintenance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "8626518c-6d69-45b9-9f9a-ebe909644d0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Break up overly-long O-RQs:\n",
    "# Ensure you have already displayed the context of an individual child string above\n",
    "# Don't close the Notepad++ window until you have replaced the child string\n",
    "display_file_in_text_editor(file_name)\n",
    "cu.rebuild_filename_node(file_name, wsu, navigable_parent=child_str, verbose=False)\n",
    "winsound.Beep(freq, duration)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c767d2cb-2c13-421a-b29e-546d2b1b401f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Remove this particular child string from the quals dictionary and database\n",
    "basic_quals_dict = s.load_object('basic_quals_dict')\n",
    "child_str = child_strs_list[idx]\n",
    "basic_quals_dict.pop(child_str, None)\n",
    "# basic_quals_dict[child_str] = 0\n",
    "s.store_objects(basic_quals_dict=basic_quals_dict)\n",
    "print(f'\"{child_str}\" in basic_quals_dict: {child_str in basic_quals_dict}')\n",
    "def do_cypher_tx(tx, qualification_str, verbose=False):\n",
    "    cypher_str = '''\n",
    "        MATCH (qs:QualificationStrings {qualification_str: $qualification_str})\n",
    "        DETACH DELETE qs;\n",
    "        '''\n",
    "    results_list = tx.run(query=cypher_str, parameters={'qualification_str': qualification_str})\n",
    "\n",
    "    return [dict(record.items()) for record in results_list]\n",
    "with cu.driver.session() as session:\n",
    "    row_objs_list = session.write_transaction(do_cypher_tx, qualification_str=child_str, verbose=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "872a9fa5-9f15-4bcc-a3df-b75dbb52ed51",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Remove file name from database\n",
    "for file_name in ['']:\n",
    "    cu.delete_filename_node(file_name, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72f2828f-ab7b-4550-b5cc-92078c40f331",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Job Hunting (Python 3.10.9)",
   "language": "python",
   "name": "jh_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
