{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "abb7318f-5285-4f7b-bc3e-2859fe05f928",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pretty printing has been turned OFF\n",
      "======== Neo4j/4.4.7 ========\n",
      "Utility libraries created in 8 seconds\n"
     ]
    }
   ],
   "source": [
    "\n",
    "%pprint\n",
    "%matplotlib inline\n",
    "import sys\n",
    "sys.path.insert(1, '../py')\n",
    "from jobpostlib import (crf, cu, datetime, duration, hau, hc, humanize, ihu, lru, nu, osp, scrfcu, slrcu, ssgdcu, su, t0, time, wsu, speech_engine)\n",
    "import os\n",
    "from pandas import DataFrame\n",
    "import re\n",
    "\n",
    "freq = 990"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fd3f331b-c66a-4c65-906b-22bc6fc034c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I have 49,102 labeled parts of speech in here\n",
      "predict_single is available\n",
      "Parts-of-speech logistic regression elements built in 6 seconds\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Check if the slrcu has built its parts-of-speech logistic regression elements\n",
    "# Parts-of-speech logistic regression elements is normally built in 1 hour, 27 minutes and 21 seconds\n",
    "t1 = time.time()\n",
    "if not hasattr(slrcu, 'pos_predict_percent_fit_dict'):\n",
    "    slrcu.build_pos_logistic_regression_elements(sampling_strategy_limit=None, verbose=True)\n",
    "if hasattr(slrcu, 'pos_predict_percent_fit_dict'): print('predict_single is available')\n",
    "else: print('predict_single is not available')\n",
    "duration_str = humanize.precisedelta(time.time() - t1, minimum_unit='seconds', format='%0.0f')\n",
    "speech_str = f'Parts-of-speech logistic regression elements built in {duration_str}'; print(speech_str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3fe80356-c69a-4579-b298-31e40e871b31",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "predict_single is now available\n",
      "Parts-of-speech conditional random field elements built in 1 second\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Check if the scrfcu has built its parts-of-speech conditional random field elements\n",
    "# Parts-of-speech CRF elements normally built in 29 minutes and 57 seconds\n",
    "t1 = time.time()\n",
    "if not hasattr(scrfcu, 'pos_symbol_crf'):\n",
    "    scrfcu.build_pos_conditional_random_field_elements(verbose=True)\n",
    "if hasattr(scrfcu, 'pos_predict_percent_fit_dict'): print('predict_single is now available')\n",
    "else: print('predict_single is not available')\n",
    "duration_str = humanize.precisedelta(time.time() - t1, minimum_unit='seconds', format='%0.0f')\n",
    "speech_str = f'Parts-of-speech conditional random field elements built in {duration_str}'; print(speech_str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3453a69d-7080-4ebf-96a8-528e9541ea19",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I have 49,102 labeled parts of speech in here\n",
      "predict_single is now available\n",
      "Parts-of-speech stochastic gradient descent elements built in 9 seconds\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Check if the ssgdcu has built its parts-of-speech stochastic gradient decent elements\n",
    "t1 = time.time()\n",
    "if not hasattr(ssgdcu, 'pos_predict_percent_fit_dict'):\n",
    "    ssgdcu.build_pos_stochastic_gradient_descent_elements(sampling_strategy_limit=None, verbose=True)\n",
    "if hasattr(ssgdcu, 'pos_predict_percent_fit_dict'): print('predict_single is now available')\n",
    "else: print('predict_single is not available')\n",
    "duration_str = humanize.precisedelta(time.time() - t1, minimum_unit='seconds', format='%0.0f')\n",
    "speech_str = f'Parts-of-speech stochastic gradient descent elements built in {duration_str}'; print(speech_str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "01e7d031-531c-4024-b9f4-03485a91a943",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "predict_single is now available\n",
      "POS classifier trained in 0 seconds\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Check if the crf has built its parts-of-speech classifier\n",
    "# POS classifier normally trained in 15 hours, 42 minutes and 41 seconds\n",
    "t1 = time.time()\n",
    "if not hasattr(crf, 'CRF'): crf.build_pos_conditional_random_field_elements(verbose=True)\n",
    "if hasattr(crf, 'CRF'): print('predict_single is now available')\n",
    "else: print('predict_single is not available')\n",
    "duration_str = humanize.precisedelta(time.time() - t1, minimum_unit='seconds', format='%0.0f')\n",
    "speech_str = f'POS classifier trained in {duration_str}'; print(speech_str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d259dffb-c212-4493-a879-1ae5d503f8ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I have 424,879 is-qualified vocabulary tokens in here\n",
      "Is-qualified LR elements built in 5 seconds\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Check if the lru has built its is-qualified classifier\n",
    "t1 = time.time()\n",
    "if not (hasattr(lru, 'ISQUALIFIED_LR') and hasattr(lru, 'ISQUALIFIED_CV')):\n",
    "    lru.build_isqualified_logistic_regression_elements(sampling_strategy_limit=None, verbose=True)\n",
    "duration_str = humanize.precisedelta(time.time() - t1, minimum_unit='seconds', format='%0.0f')\n",
    "speech_str = f'Is-qualified LR elements built in {duration_str}'; print(speech_str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a9d67a34-e133-45dc-9bf3-a830af359d77",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I have 49,902 hand-labeled header htmls prepared\n",
      "7 iterations seen during training fit for a total of 49,902 records trained\n",
      "Is-header classifier trained in 6 seconds\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Train the isheader classifier\n",
    "t1 = time.time()\n",
    "ihu.build_pos_stochastic_gradient_descent_elements(verbose=True)\n",
    "duration_str = humanize.precisedelta(time.time() - t1, minimum_unit='seconds', format='%0.0f')\n",
    "speech_str = f'Is-header classifier trained in {duration_str}'; print(speech_str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "dcef1d94-4466-4100-86cd-0d66f23d0d40",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Last run on 2024-06-08 07:27:57.965600\n"
     ]
    }
   ],
   "source": [
    "\n",
    "speech_str = f'Last run on {datetime.now()}'; print(speech_str)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f438163-0f14-4a74-978c-769c9dba95c8",
   "metadata": {},
   "source": [
    "\n",
    "---\n",
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "f02bfeac-67b7-4df6-a4a0-32ec3dec0022",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I have 16,991 hand-labeled qualification strings in here\n",
      "I have 544,908 is-qualified vocabulary tokens in here\n",
      "Is-qualified classifer retrained in 28 seconds\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# You need to run this again if you changed the\n",
    "# qualification dictionary below or in another notebook\n",
    "t1 = time.time()\n",
    "\n",
    "# Keep the total retraining time to less than two minutes by adjusting the sampling strategy limit\n",
    "lru.sync_basic_quals_dict(sampling_strategy_limit=None, verbose=False)\n",
    "\n",
    "lru.retrain_isqualified_classifier(verbose=True)\n",
    "duration_str = humanize.precisedelta(time.time() - t1, minimum_unit='seconds', format='%0.0f')\n",
    "speech_str = f'Is-qualified classifer retrained in {duration_str}'; print(speech_str)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52ad3579-ebc3-4d21-a7c3-f9bd168ac6ca",
   "metadata": {},
   "source": [
    "\n",
    "---\n",
    "# Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "d334d815-e43b-474e-9dbb-db509176f6e5",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Basic Qualifications for Senior Applied Scientist Remote:\n",
      "*quals_list[0] = \"<li>You can use your grit and initiative to fill in gaps and drive projects to completion.</li>\" (1.0)\n",
      "*quals_list[1] = \"<li>You can effectively communicate with applied scientists, data scientists, engineers and product managers.</li>\" (1.0)\n",
      "*quals_list[2] = \"<li>You can evaluate, analyze and interpret model results resulting in further improvement of existing statistical model performance</li>\" (1.0)\n",
      "*quals_list[3] = \"<li>You can write production code in Python and can reason about algorithmic efficiency.</li>\" (1.0)\n",
      "*quals_list[4] = \"<li>You have at least two years experience as an applied scientist and preferably one year of experience within the Ad Tech ecosystem.</li>\" (1.0)\n",
      "*quals_list[5] = \"<p>At Sojern, we work to create a brave space that seeks out, embraces, and promotes diverse thoughts, beliefs, and experiences of Sojernistas globally.</p>\" (1.0)\n",
      "*quals_list[6] = \"<li>You can design and apply machine learning models (supervised, unsupervised, reinforcement learning, causal inference and uplift, etc) to solve business problems.</li>\" (1.0)\n",
      "*quals_list[7] = \"<li>You partner with product managers to improve requirements.</li>\" (1.0)\n",
      "*quals_list[8] = \"Supporting, developing, and empowering those who identify as women at Sojern\" (1.0)\n",
      "*quals_list[9] = \"<li>You can perform complex data analysis using SQL/Jupyter notebook to find underlying issues and propose a solution to stakeholders explaining the various trade-offs associated with the solution.</li>\" (1.0)\n",
      "*quals_list[10] = \"<li>Bayesian statistics and supervised learning to model conversion rates and revenue</li>\" (1.0)\n",
      "*quals_list[11] = \"<li>You have an MS or PhD in computer science or a quantitative discipline, or equivalent experience.</li>\" (1.0)\n",
      "*quals_list[12] = \"<li>You mentor and participate in code/design reviews.</li>\" (1.0)\n",
      "*quals_list[13] = \"<li>Thompson sampling and linear programming to balance campaign objectives</li>\" (1.0)\n",
      "*quals_list[14] = \"<li>You have hands-on experience building production-level ML and data products.</li>\" (0.0)\n",
      "*quals_list[15] = \"<li>You can work independently to solve data challenges with data science, machine learning and algorithms.</li>\" (1.0)\n",
      "*quals_list[16] = \"Advocating for balance in life and overall wellbeing and providing space and resources for Sojernistas with different needs\" (1.0)\n",
      "*quals_list[17] = \"Home office tech set up (laptop, monitor, keyboard, mouse), monthly internet and phone allowance, modern tools to communicate and collaborate (Slack, Google Suite)\" (1.0)\n",
      "*quals_list[18] = \"<p>And as a part of a broader engineering team, we deliver through the full data product life-cycle: from prototyping to production component development\" (0.0)\n",
      "*quals_list[19] = \"<li>You drive technical impact by making meaningful, well-designed, reusable, efficient contributions to data-driven products.</li>\" (1.0)\n",
      "*quals_list[20] = \"Creating space, connection and resources for Sojern's young and rising professionals\" (1.0)\n",
      "*quals_list[21] = \"<li>You innovate in the context of trade-offs.</li>\" (1.0)\n",
      "*quals_list[22] = \"This provides each team member the opportunity to gain additional experience with a range of technologies such as Tensorflow, Kubeflow, Docker and the Google Cloud Platform\" (1.0)\n",
      "*quals_list[23] = \"<p>#LI-BP1</p>\" (0.0288)\n",
      "*quals_list[24] = \"<li>Scaled AI/ML with Tensorflow/Kubeflow to model a traveler's propensity to book</li>\" (1.0)\n",
      "*quals_list[25] = \"<li>You have expert-level understanding of data science topics.</li>\" (1.0)\n",
      "88.46%\n",
      "\n",
      "hunting_df.loc[3929, 'percent_fit'] = (000+000+000+000+000+000+000+000+000+000+000+000+000+000+000+000+000+000+000+000+000+000+000+000+000+000)/26\n",
      "3928/3950 = 99.4% completed\n",
      "Inference completed in 2 minutes and 19 seconds\n"
     ]
    }
   ],
   "source": [
    "\n",
    "t1 = time.time()\n",
    "\n",
    "# Loop through all the unset %fit values, set them if you can, break for help if you can't\n",
    "quals_list, file_name = lru.infer_from_hunting_dataframe(fitness_threshold=3/4, verbose=True)\n",
    "duration_str = humanize.precisedelta(time.time() - t1, minimum_unit='seconds', format='%0.0f')\n",
    "speech_str = f'Inference completed in {duration_str}'; print(speech_str); speech_engine.say(speech_str); speech_engine.runAndWait()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63264da9-f96a-491a-83d9-25d15fcdfef6",
   "metadata": {},
   "outputs": [],
   "source": [
    "raise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "47bd03b8-c0a3-404c-a16d-eb4ce95c6561",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<li>You can effectively communicate with applied scientists, data scientists, engineers and product managers.</li>\n",
      "Pickling to C:\\Users\\daveb\\OneDrive\\Documents\\GitHub\\job-hunting\\saves\\pkl\\basic_quals_dict.pkl\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Manually label the unscored qualification string\n",
    "qualification_str = quals_list[1]\n",
    "print(qualification_str); basic_quals_dict = nu.load_object('basic_quals_dict')\n",
    "print(str(basic_quals_dict[qualification_str]) + '\\n' if(qualification_str in basic_quals_dict) else '', end='')\n",
    "basic_quals_dict[qualification_str] = 1\n",
    "nu.store_objects(basic_quals_dict=basic_quals_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89c4212b-9280-46d1-8e46-104aada7c0df",
   "metadata": {},
   "source": [
    "\n",
    "----\n",
    "## Fix Parts-of-Speech and Quals for this posting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "8ac6efb8-6b56-43dd-9079-d6a87491ecc5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "b4a86d743934facf_Senior_Machine_Learning_Engineer_London_Indeed_com.html\n",
      "CRF and child strings list recreated in 48 seconds\n"
     ]
    }
   ],
   "source": [
    "\n",
    "t1 = time.time()\n",
    "file_path = os.path.join(cu.SAVES_HTML_FOLDER, file_name)\n",
    "if os.path.isfile(file_path):\n",
    "    child_strs_list = hau.get_child_strs_from_file(file_name=file_name)\n",
    "    cu.ensure_filename(file_name, verbose=False)\n",
    "    cu.populate_from_child_strings(child_strs_list, file_name, verbose=False)\n",
    "    print(file_name)\n",
    "    child_tags_list = hau.get_child_tags_list(child_strs_list)\n",
    "    feature_dict_list = cu.get_feature_dict_list(child_tags_list, child_strs_list)\n",
    "    feature_tuple_list = []\n",
    "    for feature_dict in feature_dict_list:\n",
    "        feature_tuple_list.append(hc.get_feature_tuple(\n",
    "            feature_dict, pos_lr_predict_single=slrcu.predict_single, pos_crf_predict_single=scrfcu.predict_single,\n",
    "            pos_sgd_predict_single=ssgdcu.predict_single\n",
    "        ))\n",
    "    crf_list = crf.CRF.predict_single(crf.sent2features(feature_tuple_list))\n",
    "duration_str = humanize.precisedelta(time.time() - t1, minimum_unit='seconds', format='%0.0f')\n",
    "speech_str = f'CRF and child strings list recreated in {duration_str}'; print(speech_str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "affcb06a-3165-4349-a936-037af549d417",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['H-OL', 'O-RQ', 'H-TS', 'O-TS', 'H-TS', 'H-TS', 'O-RQ', 'O-TS', 'O-TS', 'H-TS', 'O-TS', 'O-RQ', 'H-RQ', 'O-RQ', 'O-RQ', 'O-RQ', 'O-RQ', 'O-PQ', 'O-RQ', 'O-RQ', 'O-RQ', 'H-ER', 'O-RQ', 'H-PQ', 'O-PQ', 'O-PQ', 'H-LN', 'O-LN']\n",
      "[1, 6, 11, 13, 14, 15, 16, 18, 19, 20, 22]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "0 H-OL) <span style=\"color:#c49c94ff;\"><b>Office Location: (H-OL Office Location Header)</b></span><br /><hr />1 O-RQ) <span style=\"color:#bcbd2280;\"><li>Remote - UK (O-RQ Required Qualifications Non-header)</li></span><br />2 H-TS) <span style=\"color:#9edae5ff;\"><b>Task Scope: (H-TS Task Scope Header)</b></span><br />3 O-TS) <span style=\"color:#9edae580;\"><li>About the role: We are seeking a talented Senior Machine Learning Engineer to join our Integrity Team, focused on enhancing the safety and reliability of our systems across various content modalities. (O-TS Task Scope Non-header)</li></span><br />4 H-TS) <span style=\"color:#9edae5ff;\"><li>About the role: In this role, you will leverage your expertise to lead, design, build, and implement critical solutions that uphold our high standards of AI safety and integrity. (H-TS Task Scope Header)</li></span><br />5 H-TS) <span style=\"color:#9edae5ff;\"><li>About the role: Your responsibilities will also include working closely with cross-functional teams to ensure that these systems integrate seamlessly and support the broader goals of our organization. (H-TS Task Scope Header)</li></span><br />6 O-RQ) <span style=\"color:#bcbd2280;\"><li>About the role: You will have access to state-of-the-art high performance computing resources and you will be able to work alongside top researchers and engineers to truly make an impact in the fast growing world of generative AI. (O-RQ Required Qualifications Non-header)</li></span><br />7 O-TS) <span style=\"color:#9edae580;\"><li>Responsibilities: Lead efforts to drive the design development and production of ML systems, and present the solutions to the leadership teams and customers (O-TS Task Scope Non-header)</li></span><br />8 O-TS) <span style=\"color:#9edae580;\"><li>Responsibilities: Work with the Research team on developing the next generation of safe models, where you may assist with areas such as optimization of model training, model tuning, dataset engineering, HPC clusters, tooling, and work on open-source efforts. (O-TS Task Scope Non-header)</li></span><br />9 H-TS) <span style=\"color:#9edae5ff;\"><li>Responsibilities: Be a strategic thought partner for leaders across the organization on driving business impact through machine learning (H-TS Task Scope Header)</li></span><br />10 O-TS) <span style=\"color:#9edae580;\"><li>Responsibilities: Prototype and productionize model architecture improvements and new features enhancing our overall safety valves. (O-TS Task Scope Non-header)</li></span><br />11 O-RQ) <span style=\"color:#bcbd2280;\"><li>Responsibilities: Writing and maintaining production-quality code, ensuring robustness, efficiency, and scalability to support the safety of our models in a live environment. (O-RQ Required Qualifications Non-header)</li></span><br />12 H-RQ) <span style=\"color:#bcbd22ff;\"><b>Minimum Qualifications: (H-RQ Required Qualifications Header)</b></span><br />13 O-RQ) <span style=\"color:#bcbd2280;\"><li>About the role: The ideal candidate will excel at tackling complex challenges, is deeply committed to developing integrity based machine learning models and is comfortable working at a fast pace on a variety of tasks. (O-RQ Required Qualifications Non-header)</li></span><br />14 O-RQ) <span style=\"color:#bcbd2280;\"><li>About the role: We want highly creative ML engineers who are motivated to push the boundaries of generative models. (O-RQ Required Qualifications Non-header)</li></span><br />15 O-RQ) <span style=\"color:#bcbd2280;\"><li>Qualifications: Experience with Python scientific stack, PyTorch, creating Jupyter/Colab notebooks (O-RQ Required Qualifications Non-header)</li></span><br />16 O-RQ) <span style=\"color:#bcbd2280;\"><li>Qualifications: Ability to communicate machine learning concepts and results effectively through writing and visualization (O-RQ Required Qualifications Non-header)</li></span><br />17 O-PQ) <span style=\"color:#c7c7c780;\"><li>Qualifications: Experience with building interactive web demos that serve generative ML models (O-PQ Preferred Qualifications Non-header)</li></span><br />18 O-RQ) <span style=\"color:#bcbd2280;\"><li>Qualifications: Experience with the open-source ML ecosystem (HuggingFace, W&amp;B, etc.) (O-RQ Required Qualifications Non-header)</li></span><br />19 O-RQ) <span style=\"color:#bcbd2280;\"><li>Qualifications: Excellent communication skills with a proven ability to articulate technical challenges and solutions to varied audiences. (O-RQ Required Qualifications Non-header)</li></span><br />20 O-RQ) <span style=\"color:#bcbd2280;\"><li>Qualifications: Strong problem-solving ability and the ability to work in ambiguous and constantly evolving environment (O-RQ Required Qualifications Non-header)</li></span><br />21 H-ER) <span style=\"color:#aec7e8ff;\"><b>Educational Requirements: (H-ER Education Requirements Header)</b></span><br />22 O-RQ) <span style=\"color:#bcbd2280;\"><li>Qualifications: Masters or PhD in computer science with 5+ years working on machine learning projects, including training, fine tuning and refining models (O-RQ Required Qualifications Non-header)</li></span><br /><hr />23 H-PQ) <span style=\"color:#c7c7c7ff;\"><b>Preferred Qualifications: (H-PQ Preferred Qualifications Header)</b></span><br />24 O-PQ) <span style=\"color:#c7c7c780;\"><li>Qualifications: Experience with JAX / TPUs / CUDA-level / JavaScript (TensorFlow.js etc) a plus (O-PQ Preferred Qualifications Non-header)</li></span><br />25 O-PQ) <span style=\"color:#c7c7c780;\"><li>Qualifications: Experience working on Trust &amp; Safety / Integrity related areas is a plus (O-PQ Preferred Qualifications Non-header)</li></span><br />26 H-LN) <span style=\"color:#9467bdff;\"><b>Legal Notifications: (H-LN Legal Notifications Header)</b></span><br />27 O-LN) <span style=\"color:#9467bd80;\"><li>Equal Employment Opportunity: We are an equal opportunity employer and do not discriminate on the basis of race, religion, national origin, gender, sexual orientation, age, veteran status, disability or other legally protected statuses. (O-LN Legal Notifications Non-header)</li></span><br />"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 6, 11, 13, 14, 15, 16, 18, 19, 20, 22]\n",
      "Parts-of-speech displayed in 51 seconds\n"
     ]
    }
   ],
   "source": [
    "\n",
    "db_pos_list = []\n",
    "for navigable_parent in child_strs_list:\n",
    "    db_pos_list = cu.append_parts_of_speech_list(navigable_parent, pos_list=db_pos_list)\n",
    "pos_list, indices_list = su.visualize_basic_quals_section(crf_list, child_strs_list, db_pos_list=db_pos_list, verbose=True)\n",
    "duration_str = humanize.precisedelta(time.time() - t1, minimum_unit='seconds', format='%0.0f')\n",
    "speech_str = f'Parts-of-speech displayed in {duration_str}'; print(speech_str); speech_engine.say(speech_str); speech_engine.runAndWait()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91379a3d-1a77-4d8b-9ef6-a76f12204ac2",
   "metadata": {},
   "outputs": [],
   "source": [
    "raise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "26c4dd4e-1418-4420-aff5-ea7b65273194",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23 H-PQ) <b>Preferred Qualifications:</b>\n",
      "24 O-PQ) <li>Qualifications: Experience with JAX / TPUs / CUDA-level / JavaScript (TensorFlow.js etc) a plus</li>\n",
      "25 O-PQ) <li>Qualifications: Experience working on Trust &amp; Safety / Integrity related areas is a plus</li>\n"
     ]
    }
   ],
   "source": [
    "\n",
    "basic_quals_dict = nu.load_object('basic_quals_dict'); column_name = 'is_preferred_qualification'\n",
    "for idx in list(range(23, 26)):\n",
    "    child_str = child_strs_list[idx]; pos_symbol = pos_list[idx]\n",
    "    def do_cypher_tx(tx, navigable_parent, verbose=False):\n",
    "        cypher_str = '''\\n            MATCH (np:NavigableParents {navigable_parent: $navigable_parent})\\n            ''' + cu.return_everything_str + ';'\n",
    "        results_list = tx.run(query=cypher_str, parameters={'navigable_parent': navigable_parent})\n",
    "        return [dict(record.items()) for record in results_list]\n",
    "    with cu.driver.session() as session: row_objs_list = session.write_transaction(do_cypher_tx, navigable_parent=child_str, verbose=False)\n",
    "    print(str(basic_quals_dict[child_str]) + '\\n' if(child_str in basic_quals_dict) else '', end='')\n",
    "    print(f'{idx} {pos_symbol}) {child_str}')\n",
    "    def do_cypher_tx(tx, navigable_parent, verbose=False):\n",
    "        cypher_str = f'''\n",
    "            MATCH (np:NavigableParents {{navigable_parent: $navigable_parent}})\n",
    "            SET\n",
    "                np.is_job_title = {str(column_name == 'is_job_title').lower()},\n",
    "                np.is_corporate_scope = {str(column_name == 'is_corporate_scope').lower()},\n",
    "                np.is_task_scope = {str(column_name == 'is_task_scope').lower()},\n",
    "                np.is_minimum_qualification = {str(column_name == 'is_minimum_qualification').lower()},\n",
    "                np.is_preferred_qualification = {str(column_name == 'is_preferred_qualification').lower()},\n",
    "                np.is_educational_requirement = {str(column_name == 'is_educational_requirement').lower()},\n",
    "                np.is_supplemental_pay = {str(column_name == 'is_supplemental_pay').lower()},\n",
    "                np.is_legal_notification = {str(column_name == 'is_legal_notification').lower()},\n",
    "                np.is_office_location = {str(column_name == 'is_office_location').lower()},\n",
    "                np.is_job_duration = {str(column_name == 'is_job_duration').lower()},\n",
    "                np.is_interview_procedure = {str(column_name == 'is_interview_procedure').lower()},\n",
    "                np.is_other = {str(column_name == 'is_other').lower()},\n",
    "                np.is_posting_date = {str(column_name == 'is_posting_date').lower()}\n",
    "            ''' + cu.return_everything_str + ';'\n",
    "        return [dict(record.items()) for record in tx.run(query=cypher_str, parameters={'navigable_parent': navigable_parent})]\n",
    "    with cu.driver.session() as session: row_objs_list = session.write_transaction(do_cypher_tx, navigable_parent=child_str, verbose=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "1d97acb2-2b60-4a56-9b3b-ecfc2b6582e6",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 6, 11, 13, 14, 15, 16, 18, 19, 20, 22]\n",
      "25 O-PQ) <li>Qualifications: Experience working on Trust &amp; Safety / Integrity related areas is a plus</li>\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Display the context of an individual child string\n",
    "idx = 25\n",
    "print(indices_list); child_str = child_strs_list[idx]; pos_symbol = pos_list[idx]; basic_quals_dict = nu.load_object('basic_quals_dict')\n",
    "print(str(basic_quals_dict[child_str]) + '\\n' if(child_str in basic_quals_dict) else '', end='')\n",
    "print(f'{idx} {pos_symbol}) {child_str}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "b2f46926-880b-4aed-bc99-4a6bd9ed7903",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pickling to C:\\Users\\daveb\\OneDrive\\Documents\\GitHub\\job-hunting\\saves\\pkl\\basic_quals_dict.pkl\n",
      "\"<li>Qualifications: Experience working on Trust &amp; Safety / Integrity related areas is a plus</li>\" in basic_quals_dict: 0\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Hand-label this particular child string in the quals dictionary\n",
    "basic_quals_dict = nu.load_object('basic_quals_dict')\n",
    "basic_quals_dict[child_str] = 0\n",
    "nu.store_objects(basic_quals_dict=basic_quals_dict); print(f'\"{child_str}\" in basic_quals_dict: {basic_quals_dict[child_str]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "326ab607-cc5c-4476-b692-395b96d293ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14 O-RQ) Competencies:\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Fix Headers\n",
    "basic_quals_dict = nu.load_object('basic_quals_dict')\n",
    "for idx in [14]:\n",
    "    child_str = child_strs_list[idx]; pos_symbol = pos_list[idx]\n",
    "    print(str(basic_quals_dict[child_str]) + '\\n' if(child_str in basic_quals_dict) else '', end='')\n",
    "    print(f'{idx} {pos_symbol}) {child_str}')\n",
    "    def do_cypher_tx(tx, navigable_parent, verbose=False):\n",
    "        cypher_str = f'''\n",
    "            MATCH (np:NavigableParents {{navigable_parent: $navigable_parent}})\n",
    "            SET\n",
    "                np.is_header = true\n",
    "            ''' + cu.return_everything_str + ';'\n",
    "        return [dict(record.items()) for record in tx.run(query=cypher_str, parameters={'navigable_parent': navigable_parent})]\n",
    "    with cu.driver.session() as session: row_objs_list = session.write_transaction(do_cypher_tx, navigable_parent=child_str, verbose=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "24e7695d-caf6-416f-a761-ec17b839ccf9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "39 H-RQ) <li>Qualifications: You have: Experience in a high-growth tech environment</li>\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Fix Non-headers\n",
    "basic_quals_dict = nu.load_object('basic_quals_dict')\n",
    "for idx in [39]:\n",
    "    child_str = child_strs_list[idx]; pos_symbol = pos_list[idx]\n",
    "    print(str(basic_quals_dict[child_str]) + '\\n' if(child_str in basic_quals_dict) else '', end='')\n",
    "    print(f'{idx} {pos_symbol}) {child_str}')\n",
    "    def do_cypher_tx(tx, navigable_parent, verbose=False):\n",
    "        cypher_str = f'''\n",
    "            MATCH (np:NavigableParents {{navigable_parent: $navigable_parent}})\n",
    "            SET\n",
    "                np.is_header = false\n",
    "            ''' + cu.return_everything_str + ';'\n",
    "        return [dict(record.items()) for record in tx.run(query=cypher_str, parameters={'navigable_parent': navigable_parent})]\n",
    "    with cu.driver.session() as session: row_objs_list = session.write_transaction(do_cypher_tx, navigable_parent=child_str, verbose=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "id": "4005f4a8-f07c-4538-8d1b-210d32dd3199",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'navigable_parent': '<b>Educational Requirements:</b>', 'is_header': True, 'is_task_scope': False, 'is_minimum_qualification': False, 'is_preferred_qualification': False, 'is_legal_notification': False, 'is_job_title': False, 'is_office_location': False, 'is_job_duration': False, 'is_supplemental_pay': False, 'is_educational_requirement': True, 'is_interview_procedure': False, 'is_corporate_scope': False, 'is_posting_date': False, 'is_other': False}]"
      ]
     },
     "execution_count": 177,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "# Manually set each feature\n",
    "def do_cypher_tx(tx, navigable_parent, verbose=False):\n",
    "    cypher_str = '''\n",
    "        MATCH (np:NavigableParents {navigable_parent: $navigable_parent})\n",
    "        SET\n",
    "            np.is_header = true,\n",
    "            np.is_task_scope = false,\n",
    "            np.is_minimum_qualification = false,\n",
    "            np.is_preferred_qualification = false,\n",
    "            np.is_educational_requirement = true,\n",
    "            np.is_legal_notification = false,\n",
    "            np.is_other = false,\n",
    "            np.is_corporate_scope = false,\n",
    "            np.is_job_title = false,\n",
    "            np.is_office_location = false,\n",
    "            np.is_job_duration = false,\n",
    "            np.is_supplemental_pay = false,\n",
    "            np.is_interview_procedure = false,\n",
    "            np.is_posting_date = false\n",
    "        ''' + cu.return_everything_str + ';'\n",
    "    return [dict(record.items()) for record in tx.run(query=cypher_str, parameters={'navigable_parent': navigable_parent})]\n",
    "with cu.driver.session() as session: row_objs_list = session.write_transaction(do_cypher_tx, navigable_parent=child_str, verbose=False)\n",
    "ihu.retrain_classifier(row_objs_list[0]['navigable_parent'], row_objs_list[0]['is_header'], verbose=False); row_objs_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b2c062a-8ec0-4daf-9727-6b8676c26fe2",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Set all the questions to interview procedures\n",
    "basic_quals_dict = nu.load_object('basic_quals_dict')\n",
    "column_name = 'is_interview_procedure'\n",
    "for idx in range(len(child_strs_list)):\n",
    "    child_str = child_strs_list[idx]; pos_symbol = pos_list[idx]\n",
    "    if '?' in child_str:\n",
    "        def do_cypher_tx(tx, navigable_parent, verbose=False):\n",
    "            cypher_str = '''\n",
    "                MATCH (np:NavigableParents {navigable_parent: $navigable_parent})\n",
    "                ''' + cu.return_everything_str + ';'\n",
    "            results_list = tx.run(query=cypher_str, parameters={'navigable_parent': navigable_parent})\n",
    "            return [dict(record.items()) for record in results_list]\n",
    "        with cu.driver.session() as session: row_objs_list = session.write_transaction(do_cypher_tx, navigable_parent=child_str, verbose=False)\n",
    "        if (row_objs_list[0][column_name] != True):\n",
    "            print(str(basic_quals_dict[child_str]) + '\\n' if(child_str in basic_quals_dict) else '', end='')\n",
    "            print(f'{idx} {pos_symbol}) {child_str}')\n",
    "            def do_cypher_tx(tx, navigable_parent, verbose=False):\n",
    "                cypher_str = f'''\n",
    "                    MATCH (np:NavigableParents {{navigable_parent: $navigable_parent}})\n",
    "                    SET\n",
    "                        np.is_job_title = {str(column_name == 'is_job_title').lower()},\n",
    "                        np.is_corporate_scope = {str(column_name == 'is_corporate_scope').lower()},\n",
    "                        np.is_task_scope = {str(column_name == 'is_task_scope').lower()},\n",
    "                        np.is_educational_requirement = {str(column_name == 'is_educational_requirement').lower()},\n",
    "                        np.is_minimum_qualification = {str(column_name == 'is_minimum_qualification').lower()},\n",
    "                        np.is_preferred_qualification = {str(column_name == 'is_preferred_qualification').lower()},\n",
    "                        np.is_supplemental_pay = {str(column_name == 'is_supplemental_pay').lower()},\n",
    "                        np.is_legal_notification = {str(column_name == 'is_legal_notification').lower()},\n",
    "                        np.is_office_location = {str(column_name == 'is_office_location').lower()},\n",
    "                        np.is_job_duration = {str(column_name == 'is_job_duration').lower()},\n",
    "                        np.is_interview_procedure = {str(column_name == 'is_interview_procedure').lower()},\n",
    "                        np.is_other = {str(column_name == 'is_other').lower()},\n",
    "                        np.is_posting_date = {str(column_name == 'is_posting_date').lower()}\n",
    "                    ''' + cu.return_everything_str + ';'\n",
    "                return [dict(record.items()) for record in tx.run(query=cypher_str, parameters={'navigable_parent': navigable_parent})]\n",
    "            with cu.driver.session() as session: row_objs_list = session.write_transaction(do_cypher_tx, navigable_parent=child_str, verbose=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56d17dac-be9e-4bc3-aa01-58eed1baa9af",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Show what's in the database already for this html string\n",
    "def do_cypher_tx(tx, navigable_parent, verbose=False):\n",
    "    cypher_str = '''\n",
    "        MATCH (np:NavigableParents {navigable_parent: $navigable_parent})\n",
    "        ''' + cu.return_everything_str + ';'\n",
    "    results_list = tx.run(query=cypher_str, parameters={'navigable_parent': navigable_parent})\n",
    "    return [dict(record.items()) for record in results_list]\n",
    "with cu.driver.session() as session: row_objs_list = session.write_transaction(do_cypher_tx, navigable_parent=child_str, verbose=False)\n",
    "row_objs_list"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "181dfa2e-4e09-4da7-b046-98f5c170ec67",
   "metadata": {},
   "source": [
    "\n",
    "----\n",
    "## Maintenance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "07255146-70be-4d52-942c-1505d6e02a9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Display cypher necessary to apply for all the jobs you qualify for that you haven't yet\n",
    "import pyperclip\n",
    "\n",
    "# lru.display_hunting_dataframe_as_histogram()\n",
    "cypher_str = f'''\n",
    "    MATCH (fn:FileNames)\n",
    "    WHERE\n",
    "        (fn.percent_fit >= 0.8) AND\n",
    "        ((fn.is_closed IS NULL) OR (fn.is_closed = false)) AND\n",
    "        ((fn.is_opportunity_application_emailed IS NULL) OR (fn.is_opportunity_application_emailed = false))\n",
    "    RETURN\n",
    "        fn.percent_fit AS percent_fit,\n",
    "        fn.file_name AS file_name,\n",
    "        fn.posting_url AS posting_url\n",
    "    ORDER BY fn.percent_fit DESC;'''\n",
    "pyperclip.copy(cypher_str)\n",
    "row_objs_list = []\n",
    "with cu.driver.session() as session: row_objs_list = session.write_transaction(cu.do_cypher_tx, cypher_str)\n",
    "if row_objs_list:\n",
    "    df = DataFrame(row_objs_list)\n",
    "    display(df)\n",
    "    \n",
    "    for file_name in df.file_name:\n",
    "        print(f\"\"\"\n",
    "MATCH (fn:FileNames)\n",
    "WHERE fn.file_name IN [\"{file_name}\"]\n",
    "SET fn.is_closed = true\n",
    "RETURN fn;\"\"\")\n",
    "        break\n",
    "    \n",
    "    for file_name in df.file_name:\n",
    "        print(f\"\"\"\n",
    "MATCH (fn:FileNames)\n",
    "WHERE fn.file_name IN [\"{file_name}\"]\n",
    "SET fn.is_opportunity_application_emailed = true, fn.opportunity_application_email_date = date()\n",
    "RETURN fn;\"\"\")\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "8626518c-6d69-45b9-9f9a-ebe909644d0d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "                MATCH\n",
      "                    (np:NavigableParents {navigable_parent: \"<li>Equal Employment Opportunity: We are an equal opportunity employer and do not discriminate on the basis of race, religion, national origin, gender, sexual orientation, age, veteran status, disability or other legally protected statuses.</li>\"}),\n",
      "                    (ht:HeaderTags {header_tag: \"li\"})\n",
      "                MERGE (ht)-[r:SUMMARIZES]->(np);\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Break up overly-long O-RQs:\n",
    "# Ensure you have already displayed the context of an individual child string above\n",
    "# Don't close the Notepad++ window until you have replaced the child string\n",
    "def display_file_in_text_editor(file_name):\n",
    "    text_editor_path = r'C:\\Program Files\\Notepad++\\notepad++.exe'\n",
    "    file_path = osp.abspath(osp.join(hau.SAVES_HTML_FOLDER, file_name))\n",
    "    !\"{text_editor_path}\" \"{file_path}\"\n",
    "display_file_in_text_editor(file_name)\n",
    "cu.rebuild_filename_node(file_name, navigable_parent=None, verbose=True)\n",
    "speech_str = 'File name node rebuild completed'; speech_engine.say(speech_str); speech_engine.runAndWait()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "c767d2cb-2c13-421a-b29e-546d2b1b401f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pickling to C:\\Users\\daveb\\OneDrive\\Documents\\GitHub\\job-hunting\\saves\\pkl\\basic_quals_dict.pkl\n",
      "\"<li>Professional development assistance</li>\" in basic_quals_dict: False\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Remove this particular child string from the quals dictionary and database\n",
    "basic_quals_dict = nu.load_object('basic_quals_dict')\n",
    "child_str = child_strs_list[idx]\n",
    "basic_quals_dict.pop(child_str, None)\n",
    "# basic_quals_dict[child_str] = 0\n",
    "nu.store_objects(basic_quals_dict=basic_quals_dict)\n",
    "print(f'\"{child_str}\" in basic_quals_dict: {child_str in basic_quals_dict}')\n",
    "def do_cypher_tx(tx, qualification_str, verbose=False):\n",
    "    cypher_str = '''\n",
    "        MATCH (qs:QualificationStrings {qualification_str: $qualification_str})\n",
    "        DETACH DELETE qs;\n",
    "        '''\n",
    "    results_list = tx.run(query=cypher_str, parameters={'qualification_str': qualification_str})\n",
    "\n",
    "    return [dict(record.items()) for record in results_list]\n",
    "with cu.driver.session() as session:\n",
    "    row_objs_list = session.write_transaction(do_cypher_tx, qualification_str=child_str, verbose=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "872a9fa5-9f15-4bcc-a3df-b75dbb52ed51",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Remove file name from database\n",
    "for file_name in ['']:\n",
    "    cu.delete_filename_node(file_name, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd3aad62-d78e-4243-b46f-dec293d4552d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import re\n",
    "\n",
    "def separate_qualifications(quals_list):\n",
    "  \"\"\"\n",
    "  This function takes a list of qualifications and separates them into individual sentences.\n",
    "\n",
    "  Args:\n",
    "      quals_list: A list of strings, where each string represents a qualification.\n",
    "\n",
    "  Returns:\n",
    "      A list of lists, where each inner list contains the individual qualifications extracted from the corresponding element in the original quals_list.\n",
    "  \"\"\"\n",
    "  separated_quals = []\n",
    "  for qual in quals_list:\n",
    "    # Split qualifications based on commas, semicolons, and colons followed by whitespace\n",
    "    split_quals = re.split(r\", |; |:\", qual)\n",
    "    # Further split based on \"and\" if it's not the first word and there's punctuation before it\n",
    "    for i, sentence in enumerate(split_quals):\n",
    "      if i > 0 and re.search(r\"\\b(and)\\b\", sentence, flags=re.IGNORECASE) and re.search(r\"[,\\.;]\", split_quals[i-1]):\n",
    "        split_quals[i-1] += f\" {sentence.strip()}\"\n",
    "        split_quals.pop(i)\n",
    "    # Remove empty strings and leading/trailing whitespace\n",
    "    separated_quals.append([q.strip() for q in split_quals if q.strip()])\n",
    "  return separated_quals\n",
    "\n",
    "separated_qualifications = separate_qualifications(quals_list)\n",
    "\n",
    "# Print the separated qualifications\n",
    "for qual_set in separated_qualifications:\n",
    "  for qual in qual_set:\n",
    "    print(qual)\n",
    "  print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72f2828f-ab7b-4550-b5cc-92078c40f331",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Job Hunting (Python 3.10.9)",
   "language": "python",
   "name": "jh_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
