{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "abb7318f-5285-4f7b-bc3e-2859fe05f928",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pretty printing has been turned OFF\n",
      "======== Neo4j/5.24.2 ========\n",
      "Utility libraries created in 18 seconds\n"
     ]
    }
   ],
   "source": [
    "\n",
    "%pprint\n",
    "%matplotlib inline\n",
    "import sys\n",
    "import os.path as osp\n",
    "\n",
    "executable_path = sys.executable; scripts_folder = osp.join(osp.dirname(executable_path), 'Scripts')\n",
    "py_folder = osp.abspath('../py'); ffmpeg_folder = r'C:\\ffmpeg\\bin'\n",
    "if (scripts_folder not in sys.path): sys.path.insert(1, scripts_folder)\n",
    "if (py_folder not in sys.path): sys.path.insert(1, py_folder)\n",
    "if (ffmpeg_folder not in sys.path): sys.path.insert(1, ffmpeg_folder)\n",
    "from jobpostlib import (crf, cu, datetime, duration, hau, hc, humanize, ihu, lru, nu, osp, scrfcu, slrcu, ssgdcu, su, t0, time, wsu, speech_engine)\n",
    "from pandas import DataFrame\n",
    "import re\n",
    "import pyperclip\n",
    "import ipywidgets as widgets\n",
    "from IPython.display import display"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fd3f331b-c66a-4c65-906b-22bc6fc034c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I have 48,129 labeled parts of speech in here\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train the POS Classifiers: 100%|█████████████████████████████████████████████████████████████████████████████████| 25/25 [00:00<00:00, 265.81it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "predict_single is available\n",
      "Parts-of-speech logistic regression elements built in 9 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Check if the slrcu has built its parts-of-speech logistic regression elements\n",
    "# Parts-of-speech logistic regression elements is normally built in 1 hour, 55 minutes and 45 seconds\n",
    "t1 = time.time()\n",
    "if not hasattr(slrcu, 'pos_predict_percent_fit_dict'):\n",
    "    slrcu.build_pos_logistic_regression_elements(sampling_strategy_limit=None, verbose=True)\n",
    "if hasattr(slrcu, 'pos_predict_percent_fit_dict'): print('predict_single is available')\n",
    "else: print('predict_single is not available')\n",
    "duration_str = humanize.precisedelta(time.time() - t1, minimum_unit='seconds', format='%0.0f')\n",
    "speech_str = f'Parts-of-speech logistic regression elements built in {duration_str}'; print(speech_str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3fe80356-c69a-4579-b298-31e40e871b31",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "predict_single is now available\n",
      "Parts-of-speech conditional random field elements built in 2 seconds\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Check if the scrfcu has built its parts-of-speech conditional random field elements\n",
    "# Parts-of-speech CRF elements normally built in 29 minutes and 57 seconds\n",
    "t1 = time.time()\n",
    "if not hasattr(scrfcu, 'pos_symbol_crf'):\n",
    "    scrfcu.build_pos_conditional_random_field_elements(verbose=True)\n",
    "if hasattr(scrfcu, 'pos_predict_percent_fit_dict'): print('predict_single is now available')\n",
    "else: print('predict_single is not available')\n",
    "duration_str = humanize.precisedelta(time.time() - t1, minimum_unit='seconds', format='%0.0f')\n",
    "speech_str = f'Parts-of-speech conditional random field elements built in {duration_str}'; print(speech_str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3453a69d-7080-4ebf-96a8-528e9541ea19",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I have 48,129 labeled parts of speech in here\n",
      "predict_single is now available\n",
      "Parts-of-speech stochastic gradient descent elements built in 17 seconds\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Check if the ssgdcu has built its parts-of-speech stochastic gradient decent elements\n",
    "t1 = time.time()\n",
    "if not hasattr(ssgdcu, 'pos_predict_percent_fit_dict'):\n",
    "    ssgdcu.build_pos_stochastic_gradient_descent_elements(sampling_strategy_limit=None, verbose=True)\n",
    "if hasattr(ssgdcu, 'pos_predict_percent_fit_dict'): print('predict_single is now available')\n",
    "else: print('predict_single is not available')\n",
    "duration_str = humanize.precisedelta(time.time() - t1, minimum_unit='seconds', format='%0.0f')\n",
    "speech_str = f'Parts-of-speech stochastic gradient descent elements built in {duration_str}'; print(speech_str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "01e7d031-531c-4024-b9f4-03485a91a943",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "predict_single is now available\n",
      "POS classifier trained in 0 seconds\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Check if the crf has built its parts-of-speech classifier\n",
    "# POS classifier normally trained in 15 hours, 42 minutes and 41 seconds\n",
    "t1 = time.time()\n",
    "if not hasattr(crf, 'CRF'): crf.build_pos_conditional_random_field_elements(verbose=True)\n",
    "if hasattr(crf, 'CRF'): print('predict_single is now available')\n",
    "else: print('predict_single is not available')\n",
    "duration_str = humanize.precisedelta(time.time() - t1, minimum_unit='seconds', format='%0.0f')\n",
    "speech_str = f'POS classifier trained in {duration_str}'; print(speech_str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d259dffb-c212-4493-a879-1ae5d503f8ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I have 532,546 is-qualified vocabulary tokens in here\n",
      "Is-qualified LR elements built in 12 seconds\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Check if the lru has built its is-qualified classifier\n",
    "t1 = time.time()\n",
    "if not (hasattr(lru, 'ISQUALIFIED_LR') and hasattr(lru, 'ISQUALIFIED_CV')):\n",
    "    lru.build_isqualified_logistic_regression_elements(sampling_strategy_limit=None, verbose=True)\n",
    "duration_str = humanize.precisedelta(time.time() - t1, minimum_unit='seconds', format='%0.0f')\n",
    "speech_str = f'Is-qualified LR elements built in {duration_str}'; print(speech_str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a9d67a34-e133-45dc-9bf3-a830af359d77",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I have 51,990 hand-labeled header htmls prepared\n",
      "7 iterations seen during training fit for a total of 51,990 records trained\n",
      "Is-header classifier trained in 12 seconds\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Train the isheader classifier\n",
    "t1 = time.time()\n",
    "ihu.build_pos_stochastic_gradient_descent_elements(verbose=True)\n",
    "duration_str = humanize.precisedelta(time.time() - t1, minimum_unit='seconds', format='%0.0f')\n",
    "speech_str = f'Is-header classifier trained in {duration_str}'; print(speech_str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "dcef1d94-4466-4100-86cd-0d66f23d0d40",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Last run on 2024-10-26 06:46:45.108291\n"
     ]
    }
   ],
   "source": [
    "\n",
    "speech_str = f'Last run on {datetime.now()}'; print(speech_str)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f438163-0f14-4a74-978c-769c9dba95c8",
   "metadata": {},
   "source": [
    "\n",
    "---\n",
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "f02bfeac-67b7-4df6-a4a0-32ec3dec0022",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I have 21,826 hand-labeled qualification strings in here\n",
      "I have 605,026 is-qualified vocabulary tokens in here\n",
      "Is-qualified classifer retrained in 36 seconds\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# You need to run this again if you changed the qualification dictionary below or in another notebook\n",
    "t1 = time.time()\n",
    "\n",
    "# Keep the total retraining time to less than two minutes by adjusting the sampling strategy limit\n",
    "lru.sync_basic_quals_dict(sampling_strategy_limit=None, verbose=False)\n",
    "\n",
    "lru.retrain_isqualified_classifier(verbose=True)\n",
    "duration_str = humanize.precisedelta(time.time() - t1, minimum_unit='seconds', format='%0.0f')\n",
    "speech_str = f'Is-qualified classifer retrained in {duration_str}'; print(speech_str)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52ad3579-ebc3-4d21-a7c3-f9bd168ac6ca",
   "metadata": {},
   "source": [
    "\n",
    "# Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "d334d815-e43b-474e-9dbb-db509176f6e5",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Basic Qualifications for 1395049 AI Data Architect INNOVATIONS UK LTD:\n",
      "*quals_list[0] = \"Experience with statistical modeling and data analysis\" (1.0)\n",
      "*quals_list[1] = \"Experience with tabular data analysis using languages such as SQL, R, and/or Python\" (1.0)\n",
      "*quals_list[2] = \"Proficient in data exploration techniques and tools such as Amazon Web Services (AWS)\" (1.0)\n",
      "*quals_list[3] = \"2024-10-23\" (0.9141)\n",
      "*quals_list[4] = \"Experience with data visualization libraries such as Plotly, Streamlit, and matplotlib.\" (1.0)\n",
      "*quals_list[5] = \"Strong Python programming fundamentals\" (1.0)\n",
      "quals_list[6] = \"Good understanding of machine learning algorithms, tools and platforms\" (1.0)\n",
      "*quals_list[7] = \"Great communication skills, able to explain model results to a non-technical audience\" (1.0)\n",
      "*quals_list[8] = \"Experience with planning and execution\" (0.0)\n",
      "*quals_list[9] = \"Bachelor's degree in Computer Science, Data Science or related field and over 16 years of relevant experience, Masters with 14 years experience, or PhD with 10 years experience.\" (1.0)\n",
      "*quals_list[10] = \"Experience with data architecture at scale and AI Infrastructure\" (0.0)\n",
      "*quals_list[11] = \"Willing to learn new skills and platforms to support data analytics.\" (1.0)\n",
      "*quals_list[12] = \"Experience with data repositories and ETL\" (1.0)\n",
      "*quals_list[13] = \"Ability and willingness to obtain a Top Secret security clearance\" (0.0003)\n",
      "*quals_list[14] = \"Experience with AI/ML tools, such as common Python packages (eg, scikit-learn, NumPy, Pandas)\" (1.0)\n",
      "80.00%\n",
      "\n",
      "hunting_df.loc[5280, 'percent_fit'] = (000+000+000+000+000+000+1+000+000+000+000+000+000+000+000)/15\n",
      "5279/5283 = 99.9% completed\n",
      "Inference completed in 1 minute and 7 seconds\n"
     ]
    }
   ],
   "source": [
    "\n",
    "t1 = time.time()\n",
    "\n",
    "# Loop through all the unset %fit values, set them if you can, break for help if you can't\n",
    "quals_list, file_name = lru.infer_from_hunting_dataframe(fitness_threshold=3/4, verbose=True)\n",
    "duration_str = humanize.precisedelta(time.time() - t1, minimum_unit='seconds', format='%0.0f')\n",
    "speech_str = f'Inference completed in {duration_str}'; print(speech_str); speech_engine.say(speech_str); speech_engine.runAndWait()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63264da9-f96a-491a-83d9-25d15fcdfef6",
   "metadata": {},
   "outputs": [],
   "source": [
    "raise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "47bd03b8-c0a3-404c-a16d-eb4ce95c6561",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Proficient in data exploration techniques and tools such as Amazon Web Services (AWS)\n",
      "Pickling to C:\\Users\\daveb\\OneDrive\\Documents\\GitHub\\job-hunting\\saves\\pkl\\basic_quals_dict.pkl\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Manually label the unscored qualification string\n",
    "qualification_str = quals_list[2]\n",
    "print(qualification_str); basic_quals_dict = nu.load_object('basic_quals_dict')\n",
    "print(str(basic_quals_dict[qualification_str]) + '\\n' if(qualification_str in basic_quals_dict) else '', end='')\n",
    "basic_quals_dict[qualification_str] = 1\n",
    "nu.store_objects(basic_quals_dict=basic_quals_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89c4212b-9280-46d1-8e46-104aada7c0df",
   "metadata": {},
   "source": [
    "\n",
    "----\n",
    "## Fix Parts-of-Speech and Quals for this posting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "8ac6efb8-6b56-43dd-9079-d6a87491ecc5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1395049_AI_Data_Architect_LEIDOS_INNOVATIONS_UK_LTD.html\n",
      "CRF and child strings list recreated in 1 minute and 9 seconds\n"
     ]
    }
   ],
   "source": [
    "\n",
    "t1 = time.time()\n",
    "file_path = osp.join(cu.SAVES_HTML_FOLDER, file_name)\n",
    "if osp.isfile(file_path):\n",
    "    child_strs_list = hau.get_child_strs_from_file(file_name=file_name)\n",
    "    cu.ensure_filename(file_name, verbose=False)\n",
    "    cu.populate_from_child_strings(child_strs_list, file_name, verbose=False)\n",
    "    print(file_name)\n",
    "    assert hasattr(slrcu, 'pos_predict_percent_fit_dict'), 'slrcu.predict_single needs to be available'\n",
    "    pos_symbol_predictions_list = [slrcu.predict_single(sent_str) for sent_str in child_strs_list]\n",
    "duration_str = humanize.precisedelta(time.time() - t1, minimum_unit='seconds', format='%0.0f')\n",
    "speech_str = f'CRF and child strings list recreated in {duration_str}'; print(speech_str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "affcb06a-3165-4349-a936-037af549d417",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['O-IP', 'O-TS', 'O-TS', 'H-TS', 'O-TS', 'O-TS', 'O-TS', 'O-TS', 'O-TS', 'O-TS', 'O-TS', 'O-TS', 'H-RQ', 'O-RQ', 'O-RQ', 'O-PQ', 'O-RQ', 'O-PQ', 'O-RQ', 'O-RQ', 'O-RQ', 'O-RQ', 'O-RQ', 'O-PQ', 'O-RQ', 'O-RQ', 'O-RQ', 'H-PQ', 'O-RQ', 'O-PQ', 'O-PQ', 'O-RQ', 'O-RQ', 'H-PQ', 'O-RQ', 'O-TS', 'H-SP', 'H-SP', 'O-SP']\n",
      "[13, 14, 16, 18, 19, 20, 21, 22, 24, 25, 26, 28, 31, 32]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "0 O-IP) <span style=\"color:#ffbb7880;\"><p>Looking for the opportunity to work on problems that matter, with colleagues that share your interest and expertise in AI and Machine Learning? (O-IP Interview Procedures Non-header)</p></span><br />1 O-TS) <span style=\"color:#9edae580;\"><p>Leidos is looking for a Senior Data Architect to apply their expertise to help Leidos research, develop, and commercialize technologies enabling data processing at scale for AI consumption. Specific areas of R&amp;D include Data engineering, Data processing, anomaly detection and Data quality management. (O-TS Task Scope Non-header)</p></span><br />2 O-TS) <span style=\"color:#9edae580;\"><p>We are looking for someone who is intellectually adaptive, likes to collaborate, inquisitive and goal oriented. You will work alongside junior and senior research/data scientists and data engineers with expertise in information retrieval, UI development, information science, machine learning and artificial intelligence, and statistics. You will be leading the teams of engineers and scientists to define and implement research milestones, Proof of Concepts and eventually operationalizing the A-Ready data construction. (O-TS Task Scope Non-header)</p></span><br />3 H-TS) <span style=\"color:#9edae5ff;\"><strong>Primary Responsibilities. (H-TS Task Scope Header)</strong></span><br />4 O-TS) <span style=\"color:#9edae580;\"><li>Work with and lead a team of data scientists and data engineers on defining and creating scalable AI data architecture strategy (O-TS Task Scope Non-header)</li></span><br />5 O-TS) <span style=\"color:#9edae580;\"><li>Identify risks and constraints of AI infrastructure at scale and implement mitigations and workarounds (O-TS Task Scope Non-header)</li></span><br />6 O-TS) <span style=\"color:#9edae580;\"><li>Create project plan, milestones and lead the team to Implement the AI data architecture strategy (O-TS Task Scope Non-header)</li></span><br />7 O-TS) <span style=\"color:#9edae580;\"><li>Lead the design and implementation of data access controls to produce a data transformation pipeline that discovers, repairs, logs, and deploys quality data plus security policy metadata (O-TS Task Scope Non-header)</li></span><br />8 O-TS) <span style=\"color:#9edae580;\"><li>Work with a team to design, build, train, and evaluate machine learning models (O-TS Task Scope Non-header)</li></span><br />9 O-TS) <span style=\"color:#9edae580;\"><li>Be the liaison between executive level leadership, research team and other stakeholders across Leidos. (O-TS Task Scope Non-header)</li></span><br />10 O-TS) <span style=\"color:#9edae580;\"><li>Communicate results and methods for solutions to internal and external stakeholders. (O-TS Task Scope Non-header)</li></span><br />11 O-TS) <span style=\"color:#9edae580;\"><li>Work within teams of AI/ML researchers and engineers using Agile development processes (O-TS Task Scope Non-header)</li></span><br />12 H-RQ) <span style=\"color:#bcbd22ff;\"><strong>Basic Qualifications. (H-RQ Required Qualifications Header)</strong></span><br /><hr />13 O-RQ) <span style=\"color:#bcbd2280;\"><li>Bachelor's degree in Computer Science, Data Science or related field and over 16 years of relevant experience, Masters with 14 years experience, or PhD with 10 years experience. (O-RQ Required Qualifications Non-header)</li></span><br />14 O-RQ) <span style=\"color:#bcbd2280;\"><li>Experience with data architecture at scale and AI Infrastructure (O-RQ Required Qualifications Non-header)</li></span><br />15 O-PQ) <span style=\"color:#c7c7c780;\"><li>Experience with data categorization, labeling and tagging (O-PQ Preferred Qualifications Non-header)</li></span><br />16 O-RQ) <span style=\"color:#bcbd2280;\"><li>Experience with data repositories and ETL (O-RQ Required Qualifications Non-header)</li></span><br />17 O-PQ) <span style=\"color:#c7c7c780;\"><li>Experience with Data Operations including applying AI for data curation (O-PQ Preferred Qualifications Non-header)</li></span><br />18 O-RQ) <span style=\"color:#bcbd2280;\"><li>Strong Python programming fundamentals (O-RQ Required Qualifications Non-header)</li></span><br />19 O-RQ) <span style=\"color:#bcbd2280;\"><li>Good understanding of machine learning algorithms, tools and platforms (O-RQ Required Qualifications Non-header)</li></span><br />20 O-RQ) <span style=\"color:#bcbd2280;\"><li>Experience with AI/ML tools, such as common Python packages (e.g., scikit-learn, NumPy, Pandas) (O-RQ Required Qualifications Non-header)</li></span><br />21 O-RQ) <span style=\"color:#bcbd2280;\"><li>Experience with tabular data analysis using languages such as SQL, R, and/or Python (O-RQ Required Qualifications Non-header)</li></span><br />22 O-RQ) <span style=\"color:#bcbd2280;\"><li>Experience with statistical modeling and data analysis (O-RQ Required Qualifications Non-header)</li></span><br />23 O-PQ) <span style=\"color:#c7c7c780;\"><li>Experience with leading research projects and project teams (O-PQ Preferred Qualifications Non-header)</li></span><br />24 O-RQ) <span style=\"color:#bcbd2280;\"><li>Experience with planning and execution (O-RQ Required Qualifications Non-header)</li></span><br />25 O-RQ) <span style=\"color:#bcbd2280;\"><li>Great communication skills, able to explain model results to a non-technical audience (O-RQ Required Qualifications Non-header)</li></span><br />26 O-RQ) <span style=\"color:#bcbd2280;\"><li>Proficient in data exploration techniques and tools such as Amazon Web Services (AWS) (O-RQ Required Qualifications Non-header)</li></span><br />27 H-PQ) <span style=\"color:#c7c7c7ff;\"><strong>Preferred Qualifications. (H-PQ Preferred Qualifications Header)</strong></span><br />28 O-RQ) <span style=\"color:#bcbd2280;\"><li>Experience with data visualization libraries such as Plotly, Streamlit, and matplotlib. (O-RQ Required Qualifications Non-header)</li></span><br />29 O-PQ) <span style=\"color:#c7c7c780;\"><li>Experience with MLOps tools and frameworks, such as Kubeflow, MLflow, DVC, TensorBoard (O-PQ Preferred Qualifications Non-header)</li></span><br />30 O-PQ) <span style=\"color:#c7c7c780;\"><li>Experience with building LLM and other Generative AI applications. (O-PQ Preferred Qualifications Non-header)</li></span><br />31 O-RQ) <span style=\"color:#bcbd2280;\"><li>Willing to learn new skills and platforms to support data analytics. (O-RQ Required Qualifications Non-header)</li></span><br />32 O-RQ) <span style=\"color:#bcbd2280;\"><li>Ability and willingness to obtain a Top Secret security clearance (O-RQ Required Qualifications Non-header)</li></span><br /><hr />33 H-PQ) <span style=\"color:#c7c7c7ff;\"><strong>Original Posting Date: (H-PQ Preferred Qualifications Header)</strong></span><br />34 O-RQ) <span style=\"color:#bcbd2280;\"><p>2024-10-23 (O-RQ Required Qualifications Non-header)</p></span><br />35 O-TS) <span style=\"color:#9edae580;\"><p>While subject to change based on business needs, Leidos reasonably anticipates that this job requisition will remain open for at least 3 days with an anticipated close date of no earlier than 3 days after the original posting date as listed above. (O-TS Task Scope Non-header)</p></span><br />36 H-SP) <span style=\"color:#17becfff;\"><strong>Pay Range: (H-SP Supplemental Pay Header)</strong></span><br />37 H-SP) <span style=\"color:#17becfff;\"><p>Pay Range $144,300.00 - $260,850.00 (H-SP Supplemental Pay Header)</p></span><br />38 O-SP) <span style=\"color:#17becf80;\"><p>The Leidos pay range for this job level is a general guideline only and not a guarantee of compensation or salary. Additional factors considered in extending an offer include (but are not limited to) responsibilities of the job, education, experience, knowledge, skills, and abilities, as well as internal equity, alignment with market data, applicable bargaining agreement (if any), or other law. (O-SP Supplemental Pay Non-header)</p></span><br />"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[13, 14, 16, 18, 19, 20, 21, 22, 24, 25, 26, 28, 31, 32]\n",
      "Parts-of-speech displayed in 1 minute and 17 seconds\n"
     ]
    }
   ],
   "source": [
    "\n",
    "db_pos_list = []\n",
    "for navigable_parent in child_strs_list:\n",
    "    db_pos_list = cu.append_parts_of_speech_list(navigable_parent, pos_list=db_pos_list)\n",
    "pos_list, indices_list = su.visualize_basic_quals_section(pos_symbol_predictions_list, child_strs_list, db_pos_list=db_pos_list, verbose=True)\n",
    "duration_str = humanize.precisedelta(time.time() - t1, minimum_unit='seconds', format='%0.0f')\n",
    "speech_str = f'Parts-of-speech displayed in {duration_str}'; print(speech_str); speech_engine.say(speech_str); speech_engine.runAndWait()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91379a3d-1a77-4d8b-9ef6-a76f12204ac2",
   "metadata": {},
   "outputs": [],
   "source": [
    "raise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "26c4dd4e-1418-4420-aff5-ea7b65273194",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "36 H-PQ) <p>______________________________________________________________________________</p>\n",
      "47 H-PQ) <p>______________________________________________________________________________</p>\n"
     ]
    }
   ],
   "source": [
    "\n",
    "basic_quals_dict = nu.load_object('basic_quals_dict'); column_name = 'is_job_title'\n",
    "for idx in list(range(0, 3)):\n",
    "    child_str = child_strs_list[idx]; pos_symbol = pos_list[idx]\n",
    "    def do_cypher_tx(tx, navigable_parent, verbose=False):\n",
    "        cypher_str = '''\\n            MATCH (np:NavigableParents {navigable_parent: $navigable_parent})\\n            ''' + cu.return_every_np_str + ';'\n",
    "        results_list = tx.run(query=cypher_str, parameters={'navigable_parent': navigable_parent})\n",
    "        return [dict(record.items()) for record in results_list]\n",
    "    with cu.driver.session() as session: row_objs_list = session.write_transaction(do_cypher_tx, navigable_parent=child_str, verbose=False)\n",
    "    print(str(basic_quals_dict[child_str]) + '\\n' if(child_str in basic_quals_dict) else '', end='')\n",
    "    print(f'{idx} {pos_symbol}) {child_str}')\n",
    "    def do_cypher_tx(tx, navigable_parent, verbose=False):\n",
    "        cypher_str = f'''\n",
    "            MATCH (np:NavigableParents {{navigable_parent: $navigable_parent}})\n",
    "            SET\n",
    "                np.is_job_title = {str(column_name == 'is_job_title').lower()},\n",
    "                np.is_corporate_scope = {str(column_name == 'is_corporate_scope').lower()},\n",
    "                np.is_task_scope = {str(column_name == 'is_task_scope').lower()},\n",
    "                np.is_minimum_qualification = {str(column_name == 'is_minimum_qualification').lower()},\n",
    "                np.is_preferred_qualification = {str(column_name == 'is_preferred_qualification').lower()},\n",
    "                np.is_supplemental_pay = {str(column_name == 'is_supplemental_pay').lower()},\n",
    "                np.is_office_location = {str(column_name == 'is_office_location').lower()},\n",
    "                np.is_job_duration = {str(column_name == 'is_job_duration').lower()},\n",
    "                np.is_interview_procedure = {str(column_name == 'is_interview_procedure').lower()},\n",
    "                np.is_legal_notification = {str(column_name == 'is_legal_notification').lower()},\n",
    "                np.is_other = {str(column_name == 'is_other').lower()},\n",
    "                np.is_posting_date = {str(column_name == 'is_posting_date').lower()}\n",
    "            ''' + cu.return_every_np_str + ';'\n",
    "        return [dict(record.items()) for record in tx.run(query=cypher_str, parameters={'navigable_parent': navigable_parent})]\n",
    "    with cu.driver.session() as session: row_objs_list = session.write_transaction(do_cypher_tx, navigable_parent=child_str, verbose=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "1d97acb2-2b60-4a56-9b3b-ecfc2b6582e6",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[23, 24, 26, 27, 28, 29, 31]\n",
      "35 O-SP) <p>In support of pay transparency and equity, the minimum and maximum full-time annual base salary for this role in New York is $160,000 - $185,000 at the time of posting, with the potential of an incentive or bonus. While this is our reasonable expectation this is not a guarantee of compensation or salary, actual compensation is influenced by a wide range of factors including but not limited to skill set, level of experience, education, certifications, responsibility, and geographic location. Candidates hired to work in other locations will be subject to the pay range associated with that location. We offer a variety of benefits including medical, dental, vision, disability insurance, 401(k), EAP, parental leave, discretionary time off, and company-paid holidays. The specific programs and options available will vary depending on the state, start date, and employment type. Our Talent Acquisition team will be happy to answer any questions you may have.</p>\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Display the context of an individual child string\n",
    "idx = 35\n",
    "print(indices_list); child_str = child_strs_list[idx]; pos_symbol = pos_list[idx]; basic_quals_dict = nu.load_object('basic_quals_dict')\n",
    "print(str(basic_quals_dict[child_str]) + '\\n' if(child_str in basic_quals_dict) else '', end='')\n",
    "print(f'{idx} {pos_symbol}) {child_str}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "b2f46926-880b-4aed-bc99-4a6bd9ed7903",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pickling to C:\\Users\\daveb\\OneDrive\\Documents\\GitHub\\job-hunting\\saves\\pkl\\basic_quals_dict.pkl\n",
      "\"<li>Familiarity of ACR (Automatic Content Recognition) techniques and their applications in media is a plus.</li>\" in basic_quals_dict: 1\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Hand-label this particular child string in the quals dictionary\n",
    "basic_quals_dict = nu.load_object('basic_quals_dict')\n",
    "basic_quals_dict[child_str] = 1\n",
    "nu.store_objects(basic_quals_dict=basic_quals_dict); print(f'\"{child_str}\" in basic_quals_dict: {basic_quals_dict[child_str]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "24e7695d-caf6-416f-a761-ec17b839ccf9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15 O-RQ) <p>U.S. Citizenship is required.</p>\n",
      "16 O-RQ) <p>Bachelors’ Degree or equivalent experience and a minimum of 3-5 years of experience applying ML/AI in a business context.</p>\n",
      "17 O-RQ) <p>Deep expertise in classical machine learning algorithms and their mathematical foundations.</p>\n",
      "18 O-RQ) <p>Proven experience applying advanced NLP techniques and working with large language models (e.g., LLMs)</p>\n",
      "19 O-RQ) <p>Strong programming skills in Python, with extensive use of libraries such as scikit-learn, Pandas, NLTK, and Hugging Face Transformers.</p>\n",
      "20 O-RQ) <p>Demonstrated ability to develop novel ML/AI solutions that directly impact business outcomes.</p>\n",
      "21 H-RQ) <p>Experience presenting complex technical concepts to senior executives and translating them into strategic business initiatives.</p>\n",
      "22 O-RQ) <p>Ability to work in a time critical environment and prioritize workload effectively</p>\n",
      "23 O-RQ) <p>Ability to interact professionally with a diverse group of executives, managers, and subject matter experts</p>\n",
      "24 O-RQ) <p>Ability to keep information organized and confidential</p>\n",
      "25 O-RQ) <p>Strong written and verbal communication skills</p>\n",
      "26 O-RQ) <p>Proficiency with data visualization tools to create compelling, data-driven narratives.</p>\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Fix Non-headers\n",
    "basic_quals_dict = nu.load_object('basic_quals_dict')\n",
    "for idx in range(15, 27):\n",
    "    child_str = child_strs_list[idx]; pos_symbol = pos_list[idx]\n",
    "    print(str(basic_quals_dict[child_str]) + '\\n' if(child_str in basic_quals_dict) else '', end='')\n",
    "    print(f'{idx} {pos_symbol}) {child_str}')\n",
    "    def do_cypher_tx(tx, navigable_parent, verbose=False):\n",
    "        cypher_str = f'''\n",
    "            MATCH (np:NavigableParents {{navigable_parent: $navigable_parent}})\n",
    "            SET np.is_header = false\n",
    "            ''' + cu.return_every_np_str + ';'\n",
    "        return [dict(record.items()) for record in tx.run(query=cypher_str, parameters={'navigable_parent': navigable_parent})]\n",
    "    with cu.driver.session() as session:\n",
    "        row_objs_list = session.write_transaction(do_cypher_tx, navigable_parent=child_str, verbose=False)\n",
    "        ihu.retrain_classifier(row_objs_list[0]['navigable_parent'], row_objs_list[0]['is_header'], verbose=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "326ab607-cc5c-4476-b692-395b96d293ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8 H-PQ) <strong>Not your usual app</strong>\n",
      "11 O-CS) <strong>What makes our ride unique?</strong>\n",
      "15 O-CS) We have a vision:\n",
      "18 O-TS) <strong>THE JOURNEY</strong>\n",
      "24 H-RQ) <strong>WHAT YOU WILL BRING TO THE RIDE</strong>\n",
      "37 O-CS) <strong>We believe driven talent deserves:</strong>\n",
      "48 H-TS) <strong>So, ready to take the wheel and make this the ride of your life?</strong>\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Fix Headers\n",
    "basic_quals_dict = nu.load_object('basic_quals_dict')\n",
    "for idx in [8, 11, 15, 18, 24, 37, 48]:\n",
    "    child_str = child_strs_list[idx]; pos_symbol = pos_list[idx]\n",
    "    print(str(basic_quals_dict[child_str]) + '\\n' if(child_str in basic_quals_dict) else '', end='')\n",
    "    print(f'{idx} {pos_symbol}) {child_str}')\n",
    "    def do_cypher_tx(tx, navigable_parent, verbose=False):\n",
    "        cypher_str = f'''\n",
    "            MATCH (np:NavigableParents {{navigable_parent: $navigable_parent}})\n",
    "            SET np.is_header = true\n",
    "            ''' + cu.return_every_np_str + ';'\n",
    "        return [dict(record.items()) for record in tx.run(query=cypher_str, parameters={'navigable_parent': navigable_parent})]\n",
    "    with cu.driver.session() as session:\n",
    "        row_objs_list = session.write_transaction(do_cypher_tx, navigable_parent=child_str, verbose=False)\n",
    "        ihu.retrain_classifier(row_objs_list[0]['navigable_parent'], row_objs_list[0]['is_header'], verbose=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "181dfa2e-4e09-4da7-b046-98f5c170ec67",
   "metadata": {},
   "source": [
    "\n",
    "----\n",
    "## Maintenance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "07255146-70be-4d52-942c-1505d6e02a9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Display cypher necessary to apply for all the jobs you qualify for that you haven't applied for\n",
    "cypher_str = f'''\n",
    "    // Get job application links for jobs you should apply to\n",
    "    MATCH (fn:FileNames)\n",
    "    WHERE\n",
    "        fn.percent_fit >= 0.8 AND\n",
    "        ((fn.is_closed IS NULL) OR (fn.is_closed = false)) AND\n",
    "        ((fn.is_opportunity_application_emailed IS NULL) OR (fn.is_opportunity_application_emailed = false))\n",
    "    RETURN\n",
    "        fn.percent_fit AS percent_fit,\n",
    "        fn.file_name AS filename,\n",
    "        fn.posting_url AS posting_url\n",
    "    ORDER BY fn.percent_fit DESC;'''\n",
    "pyperclip.copy(cypher_str)\n",
    "row_objs_list = []\n",
    "with cu.driver.session() as session: row_objs_list = session.write_transaction(cu.do_cypher_tx, cypher_str)\n",
    "if row_objs_list:\n",
    "    df = DataFrame(row_objs_list)\n",
    "    display(df)\n",
    "    for filename in df.filename:\n",
    "        print(f\"\"\"\n",
    "MATCH (fn:FileNames)\n",
    "WHERE fn.file_name IN [\"{filename}\"]\n",
    "SET fn.is_closed = true\n",
    "RETURN fn;\"\"\")\n",
    "        break\n",
    "    for filename in df.filename:\n",
    "        print(f\"\"\"\n",
    "MATCH (fn:FileNames)\n",
    "WHERE fn.file_name IN [\"{filename}\"]\n",
    "SET fn.is_opportunity_application_emailed = true, fn.opportunity_application_email_date = date()\n",
    "RETURN fn;\"\"\")\n",
    "        break\n",
    "speech_str = 'Job application cypher code copied to clipboard'; speech_engine.say(speech_str); speech_engine.runAndWait()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "22c3685b-69f1-40fd-aa98-c473365ff866",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Break up overly-long O-RQs:\n",
    "# Ensure you have already run the \"Fix Parts-of-Speech and Quals for \n",
    "# this posting\" cells above or displayed the context of an \n",
    "# individual child string above. Don't close the Notepad++ window \n",
    "# until you have replaced the child string\n",
    "# file_name = '476d6e5bd50da5a6_Data_Scientist_Remote_Indeed_com.html'\n",
    "text_editor_path = r'C:\\Program Files\\Notepad++\\notepad++.exe'\n",
    "file_path = osp.abspath(osp.join(hau.SAVES_HTML_FOLDER, file_name))\n",
    "wsu.clean_job_posting(file_path)\n",
    "try: pyperclip.copy(re.sub(\"((?:<li>([^><]+)</li>\\n)+)\", \"<ul>\\n\\\\1</ul>\\n\", '\\n'.join(child_strs_list), 0, re.MULTILINE))\n",
    "except: pass\n",
    "!\"{text_editor_path}\" \"{file_path}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "af64e55d-eb9b-413d-8b71-bef7188d48a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "                MATCH\n",
      "                    (np:NavigableParents {navigable_parent: \"<p>CACI is an Equal Opportunity/Affirmative Action Employer. All qualified applicants will receive consideration for employment without regard to race, color, religion, sex, pregnancy, sexual orientation, gender identity, age, national origin, disability, status as a protected veteran, or any other protected characteristic.</p>\"}),\n",
      "                    (ht:HeaderTags {header_tag: \"p\"})\n",
      "                MERGE (ht)-[r:SUMMARIZES]->(np);\n"
     ]
    }
   ],
   "source": [
    "\n",
    "cu.rebuild_filename_node(file_name, navigable_parent=None, verbose=True)\n",
    "speech_str = f'{su.get_job_title_from_file_name(file_name)} node rebuild completed'; speech_engine.say(speech_str); speech_engine.runAndWait()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba3fae8b-14ec-4c20-96b0-70c622ac7c73",
   "metadata": {},
   "source": [
    "\n",
    "----"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56d17dac-be9e-4bc3-aa01-58eed1baa9af",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Show what's in the database already for this html string\n",
    "def do_cypher_tx(tx, navigable_parent, verbose=False):\n",
    "    cypher_str = '''\n",
    "        MATCH (np:NavigableParents {navigable_parent: $navigable_parent})\n",
    "        ''' + cu.return_every_np_str + ';'\n",
    "    results_list = tx.run(query=cypher_str, parameters={'navigable_parent': navigable_parent})\n",
    "    return [dict(record.items()) for record in results_list]\n",
    "with cu.driver.session() as session: row_objs_list = session.write_transaction(do_cypher_tx, navigable_parent=child_str, verbose=False)\n",
    "row_objs_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe1d1a8b-85e6-4fb8-aba3-43874e6dabdf",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Remove this particular qualification string from the quals dictionary\n",
    "qualification_str = quals_list[25]\n",
    "basic_quals_dict = nu.load_object('basic_quals_dict')\n",
    "basic_quals_dict.pop(qualification_str, None)\n",
    "nu.store_objects(basic_quals_dict=basic_quals_dict)\n",
    "print(f'\"{qualification_str}\" in basic_quals_dict: {qualification_str in basic_quals_dict}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e765305-d9a3-4955-a905-8389c35ad472",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Remove this particular qualification string from the database\n",
    "def do_cypher_tx(tx, qualification_str, verbose=False):\n",
    "    cypher_str = '''\n",
    "        MATCH (qs:QualificationStrings {qualification_str: $qualification_str})\n",
    "        DETACH DELETE qs;\n",
    "        '''\n",
    "    results_list = tx.run(query=cypher_str, parameters={'qualification_str': qualification_str})\n",
    "\n",
    "    return [dict(record.items()) for record in results_list]\n",
    "with cu.driver.session() as session:\n",
    "    row_objs_list = session.write_transaction(do_cypher_tx, qualification_str=qualification_str, verbose=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4005f4a8-f07c-4538-8d1b-210d32dd3199",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Manually set each feature\n",
    "def do_cypher_tx(tx, navigable_parent, verbose=False):\n",
    "    cypher_str = '''\n",
    "        MATCH (np:NavigableParents {navigable_parent: $navigable_parent})\n",
    "        SET\n",
    "            np.is_header = true,\n",
    "            np.is_task_scope = false,\n",
    "            np.is_minimum_qualification = false,\n",
    "            np.is_preferred_qualification = false,\n",
    "            np.is_educational_requirement = true,\n",
    "            np.is_legal_notification = false,\n",
    "            np.is_other = false,\n",
    "            np.is_corporate_scope = false,\n",
    "            np.is_job_title = false,\n",
    "            np.is_office_location = false,\n",
    "            np.is_job_duration = false,\n",
    "            np.is_supplemental_pay = false,\n",
    "            np.is_interview_procedure = false,\n",
    "            np.is_posting_date = false\n",
    "        ''' + cu.return_every_np_str + ';'\n",
    "    return [dict(record.items()) for record in tx.run(query=cypher_str, parameters={'navigable_parent': navigable_parent})]\n",
    "with cu.driver.session() as session: row_objs_list = session.write_transaction(do_cypher_tx, navigable_parent=child_str, verbose=False)\n",
    "ihu.retrain_classifier(row_objs_list[0]['navigable_parent'], row_objs_list[0]['is_header'], verbose=False); row_objs_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "872a9fa5-9f15-4bcc-a3df-b75dbb52ed51",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Remove file name from database\n",
    "for file_name in ['']:\n",
    "    cu.delete_filename_node(file_name, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72f2828f-ab7b-4550-b5cc-92078c40f331",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Job Hunting (Python 3.10.9)",
   "language": "python",
   "name": "jh_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
