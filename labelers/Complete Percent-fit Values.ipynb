{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "abb7318f-5285-4f7b-bc3e-2859fe05f928",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pretty printing has been turned OFF\n",
      "======== Neo4j/4.4.7 ========\n",
      "Utility libraries created in 4 seconds\n"
     ]
    }
   ],
   "source": [
    "\n",
    "%pprint\n",
    "%matplotlib inline\n",
    "import sys\n",
    "if ('../py' not in sys.path): sys.path.insert(1, '../py')\n",
    "from jobpostlib import (crf, cu, datetime, duration, hau, hc, humanize, ihu, lru, nu, osp, scrfcu, slrcu, ssgdcu, su, t0, time, wsu, speech_engine)\n",
    "from pandas import DataFrame\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fd3f331b-c66a-4c65-906b-22bc6fc034c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I have 49,102 labeled parts of speech in here\n",
      "predict_single is available\n",
      "Parts-of-speech logistic regression elements built in 7 seconds\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Check if the slrcu has built its parts-of-speech logistic regression elements\n",
    "# Parts-of-speech logistic regression elements is normally built in 1 hour, 27 minutes and 21 seconds\n",
    "t1 = time.time()\n",
    "if not hasattr(slrcu, 'pos_predict_percent_fit_dict'):\n",
    "    slrcu.build_pos_logistic_regression_elements(sampling_strategy_limit=None, verbose=True)\n",
    "if hasattr(slrcu, 'pos_predict_percent_fit_dict'): print('predict_single is available')\n",
    "else: print('predict_single is not available')\n",
    "duration_str = humanize.precisedelta(time.time() - t1, minimum_unit='seconds', format='%0.0f')\n",
    "speech_str = f'Parts-of-speech logistic regression elements built in {duration_str}'; print(speech_str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3fe80356-c69a-4579-b298-31e40e871b31",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "predict_single is now available\n",
      "Parts-of-speech conditional random field elements built in 1 second\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Check if the scrfcu has built its parts-of-speech conditional random field elements\n",
    "# Parts-of-speech CRF elements normally built in 29 minutes and 57 seconds\n",
    "t1 = time.time()\n",
    "if not hasattr(scrfcu, 'pos_symbol_crf'):\n",
    "    scrfcu.build_pos_conditional_random_field_elements(verbose=True)\n",
    "if hasattr(scrfcu, 'pos_predict_percent_fit_dict'): print('predict_single is now available')\n",
    "else: print('predict_single is not available')\n",
    "duration_str = humanize.precisedelta(time.time() - t1, minimum_unit='seconds', format='%0.0f')\n",
    "speech_str = f'Parts-of-speech conditional random field elements built in {duration_str}'; print(speech_str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3453a69d-7080-4ebf-96a8-528e9541ea19",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I have 49,102 labeled parts of speech in here\n",
      "predict_single is now available\n",
      "Parts-of-speech stochastic gradient descent elements built in 10 seconds\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Check if the ssgdcu has built its parts-of-speech stochastic gradient decent elements\n",
    "t1 = time.time()\n",
    "if not hasattr(ssgdcu, 'pos_predict_percent_fit_dict'):\n",
    "    ssgdcu.build_pos_stochastic_gradient_descent_elements(sampling_strategy_limit=None, verbose=True)\n",
    "if hasattr(ssgdcu, 'pos_predict_percent_fit_dict'): print('predict_single is now available')\n",
    "else: print('predict_single is not available')\n",
    "duration_str = humanize.precisedelta(time.time() - t1, minimum_unit='seconds', format='%0.0f')\n",
    "speech_str = f'Parts-of-speech stochastic gradient descent elements built in {duration_str}'; print(speech_str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "01e7d031-531c-4024-b9f4-03485a91a943",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "predict_single is now available\n",
      "POS classifier trained in 0 seconds\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Check if the crf has built its parts-of-speech classifier\n",
    "# POS classifier normally trained in 15 hours, 42 minutes and 41 seconds\n",
    "t1 = time.time()\n",
    "if not hasattr(crf, 'CRF'): crf.build_pos_conditional_random_field_elements(verbose=True)\n",
    "if hasattr(crf, 'CRF'): print('predict_single is now available')\n",
    "else: print('predict_single is not available')\n",
    "duration_str = humanize.precisedelta(time.time() - t1, minimum_unit='seconds', format='%0.0f')\n",
    "speech_str = f'POS classifier trained in {duration_str}'; print(speech_str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d259dffb-c212-4493-a879-1ae5d503f8ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I have 424,879 is-qualified vocabulary tokens in here\n",
      "Is-qualified LR elements built in 6 seconds\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Check if the lru has built its is-qualified classifier\n",
    "t1 = time.time()\n",
    "if not (hasattr(lru, 'ISQUALIFIED_LR') and hasattr(lru, 'ISQUALIFIED_CV')):\n",
    "    lru.build_isqualified_logistic_regression_elements(sampling_strategy_limit=None, verbose=True)\n",
    "duration_str = humanize.precisedelta(time.time() - t1, minimum_unit='seconds', format='%0.0f')\n",
    "speech_str = f'Is-qualified LR elements built in {duration_str}'; print(speech_str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a9d67a34-e133-45dc-9bf3-a830af359d77",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I have 50,181 hand-labeled header htmls prepared\n",
      "7 iterations seen during training fit for a total of 50,181 records trained\n",
      "Is-header classifier trained in 9 seconds\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Train the isheader classifier\n",
    "t1 = time.time()\n",
    "ihu.build_pos_stochastic_gradient_descent_elements(verbose=True)\n",
    "duration_str = humanize.precisedelta(time.time() - t1, minimum_unit='seconds', format='%0.0f')\n",
    "speech_str = f'Is-header classifier trained in {duration_str}'; print(speech_str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "dcef1d94-4466-4100-86cd-0d66f23d0d40",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Last run on 2024-07-02 08:43:45.708461\n"
     ]
    }
   ],
   "source": [
    "\n",
    "speech_str = f'Last run on {datetime.now()}'; print(speech_str)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f438163-0f14-4a74-978c-769c9dba95c8",
   "metadata": {},
   "source": [
    "\n",
    "---\n",
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f02bfeac-67b7-4df6-a4a0-32ec3dec0022",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I have 17,947 hand-labeled qualification strings in here\n",
      "I have 574,810 is-qualified vocabulary tokens in here\n",
      "Is-qualified classifer retrained in 2 minutes and 24 seconds\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# You need to run this again if you changed the\n",
    "# qualification dictionary below or in another notebook\n",
    "t1 = time.time()\n",
    "\n",
    "# Keep the total retraining time to less than two minutes by adjusting the sampling strategy limit\n",
    "lru.sync_basic_quals_dict(sampling_strategy_limit=None, verbose=False)\n",
    "\n",
    "lru.retrain_isqualified_classifier(verbose=True)\n",
    "duration_str = humanize.precisedelta(time.time() - t1, minimum_unit='seconds', format='%0.0f')\n",
    "speech_str = f'Is-qualified classifer retrained in {duration_str}'; print(speech_str)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52ad3579-ebc3-4d21-a7c3-f9bd168ac6ca",
   "metadata": {},
   "source": [
    "\n",
    "---\n",
    "# Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d334d815-e43b-474e-9dbb-db509176f6e5",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Basic Qualifications for Senior Data Scientist Massachusetts United States:\n",
      "*quals_list[0] = \"Advanced degree preferred in a quantitative field, and a minimum of 4 years of industry experience.\" (1.0)\n",
      "*quals_list[1] = \"Expertise in causal inference.\" (0.0)\n",
      "*quals_list[2] = \"Proficiency in programming languages (we use Python) and experience with data analysis and machine learning libraries (eg TensorFlow, PyTorch, Airflow, etc).\" (1.0)\n",
      "*quals_list[3] = \"Demonstrated ability to learn and embrace new technologies, applications, and solutions.\" (1.0)\n",
      "*quals_list[4] = \"Work Arrangement\" (1.0)\n",
      "*quals_list[5] = \"Experience as a technical lead and a passion for mentoring.\" (0.0)\n",
      "*quals_list[6] = \"Strong written and verbal communication skills and the ability to explain technical topics to data scientists, engineers, and business partners.\" (1.0)\n",
      "*quals_list[7] = \"Experience with exploratory data analysis, statistical analysis and testing, experiment design and model development, and knowledge of how to leverage these skills to solve complex business problems.\" (1.0)\n",
      "*quals_list[8] = \"Strong SQL skills (we use Snowflake).\" (0.9999)\n",
      "*quals_list[9] = \"Experience with a data visualization tool (we use Tableau).\" (1.0)\n",
      "*quals_list[10] = \"Ability to work collaboratively in a dynamic, team-based environment.\" (1.0)\n",
      "81.82%\n",
      "\n",
      "hunting_df.loc[4095, 'percent_fit'] = (000+000+000+000+000+000+000+000+000+000+000)/11\n",
      "4095/4099 = 99.9% completed\n",
      "Inference completed in 49 seconds\n"
     ]
    }
   ],
   "source": [
    "\n",
    "t1 = time.time()\n",
    "\n",
    "# Loop through all the unset %fit values, set them if you can, break for help if you can't\n",
    "quals_list, file_name = lru.infer_from_hunting_dataframe(fitness_threshold=3/4, verbose=True)\n",
    "duration_str = humanize.precisedelta(time.time() - t1, minimum_unit='seconds', format='%0.0f')\n",
    "speech_str = f'Inference completed in {duration_str}'; print(speech_str); speech_engine.say(speech_str); speech_engine.runAndWait()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63264da9-f96a-491a-83d9-25d15fcdfef6",
   "metadata": {},
   "outputs": [],
   "source": [
    "raise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "47bd03b8-c0a3-404c-a16d-eb4ce95c6561",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Demonstrated ability to learn and embrace new technologies, applications, and solutions.\n",
      "Pickling to C:\\Users\\daveb\\OneDrive\\Documents\\GitHub\\job-hunting\\saves\\pkl\\basic_quals_dict.pkl\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Manually label the unscored qualification string\n",
    "qualification_str = quals_list[3]\n",
    "print(qualification_str); basic_quals_dict = nu.load_object('basic_quals_dict')\n",
    "print(str(basic_quals_dict[qualification_str]) + '\\n' if(qualification_str in basic_quals_dict) else '', end='')\n",
    "basic_quals_dict[qualification_str] = 1\n",
    "nu.store_objects(basic_quals_dict=basic_quals_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89c4212b-9280-46d1-8e46-104aada7c0df",
   "metadata": {},
   "source": [
    "\n",
    "----\n",
    "## Fix Parts-of-Speech and Quals for this posting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "8ac6efb8-6b56-43dd-9079-d6a87491ecc5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "zfkP8JZ_uur56gXZLWAYQw_Senior_Data_Scientist_Massachusetts_United_States.html\n",
      "CRF and child strings list recreated in 57 seconds\n"
     ]
    }
   ],
   "source": [
    "\n",
    "t1 = time.time()\n",
    "file_path = osp.join(cu.SAVES_HTML_FOLDER, file_name)\n",
    "if osp.isfile(file_path):\n",
    "    child_strs_list = hau.get_child_strs_from_file(file_name=file_name)\n",
    "    cu.ensure_filename(file_name, verbose=False)\n",
    "    cu.populate_from_child_strings(child_strs_list, file_name, verbose=False)\n",
    "    print(file_name)\n",
    "    child_tags_list = hau.get_child_tags_list(child_strs_list)\n",
    "    feature_dict_list = cu.get_feature_dict_list(child_tags_list, child_strs_list)\n",
    "    feature_tuple_list = []\n",
    "    for feature_dict in feature_dict_list:\n",
    "        feature_tuple_list.append(hc.get_feature_tuple(\n",
    "            feature_dict, pos_lr_predict_single=slrcu.predict_single, pos_crf_predict_single=scrfcu.predict_single,\n",
    "            pos_sgd_predict_single=ssgdcu.predict_single\n",
    "        ))\n",
    "    crf_list = crf.CRF.predict_single(crf.sent2features(feature_tuple_list))\n",
    "duration_str = humanize.precisedelta(time.time() - t1, minimum_unit='seconds', format='%0.0f')\n",
    "speech_str = f'CRF and child strings list recreated in {duration_str}'; print(speech_str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "affcb06a-3165-4349-a936-037af549d417",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['H-TS', 'H-RQ', 'O-TS', 'O-TS', 'H-RQ', 'O-RQ', 'O-RQ', 'O-RQ', 'O-RQ', 'O-RQ', 'O-RQ', 'O-RQ', 'O-RQ', 'O-RQ', 'O-RQ', 'O-RQ', 'O-IP', 'H-CS', 'O-CS', 'O-CS', 'O-IP', 'O-CS', 'O-LN', 'O-IP', 'O-SP', 'H-SP', 'O-OL', 'O-O', 'H-RQ', 'O-O', 'O-SP']\n",
      "[5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "0 H-TS) <span style=\"color:#9edae5ff;\"><h2 class=\"text-heading-large\">About the job (H-TS Task Scope Header)</h2></span><br />1 H-RQ) <span style=\"color:#bcbd22ff;\">Impact (H-RQ Required Qualifications Header)</span><br />2 O-TS) <span style=\"color:#9edae580;\">As a Senior Data Scientist on the Marketplace Product Data Science team, you will collaborate closely with diverse teams (including product managers, engineers, and fellow data scientists) to advance our understanding of the customer, make predictions to improve our products and experiences, and design and measure experiments. Our ideal candidate will be data-driven, intellectually curious, technically and statistically rigorous, and a clear communicator. (O-TS Task Scope Non-header)</span><br />3 O-TS) <span style=\"color:#9edae580;\">In this role you will bring strong technical expertise and develop a thorough understanding of our data to drive insights in the areas of customer acquisition, engagement, retention, and loyalty through causal inference and experimentation methods. You will contribute to organizational maturity in data storytelling and best-in-class ML and statistical approaches and design product experiments to test business hypotheses, apply rigorous statistical analyses, and interpret the results to draw impactful insights. (O-TS Task Scope Non-header)</span><br />4 H-RQ) <span style=\"color:#bcbd22ff;\">What You’ll Need to Be Successful (H-RQ Required Qualifications Header)</span><br /><hr />5 O-RQ) <span style=\"color:#bcbd2280;\">Advanced degree preferred in a quantitative field, and a minimum of 4 years of industry experience. (O-RQ Required Qualifications Non-header)</span><br />6 O-RQ) <span style=\"color:#bcbd2280;\">Experience with exploratory data analysis, statistical analysis and testing, experiment design and model development, and knowledge of how to leverage these skills to solve complex business problems. (O-RQ Required Qualifications Non-header)</span><br />7 O-RQ) <span style=\"color:#bcbd2280;\">Expertise in causal inference. (O-RQ Required Qualifications Non-header)</span><br />8 O-RQ) <span style=\"color:#bcbd2280;\">Proficiency in programming languages (we use Python) and experience with data analysis and machine learning libraries (e.g. TensorFlow, PyTorch, Airflow, etc). (O-RQ Required Qualifications Non-header)</span><br />9 O-RQ) <span style=\"color:#bcbd2280;\">Strong SQL skills (we use Snowflake). (O-RQ Required Qualifications Non-header)</span><br />10 O-RQ) <span style=\"color:#bcbd2280;\">Experience with a data visualization tool (we use Tableau). (O-RQ Required Qualifications Non-header)</span><br />11 O-RQ) <span style=\"color:#bcbd2280;\">Demonstrated ability to learn and embrace new technologies, applications, and solutions. (O-RQ Required Qualifications Non-header)</span><br />12 O-RQ) <span style=\"color:#bcbd2280;\">Strong written and verbal communication skills and the ability to explain technical topics to data scientists, engineers, and business partners. (O-RQ Required Qualifications Non-header)</span><br />13 O-RQ) <span style=\"color:#bcbd2280;\">Ability to work collaboratively in a dynamic, team-based environment. (O-RQ Required Qualifications Non-header)</span><br />14 O-RQ) <span style=\"color:#bcbd2280;\">Experience as a technical lead and a passion for mentoring. (O-RQ Required Qualifications Non-header)</span><br />15 O-RQ) <span style=\"color:#bcbd2280;\">Work Arrangement (O-RQ Required Qualifications Non-header)</span><br /><hr />16 O-IP) <span style=\"color:#ffbb7880;\">Shipt considers candidates located near a Shipt office or workspace in Birmingham, San Francisco, or Minneapolis to be hybrid, which means that they have the flexibility to work from home (with leader approval) or at a Shipt office in order to facilitate the ability to innovate, collaborate, and spark team connections. In-office expectations will vary by role and leader. Certain roles may require in-office presence on a full-time basis. Please work with your recruiter to learn more about the classification of this role. (O-IP Interview Procedures Non-header)</span><br />17 H-CS) <span style=\"color:#1f77b4ff;\">About Shipt (H-CS Corporate Scope Header)</span><br />18 O-CS) <span style=\"color:#1f77b480;\">Shipt is a retail tech company that connects people to reliable, high-quality delivery with a personal touch. Shipt connects customers to the things they want from the stores they love, retail businesses to more satisfied customers, and workers to new earning opportunities. (O-CS Corporate Scope Non-header)</span><br />19 O-CS) <span style=\"color:#1f77b480;\">At Shipt, we aim to put our team first to boost a sense of belonging, spark opportunities for growth, provide unique benefits and commit to giving back to our communities in ways that make life better, both personally and professionally. We understand that our service, our culture, and our connection to our communities are only made better by every single person who shows up to work here every day. (O-CS Corporate Scope Non-header)</span><br />20 O-IP) <span style=\"color:#ffbb7880;\">Learn More. (O-IP Interview Procedures Non-header)</span><br />21 O-CS) <span style=\"color:#1f77b480;\">Shipt is an independently operated, wholly owned subsidiary of Target Corporation and available in more than 5,000 U.S. cities. Shipt was founded and is headquartered in Birmingham, Alabama. For more information, please visit Shipt’s company site at Shipt.com. (O-CS Corporate Scope Non-header)</span><br />22 O-LN) <span style=\"color:#9467bd80;\">We are an equal opportunity employer and value diversity at our company. We do not discriminate on the basis of race, color, national origin, ethnicity, religion or religious belief, sex (including pregnancy, childbirth, or related medical conditions), sexual orientation, gender, gender identity, gender expression, transgender status, sexual stereotypes, age, military or veteran status, disability, or any other characteristic protected by law. (O-LN Legal Notifications Non-header)</span><br />23 O-IP) <span style=\"color:#ffbb7880;\">Please inform your recruiting contact upon initial connection if you need any accommodations. (O-IP Interview Procedures Non-header)</span><br />24 O-SP) <span style=\"color:#17becf80;\">Employees (and eligible family members) are covered by medical, dental, vision and more. Employees may enroll in our company’s 401k plan. Employees will also be eligible to receive discretionary vacation for exempt team members, paid holidays throughout the calendar year and paid sick leave. Other compensation includes eligibility for an annual bonus and the potential for restricted stock units based on role. (O-SP Supplemental Pay Non-header)</span><br />25 H-SP) <span style=\"color:#17becfff;\">Pay Range (H-SP Supplemental Pay Header)</span><br />26 O-OL) <span style=\"color:#c49c9480;\">Metro Areas of Boston, District of Columbia, Los Angeles, San Francisco, Seattle and New York City: (O-OL Office Location Non-header)</span><br />27 O-O) <span style=\"color:#8c564b80;\">$121,487-$242,974 (O-O Other Non-header)</span><br />28 H-RQ) <span style=\"color:#bcbd22ff;\">All other locations: (H-RQ Required Qualifications Header)</span><br />29 O-O) <span style=\"color:#8c564b80;\">$101,239-$202,478 (O-O Other Non-header)</span><br />30 O-SP) <span style=\"color:#17becf80;\">Please note that the salary range above is a guideline, and individual total compensation will vary based on factors such as qualifications, skill level, competencies, and work location. (O-SP Supplemental Pay Non-header)</span><br />"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15]\n",
      "Parts-of-speech displayed in 1 minute and 1 second\n"
     ]
    }
   ],
   "source": [
    "\n",
    "db_pos_list = []\n",
    "for navigable_parent in child_strs_list:\n",
    "    db_pos_list = cu.append_parts_of_speech_list(navigable_parent, pos_list=db_pos_list)\n",
    "pos_list, indices_list = su.visualize_basic_quals_section(crf_list, child_strs_list, db_pos_list=db_pos_list, verbose=True)\n",
    "duration_str = humanize.precisedelta(time.time() - t1, minimum_unit='seconds', format='%0.0f')\n",
    "speech_str = f'Parts-of-speech displayed in {duration_str}'; print(speech_str); speech_engine.say(speech_str); speech_engine.runAndWait()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91379a3d-1a77-4d8b-9ef6-a76f12204ac2",
   "metadata": {},
   "outputs": [],
   "source": [
    "raise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "26c4dd4e-1418-4420-aff5-ea7b65273194",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "67 H-TS) JOB LOCATION\n",
      "68 O-RQ) CA PRO LOS ANGELES\n"
     ]
    }
   ],
   "source": [
    "\n",
    "basic_quals_dict = nu.load_object('basic_quals_dict'); column_name = 'is_office_location'\n",
    "for idx in list(range(67, 69)):\n",
    "    child_str = child_strs_list[idx]; pos_symbol = pos_list[idx]\n",
    "    def do_cypher_tx(tx, navigable_parent, verbose=False):\n",
    "        cypher_str = '''\\n            MATCH (np:NavigableParents {navigable_parent: $navigable_parent})\\n            ''' + cu.return_everything_str + ';'\n",
    "        results_list = tx.run(query=cypher_str, parameters={'navigable_parent': navigable_parent})\n",
    "        return [dict(record.items()) for record in results_list]\n",
    "    with cu.driver.session() as session: row_objs_list = session.write_transaction(do_cypher_tx, navigable_parent=child_str, verbose=False)\n",
    "    print(str(basic_quals_dict[child_str]) + '\\n' if(child_str in basic_quals_dict) else '', end='')\n",
    "    print(f'{idx} {pos_symbol}) {child_str}')\n",
    "    def do_cypher_tx(tx, navigable_parent, verbose=False):\n",
    "        cypher_str = f'''\n",
    "            MATCH (np:NavigableParents {{navigable_parent: $navigable_parent}})\n",
    "            SET\n",
    "                np.is_job_title = {str(column_name == 'is_job_title').lower()},\n",
    "                np.is_corporate_scope = {str(column_name == 'is_corporate_scope').lower()},\n",
    "                np.is_task_scope = {str(column_name == 'is_task_scope').lower()},\n",
    "                np.is_educational_requirement = {str(column_name == 'is_educational_requirement').lower()},\n",
    "                np.is_minimum_qualification = {str(column_name == 'is_minimum_qualification').lower()},\n",
    "                np.is_preferred_qualification = {str(column_name == 'is_preferred_qualification').lower()},\n",
    "                np.is_supplemental_pay = {str(column_name == 'is_supplemental_pay').lower()},\n",
    "                np.is_office_location = {str(column_name == 'is_office_location').lower()},\n",
    "                np.is_job_duration = {str(column_name == 'is_job_duration').lower()},\n",
    "                np.is_interview_procedure = {str(column_name == 'is_interview_procedure').lower()},\n",
    "                np.is_legal_notification = {str(column_name == 'is_legal_notification').lower()},\n",
    "                np.is_other = {str(column_name == 'is_other').lower()},\n",
    "                np.is_posting_date = {str(column_name == 'is_posting_date').lower()}\n",
    "            ''' + cu.return_everything_str + ';'\n",
    "        return [dict(record.items()) for record in tx.run(query=cypher_str, parameters={'navigable_parent': navigable_parent})]\n",
    "    with cu.driver.session() as session: row_objs_list = session.write_transaction(do_cypher_tx, navigable_parent=child_str, verbose=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "1d97acb2-2b60-4a56-9b3b-ecfc2b6582e6",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[6, 21, 25, 26, 28, 30, 31, 32, 34, 35, 36, 37, 40, 41, 68]\n",
      "42 H-RQ) Professional Certifications are a plus.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Display the context of an individual child string\n",
    "idx = 42\n",
    "print(indices_list); child_str = child_strs_list[idx]; pos_symbol = pos_list[idx]; basic_quals_dict = nu.load_object('basic_quals_dict')\n",
    "print(str(basic_quals_dict[child_str]) + '\\n' if(child_str in basic_quals_dict) else '', end='')\n",
    "print(f'{idx} {pos_symbol}) {child_str}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "b2f46926-880b-4aed-bc99-4a6bd9ed7903",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pickling to C:\\Users\\daveb\\OneDrive\\Documents\\GitHub\\job-hunting\\saves\\pkl\\basic_quals_dict.pkl\n",
      "\"Professional Certifications are a plus.\" in basic_quals_dict: 0\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Hand-label this particular child string in the quals dictionary\n",
    "basic_quals_dict = nu.load_object('basic_quals_dict')\n",
    "basic_quals_dict[child_str] = 0\n",
    "nu.store_objects(basic_quals_dict=basic_quals_dict); print(f'\"{child_str}\" in basic_quals_dict: {basic_quals_dict[child_str]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "326ab607-cc5c-4476-b692-395b96d293ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "39 O-ER) Your Educational and Professional Qualifications:\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Fix Headers\n",
    "basic_quals_dict = nu.load_object('basic_quals_dict')\n",
    "for idx in [39]:\n",
    "    child_str = child_strs_list[idx]; pos_symbol = pos_list[idx]\n",
    "    print(str(basic_quals_dict[child_str]) + '\\n' if(child_str in basic_quals_dict) else '', end='')\n",
    "    print(f'{idx} {pos_symbol}) {child_str}')\n",
    "    def do_cypher_tx(tx, navigable_parent, verbose=False):\n",
    "        cypher_str = f'''\n",
    "            MATCH (np:NavigableParents {{navigable_parent: $navigable_parent}})\n",
    "            SET\n",
    "                np.is_header = true\n",
    "            ''' + cu.return_everything_str + ';'\n",
    "        return [dict(record.items()) for record in tx.run(query=cypher_str, parameters={'navigable_parent': navigable_parent})]\n",
    "    with cu.driver.session() as session: row_objs_list = session.write_transaction(do_cypher_tx, navigable_parent=child_str, verbose=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "24e7695d-caf6-416f-a761-ec17b839ccf9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20 H-TS) As a Senior Data Scientist, you will be at the forefront of our AI initiatives, collaborating with cross-functional teams to design, develop, and implement state-of-the-art AI solutions for our clients.\n",
      "21 O-RQ) You understand the data science lifecycle and are experienced in development and implementation of advanced statistical models and machine learning algorithms.\n",
      "22 H-RQ) You collaborate well with key stakeholders, uncovering key business objectives and requirements to translate them into actionable data and modeling recommendations.\n",
      "23 O-TS) You provide technical leadership throughout the project lifecycle. You guide and mentor development teams, data scientists, and engineers in implementing best practices for AI solution development, deployment, and maintenance.\n",
      "24 H-RQ) You stay up to date with the latest advancements in AI, data science, cloud computing, and related technologies. You are passionate about lifelong learning.\n",
      "25 O-RQ) You are seen as a valued contributor to thought leadership and encourage knowledge-sharing and collaboration across teams.\n",
      "26 O-RQ) You have interest in contributing to the preparation of client proposals and strategies to win new business.\n",
      "27 O-TS) You excel at identifying opportunities to integrate product solutions and resources to improve client service capabilities.\n",
      "28 O-RQ) You have interest in working with a diverse portfolio of clients across industries.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Fix Non-headers\n",
    "basic_quals_dict = nu.load_object('basic_quals_dict')\n",
    "for idx in range(20, 29):\n",
    "    child_str = child_strs_list[idx]; pos_symbol = pos_list[idx]\n",
    "    print(str(basic_quals_dict[child_str]) + '\\n' if(child_str in basic_quals_dict) else '', end='')\n",
    "    print(f'{idx} {pos_symbol}) {child_str}')\n",
    "    def do_cypher_tx(tx, navigable_parent, verbose=False):\n",
    "        cypher_str = f'''\n",
    "            MATCH (np:NavigableParents {{navigable_parent: $navigable_parent}})\n",
    "            SET\n",
    "                np.is_header = false\n",
    "            ''' + cu.return_everything_str + ';'\n",
    "        return [dict(record.items()) for record in tx.run(query=cypher_str, parameters={'navigable_parent': navigable_parent})]\n",
    "    with cu.driver.session() as session: row_objs_list = session.write_transaction(do_cypher_tx, navigable_parent=child_str, verbose=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "id": "4005f4a8-f07c-4538-8d1b-210d32dd3199",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'navigable_parent': '<b>Educational Requirements:</b>', 'is_header': True, 'is_task_scope': False, 'is_minimum_qualification': False, 'is_preferred_qualification': False, 'is_legal_notification': False, 'is_job_title': False, 'is_office_location': False, 'is_job_duration': False, 'is_supplemental_pay': False, 'is_educational_requirement': True, 'is_interview_procedure': False, 'is_corporate_scope': False, 'is_posting_date': False, 'is_other': False}]"
      ]
     },
     "execution_count": 177,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "# Manually set each feature\n",
    "def do_cypher_tx(tx, navigable_parent, verbose=False):\n",
    "    cypher_str = '''\n",
    "        MATCH (np:NavigableParents {navigable_parent: $navigable_parent})\n",
    "        SET\n",
    "            np.is_header = true,\n",
    "            np.is_task_scope = false,\n",
    "            np.is_minimum_qualification = false,\n",
    "            np.is_preferred_qualification = false,\n",
    "            np.is_educational_requirement = true,\n",
    "            np.is_legal_notification = false,\n",
    "            np.is_other = false,\n",
    "            np.is_corporate_scope = false,\n",
    "            np.is_job_title = false,\n",
    "            np.is_office_location = false,\n",
    "            np.is_job_duration = false,\n",
    "            np.is_supplemental_pay = false,\n",
    "            np.is_interview_procedure = false,\n",
    "            np.is_posting_date = false\n",
    "        ''' + cu.return_everything_str + ';'\n",
    "    return [dict(record.items()) for record in tx.run(query=cypher_str, parameters={'navigable_parent': navigable_parent})]\n",
    "with cu.driver.session() as session: row_objs_list = session.write_transaction(do_cypher_tx, navigable_parent=child_str, verbose=False)\n",
    "ihu.retrain_classifier(row_objs_list[0]['navigable_parent'], row_objs_list[0]['is_header'], verbose=False); row_objs_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "56d17dac-be9e-4bc3-aa01-58eed1baa9af",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'navigable_parent': '<i>Peloton does not accept unsolicited agency resumes. Agencies should not forward resumes to our jobs alias, Peloton employees or any other organization location. Peloton is not responsible for any agency fees related to unsolicited resumes.</i>', 'is_header': None, 'is_task_scope': None, 'is_minimum_qualification': None, 'is_preferred_qualification': None, 'is_legal_notification': None, 'is_job_title': None, 'is_office_location': None, 'is_job_duration': None, 'is_supplemental_pay': None, 'is_educational_requirement': None, 'is_interview_procedure': None, 'is_corporate_scope': None, 'is_posting_date': None, 'is_other': None}]"
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "# Show what's in the database already for this html string\n",
    "def do_cypher_tx(tx, navigable_parent, verbose=False):\n",
    "    cypher_str = '''\n",
    "        MATCH (np:NavigableParents {navigable_parent: $navigable_parent})\n",
    "        ''' + cu.return_everything_str + ';'\n",
    "    results_list = tx.run(query=cypher_str, parameters={'navigable_parent': navigable_parent})\n",
    "    return [dict(record.items()) for record in results_list]\n",
    "with cu.driver.session() as session: row_objs_list = session.write_transaction(do_cypher_tx, navigable_parent=child_str, verbose=False)\n",
    "row_objs_list"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "181dfa2e-4e09-4da7-b046-98f5c170ec67",
   "metadata": {},
   "source": [
    "\n",
    "----\n",
    "## Maintenance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "07255146-70be-4d52-942c-1505d6e02a9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Display cypher necessary to apply for all the jobs you qualify for that you haven't yet\n",
    "import pyperclip\n",
    "\n",
    "# lru.display_hunting_dataframe_as_histogram()\n",
    "cypher_str = f'''\n",
    "    // Get job application links\n",
    "    MATCH (fn:FileNames)\n",
    "    WHERE\n",
    "        fn.percent_fit >= 0.8 AND\n",
    "        ((fn.is_closed IS NULL) OR (fn.is_closed = false)) AND\n",
    "        ((fn.is_opportunity_application_emailed IS NULL) OR (fn.is_opportunity_application_emailed = false))\n",
    "    RETURN\n",
    "        fn.percent_fit AS percent_fit,\n",
    "        fn.file_name AS filename,\n",
    "        fn.posting_url AS posting_url\n",
    "    ORDER BY fn.percent_fit DESC;'''\n",
    "pyperclip.copy(cypher_str)\n",
    "row_objs_list = []\n",
    "with cu.driver.session() as session: row_objs_list = session.write_transaction(cu.do_cypher_tx, cypher_str)\n",
    "if row_objs_list:\n",
    "    df = DataFrame(row_objs_list)\n",
    "    display(df)\n",
    "    \n",
    "    for filename in df.filename:\n",
    "        print(f\"\"\"\n",
    "MATCH (fn:FileNames)\n",
    "WHERE fn.file_name IN [\"{filename}\"]\n",
    "SET fn.is_closed = true\n",
    "RETURN fn;\"\"\")\n",
    "        break\n",
    "    \n",
    "    for filename in df.filename:\n",
    "        print(f\"\"\"\n",
    "MATCH (fn:FileNames)\n",
    "WHERE fn.file_name IN [\"{filename}\"]\n",
    "SET fn.is_opportunity_application_emailed = true, fn.opportunity_application_email_date = date()\n",
    "RETURN fn;\"\"\")\n",
    "        break\n",
    "speech_str = 'Job application cypher code copied to clipboard'; speech_engine.say(speech_str); speech_engine.runAndWait()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "8626518c-6d69-45b9-9f9a-ebe909644d0d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "                MATCH\n",
      "                    (np:NavigableParents {navigable_parent: \"<li>EEO Statement: We are an equal opportunity employer and all qualified applicants will receive consideration for employment without regard to race, color, religion, sex, national origin, disability status, protected veteran status, gender identity, sexual orientation, pregnancy and pregnancy-related conditions, or any other characteristic protected by law.</li>\"}),\n",
      "                    (ht:HeaderTags {header_tag: \"li\"})\n",
      "                MERGE (ht)-[r:SUMMARIZES]->(np);\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Break up overly-long O-RQs:\n",
    "# Ensure you have already run the \"Fix Parts-of-Speech and Quals for \n",
    "# this posting\" cells above or displayed the context of an \n",
    "# individual child string above. Don't close the Notepad++ window \n",
    "# until you have replaced the child string\n",
    "def display_file_in_text_editor(file_name):\n",
    "    text_editor_path = r'C:\\Program Files\\Notepad++\\notepad++.exe'\n",
    "    file_path = osp.abspath(osp.join(hau.SAVES_HTML_FOLDER, file_name))\n",
    "    !\"{text_editor_path}\" \"{file_path}\"\n",
    "display_file_in_text_editor(file_name)\n",
    "cu.rebuild_filename_node(file_name, navigable_parent=None, verbose=True)\n",
    "speech_str = 'File name node rebuild completed'; speech_engine.say(speech_str); speech_engine.runAndWait()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "c767d2cb-2c13-421a-b29e-546d2b1b401f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pickling to C:\\Users\\daveb\\OneDrive\\Documents\\GitHub\\job-hunting\\saves\\pkl\\basic_quals_dict.pkl\n",
      "\"<li>Professional development assistance</li>\" in basic_quals_dict: False\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Remove this particular child string from the quals dictionary and database\n",
    "basic_quals_dict = nu.load_object('basic_quals_dict')\n",
    "child_str = child_strs_list[idx]\n",
    "basic_quals_dict.pop(child_str, None)\n",
    "# basic_quals_dict[child_str] = 0\n",
    "nu.store_objects(basic_quals_dict=basic_quals_dict)\n",
    "print(f'\"{child_str}\" in basic_quals_dict: {child_str in basic_quals_dict}')\n",
    "def do_cypher_tx(tx, qualification_str, verbose=False):\n",
    "    cypher_str = '''\n",
    "        MATCH (qs:QualificationStrings {qualification_str: $qualification_str})\n",
    "        DETACH DELETE qs;\n",
    "        '''\n",
    "    results_list = tx.run(query=cypher_str, parameters={'qualification_str': qualification_str})\n",
    "\n",
    "    return [dict(record.items()) for record in results_list]\n",
    "with cu.driver.session() as session:\n",
    "    row_objs_list = session.write_transaction(do_cypher_tx, qualification_str=child_str, verbose=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "872a9fa5-9f15-4bcc-a3df-b75dbb52ed51",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Remove file name from database\n",
    "for file_name in ['']:\n",
    "    cu.delete_filename_node(file_name, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd3aad62-d78e-4243-b46f-dec293d4552d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import re\n",
    "\n",
    "def separate_qualifications(quals_list):\n",
    "  \"\"\"\n",
    "  This function takes a list of qualifications and separates them into individual sentences.\n",
    "\n",
    "  Args:\n",
    "      quals_list: A list of strings, where each string represents a qualification.\n",
    "\n",
    "  Returns:\n",
    "      A list of lists, where each inner list contains the individual qualifications extracted from the corresponding element in the original quals_list.\n",
    "  \"\"\"\n",
    "  separated_quals = []\n",
    "  for qual in quals_list:\n",
    "    # Split qualifications based on commas, semicolons, and colons followed by whitespace\n",
    "    split_quals = re.split(r\", |; |:\", qual)\n",
    "    # Further split based on \"and\" if it's not the first word and there's punctuation before it\n",
    "    for i, sentence in enumerate(split_quals):\n",
    "      if i > 0 and re.search(r\"\\b(and)\\b\", sentence, flags=re.IGNORECASE) and re.search(r\"[,\\.;]\", split_quals[i-1]):\n",
    "        split_quals[i-1] += f\" {sentence.strip()}\"\n",
    "        split_quals.pop(i)\n",
    "    # Remove empty strings and leading/trailing whitespace\n",
    "    separated_quals.append([q.strip() for q in split_quals if q.strip()])\n",
    "  return separated_quals\n",
    "\n",
    "separated_qualifications = separate_qualifications(quals_list)\n",
    "\n",
    "# Print the separated qualifications\n",
    "for qual_set in separated_qualifications:\n",
    "  for qual in qual_set:\n",
    "    print(qual)\n",
    "  print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72f2828f-ab7b-4550-b5cc-92078c40f331",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Job Hunting (Python 3.10.9)",
   "language": "python",
   "name": "jh_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
