{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pretty printing has been turned OFF\n",
      "======== Neo4j/4.4.7 ========\n",
      "Utility libraries created in 5 seconds\n"
     ]
    }
   ],
   "source": [
    "\n",
    "%pprint\n",
    "%matplotlib inline\n",
    "import sys\n",
    "if (osp.join(os.pardir, 'py') not in sys.path): sys.path.insert(1, osp.join(os.pardir, 'py'))\n",
    "from jobpostlib import (crf, cu, datetime, duration, hau, hc, humanize, ihu, lru, nu, osp, scrfcu, slrcu, ssgdcu, su, t0, time, wsu, speech_engine)\n",
    "from pandas import DataFrame\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def get_pos_and_consecutives(file_name):\n",
    "    child_strs_list = ha.get_navigable_children_from_file(file_name)\n",
    "    child_tags_list = ha.construct_child_tags_list(child_strs_list)\n",
    "    feature_dict_list = cu.query_feature_dict_list(child_tags_list, child_strs_list)\n",
    "    feature_tuple_list = [\n",
    "        hc.get_feature_tuple(feature_dict, pos_lr_predict_single=None, pos_crf_predict_single=None, pos_sgd_predict_single=None)\n",
    "        for feature_dict in feature_dict_list\n",
    "    ]\n",
    "    pos_symbol_predictions_list = ea.CRF.predict_single(ea.sent2features(feature_tuple_list))\n",
    "    pos_list = []\n",
    "    for pos, feature_tuple in zip(pos_symbol_predictions_list, feature_tuple_list):\n",
    "        navigable_parent = feature_tuple[1]\n",
    "        if navigable_parent in ha.NAVIGABLE_PARENT_IS_HEADER_DICT:\n",
    "            pos_list = cu.append_parts_of_speech_list(navigable_parent, pos_list)\n",
    "        else:\n",
    "            pos_list.append(pos)\n",
    "    consecutives_list = []\n",
    "    for pos, v in groupby(pos_list):\n",
    "        consecutives_count = len(list(v))\n",
    "        consecutives_tuple = (pos, consecutives_count)\n",
    "        consecutives_list.append(consecutives_tuple)\n",
    "    \n",
    "    return consecutives_list, pos_list, child_strs_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "TITLE_SEPARATOR = ' - '\n",
    "PARENTH_REGEX = re.compile(r'[\\(\\[][^\\)\\]]+[\\)\\]]')\n",
    "def get_extracted_posting_location(file_name):\n",
    "    title_str = ' '.join(file_name.split('_')[:-1])\n",
    "    title_str = re.sub(' - *| *- ', TITLE_SEPARATOR, title_str)\n",
    "    matched_text_list = []\n",
    "    for match_obj in PARENTH_REGEX.finditer(title_str):\n",
    "        matched_text = match_obj.group()\n",
    "        matched_text_list.append(matched_text)\n",
    "    for matched_text in matched_text_list:\n",
    "        replacement_text = matched_text.replace(TITLE_SEPARATOR, '-')\n",
    "        replacement_text = replacement_text[1:-1].strip()\n",
    "        replacement_text = replacement_text.replace('. ', TITLE_SEPARATOR).replace(' + ', TITLE_SEPARATOR)\n",
    "        replacement_text = TITLE_SEPARATOR + replacement_text + TITLE_SEPARATOR\n",
    "        title_str = title_str.replace(matched_text, replacement_text)\n",
    "    posting_title_list = title_str.split(TITLE_SEPARATOR)\n",
    "    posting_title_list = [posting_title_str.strip() for posting_title_str in posting_title_list]\n",
    "    posting_title_list = list(filter(len, posting_title_list))\n",
    "    extracted_posting_location = posting_title_list.pop()\n",
    "    if extracted_posting_location == 'Indeed.com':\n",
    "        extracted_posting_location = posting_title_list.pop()\n",
    "    \n",
    "    return extracted_posting_location, posting_title_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "REMOTE_STRS_LIST = ['remote', 'telecommute', 'home-based', 'wfh', 'work from home']\n",
    "QUAL_STRS_LIST = ['citizen', 'clearance', 'usc', 'gc', 'ead']\n",
    "US_STRS_LIST = ['united states', 'us', 'usa', 'us citizens only']\n",
    "PAY_STRS_LIST = ['b2b', 'up to $']\n",
    "DURATION_STRS_LIST = ['direct hire']\n",
    "def create_details_lists(posting_title_list):\n",
    "    EXTRACTED_LOCATION_LIST = []\n",
    "    EXTRACTED_REQ_QUALS_LIST = []\n",
    "    extracted_job_title_list = []\n",
    "    EXTRACTED_PAY_LIST = []\n",
    "    duration_details = []\n",
    "    while len(posting_title_list):\n",
    "        posting_title_str = posting_title_list.pop()\n",
    "        if any(map(lambda x: x in posting_title_str.lower(), REMOTE_STRS_LIST)):\n",
    "            EXTRACTED_LOCATION_LIST.append(posting_title_str)\n",
    "        elif any(map(lambda x: re.search(f'\\\\b{x}\\\\b', posting_title_str.lower()), QUAL_STRS_LIST)):\n",
    "            EXTRACTED_REQ_QUALS_LIST.append(posting_title_str)\n",
    "        elif any(map(lambda x: x == posting_title_str.lower(), US_STRS_LIST)):\n",
    "            EXTRACTED_LOCATION_LIST.append(posting_title_str)\n",
    "        elif any(map(lambda x: x in posting_title_str.lower(), PAY_STRS_LIST)):\n",
    "            EXTRACTED_PAY_LIST.append(posting_title_str)\n",
    "        elif any(map(lambda x: x in posting_title_str.lower(), DURATION_STRS_LIST)):\n",
    "            duration_details.append(posting_title_str)\n",
    "        else:\n",
    "            posting_title_str = posting_title_str.strip()\n",
    "            if posting_title_str.startswith(','):\n",
    "                posting_title_str = posting_title_str[1:].strip()\n",
    "            extracted_job_title_list.append(posting_title_str)\n",
    "    \n",
    "    return EXTRACTED_LOCATION_LIST, EXTRACTED_REQ_QUALS_LIST, EXTRACTED_PAY_LIST, duration_details, extracted_job_title_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "COMMAS_REGEX = re.compile(r', ([^,]+),')\n",
    "REPLACEMENT_STR = f'{TITLE_SEPARATOR}\\\\1{TITLE_SEPARATOR}'\n",
    "def extract_job_title_info(file_name):\n",
    "    extracted_posting_location, posting_title_list = get_extracted_posting_location(file_name)\n",
    "    commas_list = []\n",
    "    while len(posting_title_list):\n",
    "        posting_title_str = posting_title_list.pop()\n",
    "        posting_title_str = re.sub(COMMAS_REGEX, REPLACEMENT_STR, posting_title_str)\n",
    "        if any(map(lambda x: x in posting_title_str.lower(), REMOTE_STRS_LIST)):\n",
    "            posting_title_str = re.sub(f\"-({'|'.join(REMOTE_STRS_LIST)})\", f'{TITLE_SEPARATOR}\\\\1', posting_title_str, 0, re.IGNORECASE)\n",
    "        commas_list.append(posting_title_str)\n",
    "    title_str = TITLE_SEPARATOR.join(commas_list)\n",
    "    posting_title_list = title_str.split(TITLE_SEPARATOR)\n",
    "    assert len(posting_title_list), f'{title_str}: {title_str.split(TITLE_SEPARATOR)}'\n",
    "    (EXTRACTED_LOCATION_LIST, EXTRACTED_REQ_QUALS_LIST, EXTRACTED_PAY_LIST, duration_details,\n",
    "     extracted_job_title_list) = create_details_lists(posting_title_list)\n",
    "    \n",
    "    return (extracted_posting_location, EXTRACTED_LOCATION_LIST, EXTRACTED_REQ_QUALS_LIST, EXTRACTED_PAY_LIST, duration_details,\n",
    "            extracted_job_title_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def get_section_list(pos):\n",
    "    if pos == 'H-JD':\n",
    "        return EXTRACTED_DURATION_LIST\n",
    "    elif pos == 'H-ER':\n",
    "        return EXTRACTED_EDUCATION_LIST\n",
    "    elif pos == 'H-SP':\n",
    "        return EXTRACTED_PAY_LIST\n",
    "    elif pos == 'H-PQ':\n",
    "        return EXTRACTED_PREF_QUALS_LIST\n",
    "    elif pos == 'H-TS':\n",
    "        return EXTRACTED_TASK_SCOPE_LIST\n",
    "    elif pos == 'H-OL':\n",
    "        return EXTRACTED_LOCATION_LIST\n",
    "    elif pos == 'H-IP':\n",
    "        return EXTRACTED_INTERVIEW_LIST\n",
    "    elif pos == 'H-LN':\n",
    "        return EXTRACTED_LEGAL_LIST\n",
    "    elif pos == 'H-RQ':\n",
    "        return EXTRACTED_REQ_QUALS_LIST\n",
    "    elif pos == 'H-CS':\n",
    "        return EXTRACTED_CORP_LIST\n",
    "    elif pos == 'H-O':\n",
    "        return EXTRACTED_OTHER_LIST\n",
    "    \n",
    "    return []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import random\n",
    "\n",
    "files_list = os.listdir(ha.SAVES_HTML_FOLDER)\n",
    "file_name = random.choice(files_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "EXTRACTED_EDUCATION_LIST = []\n",
    "EXTRACTED_PREF_QUALS_LIST = []\n",
    "EXTRACTED_TASK_SCOPE_LIST = []\n",
    "EXTRACTED_INTERVIEW_LIST = []\n",
    "EXTRACTED_LEGAL_LIST = []\n",
    "EXTRACTED_CORP_LIST = []\n",
    "EXTRACTED_OTHER_LIST = []\n",
    "(extracted_posting_location, EXTRACTED_LOCATION_LIST, EXTRACTED_REQ_QUALS_LIST, EXTRACTED_PAY_LIST, EXTRACTED_DURATION_LIST,\n",
    " extracted_job_title_list) = extract_job_title_info(file_name)\n",
    "def get_pos_list(file_name):\n",
    "    child_strs_list = ha.get_navigable_children_from_file(file_name)\n",
    "    child_tags_list = ha.construct_child_tags_list(child_strs_list)\n",
    "    feature_dict_list = cu.query_feature_dict_list(child_tags_list, child_strs_list)\n",
    "    feature_tuple_list = [hc.get_feature_tuple(feature_dict, pos_lr_predict_single=None, pos_crf_predict_single=None, pos_sgd_predict_single=None) for feature_dict in feature_dict_list]\n",
    "    pos_symbol_predictions_list = ea.CRF.predict_single(ea.sent2features(feature_tuple_list))\n",
    "    pos_list = []\n",
    "    for pos, feature_tuple in zip(pos_symbol_predictions_list, feature_tuple_list):\n",
    "        navigable_parent = feature_tuple[1]\n",
    "        if navigable_parent in ha.NAVIGABLE_PARENT_IS_HEADER_DICT:\n",
    "            pos_list = cu.append_parts_of_speech_list(navigable_parent, pos_list)\n",
    "        else:\n",
    "            pos_list.append(pos)\n",
    "    \n",
    "    return pos_list, child_strs_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('O', 5), ('H-LN', 1), ('O', 1), ('H-LN', 1), ('O', 7), ('H-RQ', 1), ('O', 11), ('H-LN', 1), ('O', 1), ('H-SP', 1), ('O', 11)]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from itertools import groupby\n",
    "\n",
    "#for file_name in files_list:\n",
    "pos_list, child_strs_list = get_pos_list(file_name)\n",
    "consecutives_list = []\n",
    "for key_pos, consecutive_value in groupby(pos_list):\n",
    "    value_list = list(consecutive_value)\n",
    "    consecutives_count = len(value_list)\n",
    "    consecutives_tuple = (key_pos, consecutives_count)\n",
    "    consecutives_list.append(consecutives_tuple)\n",
    "for pos in list(set(pos_list) - set(['O'])):\n",
    "    section_list = get_section_list(pos)\n",
    "    speech_parts_idx_list = ea.get_idx_list(pos_list, pos)\n",
    "    consecutives_idx_list = ea.get_idx_list(consecutives_list, (pos, 1))\n",
    "    for speech_part_idx, consecutives_idx in zip(speech_parts_idx_list, consecutives_idx_list):\n",
    "        if consecutives_idx+1 < len(consecutives_list):\n",
    "            o_count = consecutives_list[consecutives_idx+1][1]\n",
    "            if pos == 'H-RQ':\n",
    "                o_list = []\n",
    "                for child_str in child_strs_list[speech_part_idx:speech_part_idx+o_count+1]:\n",
    "                    if child_str not in ha.NAVIGABLE_PARENT_IS_QUAL_DICT:\n",
    "                        o_list.append(child_str)\n",
    "                    elif ha.NAVIGABLE_PARENT_IS_QUAL_DICT[child_str]:\n",
    "                        o_list.append(child_str)\n",
    "            else:\n",
    "                 o_list = child_strs_list[speech_part_idx:speech_part_idx+o_count+1]\n",
    "            section_list.extend(o_list)\n",
    "print(consecutives_list)\n",
    "#break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "H-LN Legal Notifications Header [5, 7, 27]\n",
      "H-RQ Required Qualifications Header [15]\n",
      "H-SP Supplemental Pay Header [29]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "for pos in list(set(pos_list) - set(['O'])):\n",
    "    print(pos, hc.POS_EXPLANATION_DICT[pos], ea.get_idx_list(pos_list, pos))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from random import shuffle\n",
    "\n",
    "POPABLE_FILES_LIST = os.listdir(ha.SAVES_HTML_FOLDER)\n",
    "shuffle(POPABLE_FILES_LIST)\n",
    "s.store_objects(POPABLE_FILES_LIST=POPABLE_FILES_LIST)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "ZMQInteractiveShell_obj = get_ipython()\n",
    "NAVIGABLE_PARENT_IS_QUAL_DICT = s.load_object('NAVIGABLE_PARENT_IS_QUAL_DICT')\n",
    "def get_pos_code():\n",
    "    POPABLE_FILES_LIST = s.load_object('POPABLE_FILES_LIST')\n",
    "    file_name = POPABLE_FILES_LIST.pop()\n",
    "    s.store_objects(POPABLE_FILES_LIST=POPABLE_FILES_LIST)\n",
    "    NAVIGABLE_PARENT_IS_QUAL_DICT = s.load_object('NAVIGABLE_PARENT_IS_QUAL_DICT')\n",
    "    output_str = f'# {file_name}\\n'\n",
    "    EXTRACTED_EDUCATION_LIST = []\n",
    "    EXTRACTED_PREF_QUALS_LIST = []\n",
    "    EXTRACTED_TASK_SCOPE_LIST = []\n",
    "    EXTRACTED_INTERVIEW_LIST = []\n",
    "    EXTRACTED_LEGAL_LIST = []\n",
    "    EXTRACTED_CORP_LIST = []\n",
    "    EXTRACTED_OTHER_LIST = []\n",
    "    (extracted_posting_location, EXTRACTED_LOCATION_LIST, EXTRACTED_REQ_QUALS_LIST, EXTRACTED_PAY_LIST, EXTRACTED_DURATION_LIST,\n",
    "     extracted_job_title_list) = extract_job_title_info(file_name)\n",
    "    pos_list, child_strs_list = get_pos_list(file_name)\n",
    "    consecutives_list = []\n",
    "    for key_pos, consecutive_value in groupby(pos_list):\n",
    "        value_list = list(consecutive_value)\n",
    "        consecutives_count = len(value_list)\n",
    "        consecutives_tuple = (key_pos, consecutives_count)\n",
    "        consecutives_list.append(consecutives_tuple)\n",
    "    for pos in list(set(pos_list) - set(['O'])):\n",
    "        section_list = get_section_list(pos)\n",
    "        speech_parts_idx_list = ea.get_idx_list(pos_list, pos)\n",
    "        consecutives_idx_list = ea.get_idx_list(consecutives_list, (pos, 1))\n",
    "        for speech_part_idx, consecutives_idx in zip(speech_parts_idx_list, consecutives_idx_list):\n",
    "            if consecutives_idx+1 < len(consecutives_list):\n",
    "                o_count = consecutives_list[consecutives_idx+1][1]\n",
    "                if pos == 'H-RQ':\n",
    "                    o_list = []\n",
    "                    for child_str in child_strs_list[speech_part_idx:speech_part_idx+o_count+1]:\n",
    "                        if child_str not in ha.NAVIGABLE_PARENT_IS_QUAL_DICT:\n",
    "                            o_list.append(child_str)\n",
    "                        elif ha.NAVIGABLE_PARENT_IS_QUAL_DICT[child_str]:\n",
    "                            o_list.append(child_str)\n",
    "                else:\n",
    "                     o_list = child_strs_list[speech_part_idx:speech_part_idx+o_count+1]\n",
    "                section_list.extend(o_list)\n",
    "        if len(section_list):\n",
    "            output_str += '\\n'\n",
    "            output_str += '# '\n",
    "            output_str += pos\n",
    "            output_str += ' '\n",
    "            output_str += hc.POS_EXPLANATION_DICT[pos]\n",
    "            output_str += '\\n'\n",
    "            output_str += 'for tag_str in [\\n'\n",
    "            for child_str in section_list:\n",
    "                if child_str not in NAVIGABLE_PARENT_IS_QUAL_DICT:\n",
    "                    if \"'\" in child_str:\n",
    "                        output_str += f'        \"{child_str}\",\\n'\n",
    "                    else:\n",
    "                        output_str += f\"        '{child_str}',\\n\"\n",
    "            output_str += '    ]:\\n'\n",
    "            if pos in ['H-RQ', 'H-PQ']:\n",
    "                output_str += '    NAVIGABLE_PARENT_IS_QUAL_DICT[tag_str] = True\\n'\n",
    "            else:\n",
    "                output_str += '    NAVIGABLE_PARENT_IS_QUAL_DICT[tag_str] = False\\n'\n",
    "            output_str += '''print(len(NAVIGABLE_PARENT_IS_QUAL_DICT.keys()))\n",
    "s.store_objects(NAVIGABLE_PARENT_IS_QUAL_DICT=NAVIGABLE_PARENT_IS_QUAL_DICT)\\n'''\n",
    "    \n",
    "    return output_str"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "---\n",
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "ZMQInteractiveShell_obj.set_next_input(text=get_pos_code(), replace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "---\n",
    "# Inference"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "----\n",
    "# Clean up some missing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "child_strs_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       false
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "child_str = '<b>About the Team</b>'\n",
    "child_str in ha.NAVIGABLE_PARENT_IS_HEADER_DICT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pickling to C:\\Users\\dev\\Documents\\Repositories\\job-hunting\\saves\\pickle\\NAVIGABLE_PARENT_IS_HEADER_DICT.pickle\n"
     ]
    }
   ],
   "source": [
    "\n",
    "ha.NAVIGABLE_PARENT_IS_HEADER_DICT[child_str] = True\n",
    "s.store_objects(NAVIGABLE_PARENT_IS_HEADER_DICT=ha.NAVIGABLE_PARENT_IS_HEADER_DICT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       false
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "child_str in hc.CORPORATE_SCOPE_HEADERS_LIST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pickling to C:\\Users\\dev\\Documents\\Repositories\\job-hunting\\saves\\pickle\\CORPORATE_SCOPE_HEADERS_LIST.pickle\n"
     ]
    }
   ],
   "source": [
    "\n",
    "hc.CORPORATE_SCOPE_HEADERS_LIST.append(child_str)\n",
    "s.store_objects(CORPORATE_SCOPE_HEADERS_LIST=hc.CORPORATE_SCOPE_HEADERS_LIST)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pickling to C:\\Users\\dev\\Documents\\Repositories\\job-hunting\\saves\\pickle\\POPABLE_FILES_LIST.pickle\n"
     ]
    }
   ],
   "source": [
    "# Software_Engineer_-_Machine_Learning_-_Remote_-_Indeed.com_f6b4585215cfe949.html\n",
    "\n",
    "# H-TS Task Scope Header\n",
    "for tag_str in [\n",
    "        '<h2 class=\"jobsearch-JobDescriptionSection-jobDescriptionTitle icl-u-xs-my--md\" id=\"jobDescriptionTitle\">Full Job Description</h2>',\n",
    "        \"<p>O'Reilly Media is looking for a machine learning engineer to inform the direction and execution of our internal personalization and user tracking system built to support our learning platform.</p>\",\n",
    "    ]:\n",
    "    NAVIGABLE_PARENT_IS_QUAL_DICT[tag_str] = False\n",
    "print(len(NAVIGABLE_PARENT_IS_QUAL_DICT.keys()))\n",
    "s.store_objects(NAVIGABLE_PARENT_IS_QUAL_DICT=NAVIGABLE_PARENT_IS_QUAL_DICT)\n",
    "\n",
    "# H-RQ Required Qualifications Header\n",
    "for tag_str in [\n",
    "        '<b>Requirements</b>',\n",
    "        \"<li>A bachelor's degree in a math focused discipline, or equivalent real world experience</li>\",\n",
    "        '<li>Experience working with a modern data science / statistics platform such as scikit, R, SAS, matlab or similar.</li>',\n",
    "        '<li>Experience with how to design features for use in a linear model and/or a neural network to achieve specific predictive outcomes.</li>',\n",
    "        '<li>Previous experience working with an object oriented programming language such as Java or Scala.</li>',\n",
    "        '<li>The extreme desire to learn and solve problems you have never encountered before.</li>',\n",
    "        '<b>Preferred skills</b>',\n",
    "        '<li>Experience with big data systems like Hadoop, Spark, Beam, HBase, etc.</li>',\n",
    "        '<li>Strong data structure and algorithm knowledge</li>',\n",
    "        '<li>TensorFlow / DNNs and Language Models</li>',\n",
    "    ]:\n",
    "    NAVIGABLE_PARENT_IS_QUAL_DICT[tag_str] = True\n",
    "print(len(NAVIGABLE_PARENT_IS_QUAL_DICT.keys()))\n",
    "s.store_objects(NAVIGABLE_PARENT_IS_QUAL_DICT=NAVIGABLE_PARENT_IS_QUAL_DICT)\n",
    "\n",
    "# H-LN Legal Notifications Header\n",
    "for tag_str in [\n",
    "        '<b>About the Team</b>',\n",
    "        \"<p>O’Reilly Media’s Search and Personalization Platform Engineering team supports O’Reilly’s premier learning service by building out personalization systems built on data and machine learning to provide a unique and dynamic experience for all of our users. The O'Reilly Learning Platform is used by technologists, managers, and designers around the world to hone their skills and improve their craft. O'Reilly's personalization team is one of many small teams that are broadly distributed across the US, featuring diverse, tightly collaborative groups of developers, designers, and product managers constantly encouraging each other to deliver work that instills pride and fulfillment. We encourage learning, knowledge sharing, growth and collaboration in all aspects.</p>\",\n",
    "        '<b>About the Job</b>',\n",
    "        '<p>At O’Reilly our engineers focus on building a diverse set of features designed in collaboration with product managers, UX and other teammates. Engineers work with product managers to refine direction and solve user problems, exchange code reviews with other team members, provide mentorship to junior engineers, and assist QA and Ops in troubleshooting product issues.</p>',\n",
    "        '<p>In this role you will be primarily building data processing systems and developing machine learning models to provide intelligent predictions that are used within the search and personalization team or other internal teams to drive improved learning outcomes across the platform. In addition to building models, you will also be responsible for building and maintaining web services that provide search and personalization services to our users. This can include building apis, managing search engines, as well as building any other systems required to meet our goals.</p>',\n",
    "        '<b>About You</b>',\n",
    "        '<p>We are interested in people who have experience building and supporting web applications with a diverse and engaged user base. We desire candidates who work comfortably in an agile environment and with collaborators who are distributed across multiple time zones. We value colleagues who are helpful, respectful, humble, and always willing to do what’s best for our users. We desire developers who treat automated tests as essential, and believe that code reviews are a crucial path of learning and of sharing knowledge. The women and men of our platform team have taken many traditional and nontraditional paths to the developer profession, and we welcome diverse teams that are bound together by a mutual love of learning.</p>',\n",
    "        '<b>Bonus skills</b>',\n",
    "        '<li>Finagle, GCP, Apache Beam</li>',\n",
    "        '<li>Docker + Kubernetes</li>',\n",
    "        '<b>About the Company</b>',\n",
    "        '<p>O’Reilly Media has been inspiring the future for more than 40 years!</p>',\n",
    "        '<p>We share the knowledge and teach the skills people need to change their world. For more than 40 years, O’Reilly has imparted the world-shaping ideas of innovators through books, articles, conferences, and our online learning platform.</p>',\n",
    "        \"<p>When individuals, teams, and entire enterprises connect with the world's leading experts and content providers, anything is possible. Whether you're working to advance your career, be a better manager, or achieve the next breakthrough in technology or business, learning new skills is at the heart of it all.</p>\",\n",
    "        '<p>With a range of formats including live online training courses, interactive tutorials, books, videos, and case studies, we equip all members of the workforce with the insight they need to stay ahead in an ever-changing economy.</p>',\n",
    "        '<p>Learning should not be measured in terms of usage, course completion, or certifications. Learning should be measured in terms of outcomes. We want to prepare people to solve challenging problems and inspire them with what’s possible for the future.</p>',\n",
    "        '<p>Something we’ve been doing for over 40 years.</p>',\n",
    "    ]:\n",
    "    NAVIGABLE_PARENT_IS_QUAL_DICT[tag_str] = False\n",
    "print(len(NAVIGABLE_PARENT_IS_QUAL_DICT.keys()))\n",
    "s.store_objects(NAVIGABLE_PARENT_IS_QUAL_DICT=NAVIGABLE_PARENT_IS_QUAL_DICT)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
