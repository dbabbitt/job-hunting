{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d03d3049-8477-46ba-98dd-d92a962f9311",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pretty printing has been turned OFF\n"
     ]
    }
   ],
   "source": [
    "\n",
    "%pprint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "758868ff-9571-4004-b7f3-2ba81353d2dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "%matplotlib inline\n",
    "from IPython.display import clear_output, display\n",
    "from cohere.error import CohereAPIError\n",
    "from langchain.chains import LLMChain\n",
    "from langchain.llms import Cohere, OpenAI\n",
    "from langchain.prompts import PromptTemplate\n",
    "from openai.error import RateLimitError\n",
    "from pandas import DataFrame\n",
    "from ratelimit import limits, sleep_and_retry\n",
    "from tqdm.notebook import tqdm\n",
    "import humanize\n",
    "import numpy as np\n",
    "import os\n",
    "import re\n",
    "import sys\n",
    "import time\n",
    "import warnings\n",
    "import winsound\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "duration = 1000  # milliseconds\n",
    "freq = 880  # Hz\n",
    "\n",
    "# Insert at 1, 0 is the script path (or '' in REPL)\n",
    "if (osp.join('..', 'py') not in sys.path): sys.path.insert(1, osp.join('..', 'py'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5ebb3871-9513-4d4d-9feb-b1d33b3bc8dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Get the Storage object\n",
    "from storage import Storage\n",
    "s = Storage(\n",
    "    data_folder_path=os.path.abspath('../data'),\n",
    "    saves_folder_path=os.path.abspath('../saves')\n",
    ")\n",
    "\n",
    "# Get the WebScrapingUtilities object\n",
    "from scrape_utils import WebScrapingUtilities\n",
    "wsu = WebScrapingUtilities(\n",
    "    s=s,\n",
    "    secrets_json_path=os.path.abspath('../data/secrets/jh_secrets.json')\n",
    ")\n",
    "\n",
    "# To get your API key, visit https://serpapi.com/dashboard\n",
    "os.environ['SERPAPI_API_KEY'] = wsu.secrets_json['SERPAPI_API_KEY']\n",
    "\n",
    "# To get your API key, visit https://dashboard.cohere.ai/api-keys\n",
    "os.environ['COHERE_API_KEY'] = wsu.secrets_json['Cohere_API_Key']\n",
    "\n",
    "# To get your API key, visit https://beta.dreamstudio.ai/membership\n",
    "os.environ['STABILITY_KEY'] = wsu.secrets_json['Dream_Studio_API_Key']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "bcf27bb5-7a04-4a3a-8843-db4e86c49286",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I have 49,102 labeled parts of speech in here\n",
      "predict_single is now available\n",
      "Parts-of-speech logistic regression elements built in 9 seconds\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Get the HeaderAnalysis object\n",
    "from ha_utils import HeaderAnalysis\n",
    "ha = HeaderAnalysis(s=s, verbose=False)\n",
    "\n",
    "# Get the CypherUtilities object and Neo4j driver\n",
    "from cypher_utils import CypherUtilities\n",
    "uri = wsu.secrets_json['neo4j']['connect_url']\n",
    "user =  wsu.secrets_json['neo4j']['username']\n",
    "password = wsu.secrets_json['neo4j']['password']\n",
    "cu = CypherUtilities(\n",
    "    uri=uri, user=user, password=password, driver=None, s=s, ha=ha\n",
    ")\n",
    "\n",
    "# Get the SectionLRClassifierUtilities object\n",
    "from section_classifier_utils import SectionLRClassifierUtilities\n",
    "slrcu = SectionLRClassifierUtilities(ha=ha, cu=cu, verbose=False)\n",
    "\n",
    "# Check if the slrcu has built its parts-of-speech logistic regression elements\n",
    "t1 = time.time()\n",
    "if not hasattr(slrcu, 'pos_predict_percent_fit_dict'):\n",
    "    slrcu.build_pos_logistic_regression_elements(sampling_strategy_limit=None, verbose=True)\n",
    "if hasattr(slrcu, 'pos_predict_percent_fit_dict'):\n",
    "    print('predict_single is now available')\n",
    "else:\n",
    "    print('predict_single is not available')\n",
    "duration_str = humanize.precisedelta(time.time() - t1, minimum_unit='seconds', format='%0.0f')\n",
    "print(f'Parts-of-speech logistic regression elements built in {duration_str}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "bb8dc862-7853-4070-a82d-15f73a13732e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "predict_single is now available\n",
      "Parts-of-speech conditional random field elements built in 1 second\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Get the SectionCRFClassifierUtilities object\n",
    "from section_classifier_utils import SectionCRFClassifierUtilities\n",
    "scrfcu = SectionCRFClassifierUtilities(cu=cu, ha=ha, verbose=False)\n",
    "\n",
    "# Check if the scrfcu has built its parts-of-speech conditional random field elements\n",
    "t1 = time.time()\n",
    "if not hasattr(scrfcu, 'pos_symbol_crf'):\n",
    "    scrfcu.build_pos_conditional_random_field_elements(verbose=True)\n",
    "if hasattr(scrfcu, 'pos_predict_percent_fit_dict'):\n",
    "    print('predict_single is now available')\n",
    "else:\n",
    "    print('predict_single is not available')\n",
    "duration_str = humanize.precisedelta(time.time() - t1, minimum_unit='seconds', format='%0.0f')\n",
    "print(f'Parts-of-speech conditional random field elements built in {duration_str}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8a1cba7b-4a6e-49e2-bd6b-7094e1020ad4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I have 49,102 labeled parts of speech in here\n",
      "predict_single is now available\n",
      "Parts-of-speech stochastic gradient descent elements built in 9 seconds\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Get the SectionSGDClassifierUtilities object\n",
    "from section_classifier_utils import SectionSGDClassifierUtilities\n",
    "ssgdcu = SectionSGDClassifierUtilities(ha=ha, cu=cu, verbose=False)\n",
    "\n",
    "# Check if the ssgdcu has built its parts-of-speech stochastic gradient decent elements\n",
    "t1 = time.time()\n",
    "if not hasattr(ssgdcu, 'pos_predict_percent_fit_dict'):\n",
    "    ssgdcu.build_pos_stochastic_gradient_descent_elements(sampling_strategy_limit=None, verbose=True)\n",
    "if hasattr(ssgdcu, 'pos_predict_percent_fit_dict'):\n",
    "    print('predict_single is now available')\n",
    "else:\n",
    "    print('predict_single is not available')\n",
    "duration_str = humanize.precisedelta(time.time() - t1, minimum_unit='seconds', format='%0.0f')\n",
    "print(f'Parts-of-speech stochastic gradient descent elements built in {duration_str}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b5df02c7-cbb9-4291-8120-bbab653dc5de",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: []\n",
       "Index: []"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "tf_cypher_str = \"\"\"\n",
    "    // Find all properties with a value of \"False\"\n",
    "    CALL db.propertyKeys() YIELD propertyKey AS prop_name\n",
    "    MATCH (n)\n",
    "    WHERE n[prop_name] = \"False\"\n",
    "    WITH DISTINCT prop_name, labels(n)[0] AS node_name\n",
    "    RETURN prop_name, node_name;\"\"\"\n",
    "with cu.driver.session() as session: tf_df = DataFrame(session.write_transaction(cu.do_cypher_tx, tf_cypher_str))\n",
    "display(tf_df.head(20))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "12e15212-b7ab-414c-a661-88c2c9b3fb9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "for row_index, row_series in tf_df.iterrows():\n",
    "    prop_name = row_series.prop_name\n",
    "    node_name = row_series.node_name\n",
    "    def do_cypher_tx(tx):\n",
    "        cypher_str = f\"\"\"\n",
    "            // Replace \"True\" with true and \"False\" with false\n",
    "            MATCH (n:{node_name})\n",
    "            SET n.{prop_name} = CASE n.{prop_name}\n",
    "              WHEN \"False\" THEN false\n",
    "              WHEN \"True\" THEN true\n",
    "              ELSE n.{prop_name}\n",
    "            END;\"\"\"\n",
    "        tx.run(query=cypher_str, parameters={})\n",
    "    with cu.driver.session() as session: session.write_transaction(do_cypher_tx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9ad18769-78bb-49e3-a776-53c26b410089",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'TypeError'> error trying to load davez_hunting_df: Date.__new__() missing 3 required positional arguments: 'year', 'month', and 'day'\n",
      "<class 'TypeError'> error trying to load hunting_df: Date.__new__() missing 3 required positional arguments: 'year', 'month', and 'day'\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Fix all the pickles\n",
    "for file_name in os.listdir(s.saves_pickle_folder):\n",
    "    if file_name.endswith('.pkl'):\n",
    "        pickle_name = file_name[:-4]\n",
    "        if pickle_name.endswith('_df'):\n",
    "            if s.pickle_exists(pickle_name):\n",
    "                try:\n",
    "                    df = s.load_object(pickle_name)\n",
    "                    for cn in df.columns:\n",
    "                        if ('True' in df[cn].unique()) or ('False' in df[cn].unique()):\n",
    "                            print(pickle_name, cn)\n",
    "                except Exception as e:\n",
    "                    print(f'{e.__class__.__name__} error trying to load {pickle_name}: {str(e).strip()}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1808238-71fc-48a2-8cc8-ecdc0295b436",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Job Hunting (Python 3.10.9)",
   "language": "python",
   "name": "jh_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
