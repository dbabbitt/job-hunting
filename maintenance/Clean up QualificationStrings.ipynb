{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "fff7ed8c-f67a-4261-b5e8-8c06b5726b9b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pretty printing has been turned OFF\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to C:\\Users\\daveb/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "# Load needed libraries and functions\n",
    "%matplotlib inline\n",
    "%pprint\n",
    "import sys\n",
    "import os.path as osp\n",
    "\n",
    "executable_path = sys.executable; scripts_folder = osp.join(osp.dirname(executable_path), 'Scripts')\n",
    "py_folder = osp.abspath('../py'); ffmpeg_folder = r'C:\\ffmpeg\\bin'\n",
    "if (scripts_folder not in sys.path): sys.path.insert(1, scripts_folder)\n",
    "if (py_folder not in sys.path): sys.path.insert(1, py_folder)\n",
    "if (ffmpeg_folder not in sys.path): sys.path.insert(1, ffmpeg_folder)\n",
    "from jobpostlib import (cu, datetime, nu, humanize, time, lru)\n",
    "from nltk.tokenize import sent_tokenize\n",
    "from pandas import DataFrame\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "import nltk\n",
    "import numpy as np\n",
    "import pyperclip\n",
    "import re\n",
    "\n",
    "# Download necessary NLTK data\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "59c66877-dc03-4fbb-8db6-54880fe93d26",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I have 424,879 is-qualified vocabulary tokens in here\n",
      "Is-qualified LR elements built in 9 seconds\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Check if the lru has built its is-qualified classifier\n",
    "t1 = time.time()\n",
    "# lru.basic_quals_dict = None; lru.sync_basic_quals_dict()\n",
    "if not (hasattr(lru, 'ISQUALIFIED_LR') and hasattr(lru, 'ISQUALIFIED_CV')):\n",
    "    lru.build_isqualified_logistic_regression_elements(sampling_strategy_limit=None, verbose=True)\n",
    "duration_str = humanize.precisedelta(time.time() - t1, minimum_unit='seconds', format='%0.0f')\n",
    "speech_str = f'Is-qualified LR elements built in {duration_str}'; print(speech_str)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85280862-7b83-4267-98ac-d084737772c9",
   "metadata": {},
   "source": [
    "\n",
    "----"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 426,
   "id": "e0bd0dc9-0b1f-4bb7-84e1-3062cc73537c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "basic_quals_df.shape: (18257, 2)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Get the databased quals\n",
    "cypher_str = '''\n",
    "    // Get all qualification strings in the database\n",
    "    MATCH (qs:QualificationStrings)\n",
    "    RETURN qs;'''\n",
    "row_objs_list = []\n",
    "with cu.driver.session() as session: row_objs_list = session.write_transaction(cu.do_cypher_tx, cypher_str)\n",
    "if row_objs_list:\n",
    "    basic_quals_df = DataFrame(\n",
    "        [{k: v for k, v in row_obj['qs'].items()} for row_obj in row_objs_list]\n",
    "    ).drop_duplicates()\n",
    "    shape_tuple = basic_quals_df.shape\n",
    "    pyperclip.copy(str(shape_tuple))\n",
    "    print(f'basic_quals_df.shape: {shape_tuple}') # (18257, 2)\n",
    "\n",
    "# Step 1: Preprocess the sentences\n",
    "flat_sentences = sorted([sent for qualification_str in basic_quals_df.qualification_str for sent in sent_tokenize(qualification_str)], key=lambda x: len(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 427,
   "id": "23962321-4782-4b7b-b44f-2568a3ac97f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Step 2: Convert sentences to numerical representations\n",
    "vectorizer = TfidfVectorizer()\n",
    "X = vectorizer.fit_transform(flat_sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 429,
   "id": "88c8348b-8b36-4ba5-bbb7-683a5632b388",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Clusters assigned in 1 minute and 21 seconds\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Step 3: Choose a clustering algorithm (DBSCAN in this example)\n",
    "t1 = time.time()\n",
    "from sklearn.cluster import DBSCAN\n",
    "dbscan = DBSCAN(eps=0.5, min_samples=5)  # You may need to adjust these parameters\n",
    "\n",
    "# Get cluster assignments\n",
    "cluster_assignments = dbscan.fit_predict(X)\n",
    "\n",
    "# Apply PCA to reduce dimensionality to 2\n",
    "pca = PCA(n_components=2)\n",
    "X_2d = pca.fit_transform(X.toarray())\n",
    "\n",
    "# Separate non-noise points\n",
    "non_noise_mask = cluster_assignments != -1\n",
    "X_2d_non_noise = X_2d[non_noise_mask]\n",
    "cluster_assignments_non_noise = cluster_assignments[non_noise_mask]\n",
    "duration_str = humanize.precisedelta(time.time() - t1, minimum_unit='seconds', format='%0.0f')\n",
    "speech_str = f'Clusters assigned in {duration_str}'; print(speech_str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 430,
   "id": "7235542b-16d9-43ef-8522-a9b307c010e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of clusters found: 60\n",
      "Noise points: 18744\n",
      "\n",
      "Example sentences from Cluster 0:\n",
      "- ;\n",
      "- ;\n",
      "- ;\n",
      "\n",
      "Example sentences from Cluster 1:\n",
      "- Min.\n",
      "- Min.\n",
      "- Min.\n",
      "\n",
      "Example sentences from Cluster 2:\n",
      "- </div>\n",
      "- </div>\n",
      "- </div>\n",
      "\n",
      "Example sentences from Cluster 3:\n",
      "- is a plus\n",
      "- is a plus\n",
      "- is a plus.\n",
      "\n",
      "Example sentences from Cluster 4:\n",
      "- Master Degree\n",
      "- Master's degree\n",
      "- Master’s degree\n",
      "\n",
      "Example sentences from Cluster 5:\n",
      "- Bachelor Degree\n",
      "- Bachelor degree\n",
      "- Bachelor s degree\n",
      "\n",
      "Example sentences from Cluster 6:\n",
      "- SQL - 3 years\n",
      "- 5+ Years of SQL.\n",
      "- 3+ years of SQL experience.\n",
      "\n",
      "Example sentences from Cluster 7:\n",
      "- Python - 3 years\n",
      "- Python 5+ years,\n",
      "- 5+ years in python\n",
      "\n",
      "Example sentences from Cluster 8:\n",
      "- Proficiency Python\n",
      "- Python proficiency\n",
      "- Proficiency in Python\n",
      "\n",
      "Example sentences from Cluster 9:\n",
      "- Must be US Citizen\n",
      "- Must be US citizen\n",
      "- Must be US Citizen.\n",
      "\n",
      "Example sentences from Cluster 10:\n",
      "- What motivates you?\n",
      "- What motivates you?\n",
      "- What motivates you?\n",
      "\n",
      "Example sentences from Cluster 11:\n",
      "- Docker and Kubernetes.\n",
      "- Kubernetes and/or Docker\n",
      "- Experience in Docker/Kubernetes\n",
      "\n",
      "Example sentences from Cluster 12:\n",
      "- SQL: 3 years (Required)\n",
      "- SQL: 2 years (Required)\n",
      "- SQL: 4 years (Required)\n",
      "\n",
      "Example sentences from Cluster 13:\n",
      "- AWS: 4 years (Required)\n",
      "- AWS: 6 years (Required)\n",
      "- aws: 3 years (Required)\n",
      "\n",
      "Example sentences from Cluster 14:\n",
      "- Master Degree Preferred\n",
      "- Master's degree preferred\n",
      "- Master\\'s degree preferred\n",
      "\n",
      "Example sentences from Cluster 15:\n",
      "- US Citizenship required.\n",
      "- US Citizenship Required?\n",
      "- US Citizenship: Required\n",
      "\n",
      "Example sentences from Cluster 16:\n",
      "- SQL: 5 years (Preferred)\n",
      "- SQL: 8 years (Preferred)\n",
      "- SQL: 7 years (Preferred)\n",
      "\n",
      "Example sentences from Cluster 17:\n",
      "- Python: 6 years (Required)\n",
      "- Python: 3 years (Required)\n",
      "- Python: 5 years (Required)\n",
      "\n",
      "Example sentences from Cluster 18:\n",
      "- Python: 5 years (Preferred)\n",
      "- Python: 3 years (Preferred)\n",
      "- Python: 7 years (Preferred)\n",
      "\n",
      "Example sentences from Cluster 19:\n",
      "- data visualization and tools\n",
      "- Experience with data visualization tools\n",
      "- Experience with data visualization tools\n",
      "\n",
      "Example sentences from Cluster 20:\n",
      "- Agile Methodology\n",
      "- Agile methodology\n",
      "- Agile Development Methodology\n",
      "\n",
      "Example sentences from Cluster 21:\n",
      "- 3+ years of Python experience\n",
      "- 3+ years experience with Python\n",
      "- 1+ years experience with Python\n",
      "\n",
      "Example sentences from Cluster 22:\n",
      "- Bachelor’s degree or equivalent\n",
      "- Bachelor’s degree or equivalent.\n",
      "- Bachelor’s (or equivalent) degree.\n",
      "\n",
      "Example sentences from Cluster 23:\n",
      "- data science: 7 years (Required)\n",
      "- Data science: 5 years (Required)\n",
      "- Data Science: 6 years (Required)\n",
      "\n",
      "Example sentences from Cluster 24:\n",
      "- Strong programming skills\n",
      "- Strong Python programming skills\n",
      "- Strong Python programming skills.\n",
      "\n",
      "Example sentences from Cluster 25:\n",
      "- Natural Language Processing (NLP)\n",
      "- Natural language processing: 2 years (Required)\n",
      "- Natural language processing: 5 years (Required)\n",
      "\n",
      "Example sentences from Cluster 26:\n",
      "- 5+ years experience in data science\n",
      "- 4+ years of data science experience\n",
      "- 3+ years of data science experience\n",
      "\n",
      "Example sentences from Cluster 27:\n",
      "- Machine Learning: 5 years (Required)\n",
      "- Machine Learning: 8 years (Required)\n",
      "- Machine Learning, 3 years (Required)\n",
      "\n",
      "Example sentences from Cluster 28:\n",
      "- 5+ years of relevant work experience\n",
      "- 4-6 years of relevant work experience\n",
      "- 4+ years of relevant work experience.\n",
      "\n",
      "Example sentences from Cluster 29:\n",
      "- 5 years experience as a Data Scientist\n",
      "- 3 years of experience as a Data Scientist\n",
      "- 5 years of experience as a Data Scientist\n",
      "\n",
      "Example sentences from Cluster 30:\n",
      "- Excellent Verbal/Written communication\n",
      "- Excellent verbal and written communication\n",
      "- Excellent written and verbal communication\n",
      "\n",
      "Example sentences from Cluster 31:\n",
      "- Problem Solving/Analytical\n",
      "- Excellent problem solving skills\n",
      "- 9.Excellent problem solving skills\n",
      "\n",
      "Example sentences from Cluster 32:\n",
      "- Strong written and verbal communication\n",
      "- Strong written and verbal communication skills\n",
      "- Strong verbal and written communication skills\n",
      "\n",
      "Example sentences from Cluster 33:\n",
      "- Python, Jupyter Notebooks\n",
      "- Experience with Jupyter notebooks\n",
      "- Experience with Python/Jupyter Notebooks\n",
      "\n",
      "Example sentences from Cluster 34:\n",
      "- 0-5 years software development experience\n",
      "- 7+ years of software development experience\n",
      "- 5+ years of software development experience\n",
      "\n",
      "Example sentences from Cluster 35:\n",
      "- Strong written and oral communication skills\n",
      "- Strong oral and written communication skills.\n",
      "- Strong written and oral communication skills.\n",
      "\n",
      "Example sentences from Cluster 36:\n",
      "- Good verbal and written communication skills\n",
      "- Good written and verbal communication skills\n",
      "- Good verbal and written communication skills.\n",
      "\n",
      "Example sentences from Cluster 37:\n",
      "- relational databases and SQL\n",
      "- Experience with relational databases\n",
      "- Experience with Relational Databases, e.g.\n",
      "\n",
      "Example sentences from Cluster 38:\n",
      "- Strong communication and interpersonal skills\n",
      "- Strong interpersonal and communication skills\n",
      "- Strong communication and interpersonal skills\n",
      "\n",
      "Example sentences from Cluster 39:\n",
      "- Building and Training Machine Learning Models\n",
      "- Building and Training Machine Learning Models Required 3 Years\n",
      "- Building and Training Machine Learning Models Required 3 Years.\n",
      "\n",
      "Example sentences from Cluster 40:\n",
      "- Experience with MATLAB, Python, C/C++, Julia,\n",
      "- Experience with MATLAB, Python, C/C++, Julia, etc\n",
      "- Experience with Matlab, Python, C/C++, Julia, etc\n",
      "\n",
      "Example sentences from Cluster 41:\n",
      "- Excellent communication & presentation skills.\n",
      "- Excellent communication and presentation skills\n",
      "- Excellent communication and presentation skills.\n",
      "\n",
      "Example sentences from Cluster 42:\n",
      "- Data Analysis & Interpretation (P3 - Advanced)\n",
      "- 2 - Data Analysis & Interpretation (P3 - Advanced)\n",
      "- Data Analysis &amp; Interpretation (P3 - Advanced)\n",
      "\n",
      "Example sentences from Cluster 43:\n",
      "- Excellent written and oral communication skills\n",
      "- Excellent oral and written communication skills\n",
      "- Excellent written and oral presentation skills.\n",
      "\n",
      "Example sentences from Cluster 44:\n",
      "- Bachelor's in Computer Science or related field\n",
      "- Bachelor in Computer Science or a related field\n",
      "- Bachelor’s degree in computer science or related field\n",
      "\n",
      "Example sentences from Cluster 45:\n",
      "- Strong communication and presentation skills\n",
      "- Strong communication and presentation skills.\n",
      "- Strong communication and presentation skills.\n",
      "\n",
      "Example sentences from Cluster 46:\n",
      "- Ability to work in fast paced environment\n",
      "- Ability to work in a fast paced environment\n",
      "- Ability to work in a fast-paced environment.\n",
      "\n",
      "Example sentences from Cluster 47:\n",
      "- Excellent verbal, written, and interpersonal skills\n",
      "- Excellent written &amp; verbal communication skills\n",
      "- Excellent interpersonal, verbal and written communication skills\n",
      "\n",
      "Example sentences from Cluster 48:\n",
      "- Proficiency in programming languages such as Python\n",
      "- Proficiency in programming languages such as Python.\n",
      "- Proficiency in programming languages such as Python.\n",
      "\n",
      "Example sentences from Cluster 49:\n",
      "- Clearance: Ability to Obtain a Public Trust\n",
      "- Ability to obtain and maintain a Public Trust.\n",
      "- Ability to obtain and maintain a public trust clearance.\n",
      "\n",
      "Example sentences from Cluster 50:\n",
      "- Additional years’ experience may be used in lieu of degree\n",
      "- Additional years experience may be used in lieu of degree.\n",
      "- Additional years’ experience may be used in lieu of degree.\n",
      "\n",
      "Example sentences from Cluster 51:\n",
      "- Bachelor’s in computer science, information technology or related field\n",
      "- Bachelor’s degree in computer science, Information Technology, or a related field.\n",
      "- Bachelor's degree in Computer Science, Information Technology, or a related field.\n",
      "\n",
      "Example sentences from Cluster 52:\n",
      "- ), preferably in Statistics, or equivalent, At least 6 years clinical trial experience\n",
      "- ), preferably in Statistics, or equivalent, At least 8 years clinical trial experience\n",
      "- ), preferably in Statistics, or equivalent, At least 4 years clinical trial experience\n",
      "\n",
      "Example sentences from Cluster 53:\n",
      "- Applied statistics skills, such as distributions, statistical testing, regression, etc\n",
      "- Good applied statistics skills, such as distributions, statistical testing, regression, etc\n",
      "- Good applied statistics skills, such as distributions, statistical testing, regression, etc.\n",
      "\n",
      "Example sentences from Cluster 54:\n",
      "- Experience with exploratory data analysis using tools like iPython Notebook, Pandas & matplotlib, etc\n",
      "- Experience with exploratory data analysis using tools like iPython Notebook, Pandas & matplotlib, etc\n",
      "- Experience with exploratory data analysis using tools like iPython Notebook, Pandas & matplotlib, etc.\n",
      "\n",
      "Example sentences from Cluster 55:\n",
      "- 10+ years experience with R, SQL, Python, or other data science programming languages; OR Master's/PhD\n",
      "- 10+ years experience with R, SQL, Python, or other data science programming languages; OR Master's/PhD degree and 5+ years experience\n",
      "- 10+ years' experience with R, SQL, Python, or other data science programming languages; OR Master's/PhD degree and 5+ years experience\n",
      "\n",
      "Example sentences from Cluster 56:\n",
      "- Specific vision abilities required by this job include close vision and the ability to adjust focus\n",
      "- Specific vision abilities required by this job include close vision and the ability to adjust focus.\n",
      "- Specific vision abilities required by this job include close vision, distance vision, and ability to adjust focus.\n",
      "\n",
      "Example sentences from Cluster 57:\n",
      "- A graduate degree in statistics, data science, or similar STEM field with 2+ years of experience in a relevant role\n",
      "- A graduate degree in statistics, data science, or similar STEM field with 5+ years of experience in a relevant role\n",
      "- A bachelor degree in statistics, data science, or similar STEM field with 5+ years of experience in a relevant role OR\n",
      "\n",
      "Example sentences from Cluster 58:\n",
      "- Bachelor’s degree in computer science, data science, applied mathematics, statistics, electrical engineering, or related discipline.\n",
      "- Master's Degree and 8 years in computer science, data science, applied mathematics, statistics, electrical engineering, or related discipline\n",
      "- Master's Degree and 8 years in computer science, data science, applied mathematics, statistics, electrical engineering, or related discipline.\n",
      "\n",
      "Example sentences from Cluster 59:\n",
      "- Gone deep with cohort and funnel analyses, a deep understanding statistical concepts such as selection bias, probability distributions, and conditional probabilities\n",
      "- Gone deep with cohort and funnel analyses, a deep understanding statistical concepts such as selection bias, probability distributions, and conditional probabilities\n",
      "- You have: Gone deep with cohort and funnel analyses, a deep understanding statistical concepts such as selection bias, probability distributions, and conditional probabilities\n",
      "\n",
      "Example sentences from Noise:\n",
      "- PhD\n",
      "- BS.\n",
      "- SQL?\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Print some statistics\n",
    "n_clusters = len(set(cluster_assignments)) - (1 if -1 in cluster_assignments else 0)\n",
    "print(f\"Number of clusters found: {n_clusters}\")\n",
    "\n",
    "for cluster in set(cluster_assignments):\n",
    "    cluster_size = np.sum(cluster_assignments == cluster)\n",
    "    if cluster == -1:\n",
    "        print(f\"Noise points: {cluster_size}\")\n",
    "    # else:\n",
    "    #     print(f\"Cluster {cluster} size: {cluster_size}\")\n",
    "\n",
    "# Print a few example sentences from each cluster\n",
    "for cluster in set(cluster_assignments):\n",
    "    if cluster == -1:\n",
    "        print(f\"\\nExample sentences from Noise:\")\n",
    "    else:\n",
    "        print(f\"\\nExample sentences from Cluster {cluster}:\")\n",
    "    cluster_sentences = [sent for sent, clust in zip(flat_sentences, cluster_assignments) if clust == cluster]\n",
    "    for sentence in cluster_sentences[:3]:  # Print first 3 sentences\n",
    "        print(f\"- {sentence}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1007e53-39f4-4f5b-b04c-fd5b4343a05b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Create a scatter plot\n",
    "plt.figure(figsize=(12, 8))\n",
    "\n",
    "# Plot only non-noise points\n",
    "scatter = plt.scatter(\n",
    "    X_2d_non_noise[:, 0], X_2d_non_noise[:, 1], c=cluster_assignments_non_noise,\n",
    "    cmap='viridis', alpha=0.7\n",
    ")\n",
    "\n",
    "plt.colorbar(scatter)\n",
    "\n",
    "# Add labels and title\n",
    "plt.xlabel('First Principal Component')\n",
    "plt.ylabel('Second Principal Component')\n",
    "plt.title('DBSCAN Cluster Visualization of Sentences (Noise Points Hidden)')\n",
    "\n",
    "# Show the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ddf1b32-8402-433f-8e1a-a167503f2b02",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Step 3: Choose a clustering algorithm (KMeans in this example)\n",
    "n_clusters = 3  # You can adjust this number\n",
    "kmeans = KMeans(n_clusters=n_clusters, random_state=42)\n",
    "\n",
    "# Step 4: Apply the clustering algorithm\n",
    "kmeans.fit(X)\n",
    "\n",
    "# Get cluster assignments for each sentence\n",
    "cluster_assignments = kmeans.labels_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "640211b9-9ac8-4dfd-97a3-3a1c74c1b1ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Print results\n",
    "for i, sentence in enumerate(flat_sentences):\n",
    "    print(f\"Sentence: {sentence}\")\n",
    "    print(f\"Cluster: {cluster_assignments[i]}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f922fa76-9586-4a7d-b6c2-52fed1434668",
   "metadata": {},
   "source": [
    "\n",
    "----\n",
    "## Maintenance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 421,
   "id": "a5243e47-9cdc-45e0-86cd-cb512910acd7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "basic_quals_df.shape: (21, 2)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Get the bad databased quals\n",
    "cypher_str = '''\n",
    "    // Get all qualification strings in the database\n",
    "    MATCH (qs:QualificationStrings)\n",
    "    WHERE qs.qualification_str =~ '^[^A-Za-z0-9].*'\n",
    "    RETURN qs;'''\n",
    "row_objs_list = []\n",
    "with cu.driver.session() as session: row_objs_list = session.write_transaction(cu.do_cypher_tx, cypher_str)\n",
    "if row_objs_list:\n",
    "    basic_quals_df = DataFrame(\n",
    "        [{k: v for k, v in row_obj['qs'].items()} for row_obj in row_objs_list]\n",
    "    ).drop_duplicates()\n",
    "    shape_tuple = basic_quals_df.shape\n",
    "    pyperclip.copy(str(shape_tuple))\n",
    "    print(f'basic_quals_df.shape: {shape_tuple}') # (28, 2)\n",
    "\n",
    "# Step 1: Preprocess the sentences\n",
    "flat_sentences = sorted([sent for qualification_str in basic_quals_df.qualification_str for sent in sent_tokenize(qualification_str)], key=lambda x: len(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 422,
   "id": "b811dc44-91c8-49d4-a339-7e6b35077f33",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['.NET Core (P3 - Advanced)', '.NET: 10 years (Preferred)', '(Nice-to-have) AWS experience', '.NET Core 3.1 or above: 3 years (Required)', '(Optimal) Publication in robotics/ML/CV conference', '+3 years experience with the Go programming language', '(preferred) Experience with Agile software development', '+1 years of professional experience with dbt (cloud or core)', '[Bonus] You might have experience with Snowflake, Jupyter, Pandas.', '(Desired) Experience with ETL tools such as Azure Data Factory or Apache NiFi.']"
      ]
     },
     "execution_count": 422,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "flat_sentences[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 425,
   "id": "8f0b9529-f566-4ae2-abd0-19bfa3f17203",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 425,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "poppable_list = [qs for qs in basic_quals_df.qualification_str if (len(min(sent_tokenize(qs), key=lambda s: len(s))) < 3000) and all(map(\n",
    "    lambda x: not qs.startswith(x), ['.NET', '(Nice-to-have) ', '[Bonus] ', '(Optimal) ', '+3 years ', '(preferred) ', '+1 years ', '(Desired) ', '(Optional) ', '&gt; ', '(3-6) years ', '+2 years ']\n",
    "))]\n",
    "poppable_list[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 419,
   "id": "27bfbd3f-934f-474f-8552-ba37a930675e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0) (Google Earth Engine, Amazon Web Services) to obtain and analyze data, and analysis and visualization technologies such as Drupal, Leaflet, R Shiny, Tableau, etc.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "child_str = poppable_list.pop()\n",
    "print(f'{len(poppable_list)}) {child_str}')\n",
    "pyperclip.copy(f'\\nnew_child_str = \"{child_str}\"')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 413,
   "id": "ac6f9b0c-b3de-4765-a91d-988f5f90b000",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pickling to C:\\Users\\daveb\\OneDrive\\Documents\\GitHub\\job-hunting\\saves\\pkl\\basic_quals_dict.pkl\n",
      "\"(Please note, at this time, remote work in this role cannot be carried out from states on the West coast, within the Pacific Time Zone\" in basic_quals_dict: False\n",
      "\"Please note, at this time, remote work in this role cannot be carried out from states on the West coast, within the Pacific Time Zone\" in basic_quals_dict: True\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Replace this particular child string in the quals dictionary\n",
    "new_child_str = \"Please note, at this time, remote work in this role cannot be carried out from states on the West coast, within the Pacific Time Zone\"\n",
    "basic_quals_dict = nu.load_object('basic_quals_dict')\n",
    "if child_str in basic_quals_dict:\n",
    "    basic_quals_dict[new_child_str] = basic_quals_dict[child_str]\n",
    "    basic_quals_dict.pop(child_str, None)\n",
    "    nu.store_objects(basic_quals_dict=basic_quals_dict)\n",
    "print(f'\"{child_str}\" in basic_quals_dict: {child_str in basic_quals_dict}')\n",
    "print(f'\"{new_child_str}\" in basic_quals_dict: {new_child_str in basic_quals_dict}')\n",
    "\n",
    "# Replace this particular child string in the database\n",
    "def do_cypher_tx(tx, old_child_str, new_child_str):\n",
    "    cypher_str = '''\n",
    "        MATCH (qs:QualificationStrings {qualification_str: $old_child_str})\n",
    "        SET qs.qualification_str = $new_child_str;\n",
    "        '''\n",
    "    results_list = tx.run(query=cypher_str, parameters={'old_child_str': old_child_str, 'new_child_str': new_child_str})\n",
    "\n",
    "    return [dict(record.items()) for record in results_list]\n",
    "with cu.driver.session() as session:\n",
    "    row_objs_list = session.write_transaction(do_cypher_tx, old_child_str=child_str, new_child_str=new_child_str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 420,
   "id": "da7b5c65-b156-45ab-ab9b-aabd95a52b16",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pickling to C:\\Users\\daveb\\OneDrive\\Documents\\GitHub\\job-hunting\\saves\\pkl\\basic_quals_dict.pkl\n",
      "\"(Google Earth Engine, Amazon Web Services) to obtain and analyze data, and analysis and visualization technologies such as Drupal, Leaflet, R Shiny, Tableau, etc.\" in basic_quals_dict: False\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Remove this particular child string from the quals dictionary\n",
    "basic_quals_dict = nu.load_object('basic_quals_dict')\n",
    "basic_quals_dict.pop(child_str, None)\n",
    "nu.store_objects(basic_quals_dict=basic_quals_dict)\n",
    "print(f'\"{child_str}\" in basic_quals_dict: {child_str in basic_quals_dict}')\n",
    "\n",
    "# Remove this particular child string from the database\n",
    "def do_cypher_tx(tx, qualification_str, verbose=False):\n",
    "    cypher_str = '''\n",
    "        MATCH (qs:QualificationStrings {qualification_str: $qualification_str})\n",
    "        DETACH DELETE qs;\n",
    "        '''\n",
    "    results_list = tx.run(query=cypher_str, parameters={'qualification_str': qualification_str})\n",
    "\n",
    "    return [dict(record.items()) for record in results_list]\n",
    "with cu.driver.session() as session:\n",
    "    row_objs_list = session.write_transaction(do_cypher_tx, qualification_str=child_str, verbose=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 304,
   "id": "026d9d50-c3b9-4791-9b8c-75a3b61b60fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Remove *s: 100%|█████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00,  3.59it/s]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Replace child strings beginning with and ending with an asterisk in the quals dictionary\n",
    "pattern = r'\\*([^<]+)\\*'\n",
    "poppable_list = [child_str for child_str in basic_quals_df.qualification_str if re.search(pattern, child_str)]\n",
    "progress_bar = tqdm(\n",
    "    poppable_list, total=len(poppable_list), desc=f\"Remove *s\"\n",
    ")\n",
    "for child_str in progress_bar:\n",
    "    new_child_str = re.sub(pattern, r'\\g<1>', child_str).strip()\n",
    "    basic_quals_dict = nu.load_object('basic_quals_dict')\n",
    "    if child_str in basic_quals_dict:\n",
    "        basic_quals_dict[new_child_str] = basic_quals_dict[child_str]\n",
    "        basic_quals_dict.pop(child_str, None)\n",
    "        nu.store_objects(basic_quals_dict=basic_quals_dict, verbose=False)\n",
    "    \n",
    "    # Replace this particular child string in the database\n",
    "    def do_cypher_tx(tx, old_child_str, new_child_str):\n",
    "        cypher_str = '''\n",
    "            MATCH (qs:QualificationStrings {qualification_str: $old_child_str})\n",
    "            SET qs.qualification_str = $new_child_str;\n",
    "            '''\n",
    "        results_list = tx.run(query=cypher_str, parameters={'old_child_str': old_child_str, 'new_child_str': new_child_str})\n",
    "    \n",
    "        return [dict(record.items()) for record in results_list]\n",
    "    with cu.driver.session() as session:\n",
    "        row_objs_list = session.write_transaction(do_cypher_tx, old_child_str=child_str, new_child_str=new_child_str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 345,
   "id": "53eefb35-2561-494d-ae24-2f654c751ea0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Remove <orq>s: 100%|█████████████████████████████████████████████████████████████████████| 9/9 [00:01<00:00,  5.54it/s]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Replace child strings beginning with (3) or some other number in the quals dictionary\n",
    "pattern = r'^\\(\\d+\\) (.+)'\n",
    "poppable_list = [child_str for child_str in basic_quals_df.qualification_str if re.search(pattern, child_str)]\n",
    "progress_bar = tqdm(\n",
    "    poppable_list, total=len(poppable_list), desc=f\"Remove <orq>s\"\n",
    ")\n",
    "for child_str in progress_bar:\n",
    "    new_child_str = re.sub(pattern, r'\\g<1>', child_str).strip()\n",
    "    basic_quals_dict = nu.load_object('basic_quals_dict')\n",
    "    if child_str in basic_quals_dict:\n",
    "        basic_quals_dict[new_child_str] = basic_quals_dict[child_str]\n",
    "        basic_quals_dict.pop(child_str, None)\n",
    "        nu.store_objects(basic_quals_dict=basic_quals_dict, verbose=False)\n",
    "    \n",
    "    # Replace this particular child string in the database\n",
    "    def do_cypher_tx(tx, old_child_str, new_child_str):\n",
    "        cypher_str = '''\n",
    "            MATCH (qs:QualificationStrings {qualification_str: $old_child_str})\n",
    "            SET qs.qualification_str = $new_child_str;\n",
    "            '''\n",
    "        results_list = tx.run(query=cypher_str, parameters={'old_child_str': old_child_str, 'new_child_str': new_child_str})\n",
    "    \n",
    "        return [dict(record.items()) for record in results_list]\n",
    "    with cu.driver.session() as session:\n",
    "        row_objs_list = session.write_transaction(do_cypher_tx, old_child_str=child_str, new_child_str=new_child_str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 405,
   "id": "3b2e802d-b959-46c9-b513-a3aef2c4b45f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Remove  s: 0it [00:00, ?it/s]\n",
      "Remove - s: 0it [00:00, ?it/s]\n",
      "Remove · s: 0it [00:00, ?it/s]\n",
      "Remove • s: 0it [00:00, ?it/s]\n",
      "Remove •s: 0it [00:00, ?it/s]\n",
      "Remove , s: 0it [00:00, ?it/s]\n",
      "Remove ● s: 0it [00:00, ?it/s]\n",
      "Remove **s: 0it [00:00, ?it/s]\n",
      "Remove * s: 0it [00:00, ?it/s]\n",
      "Remove — s: 0it [00:00, ?it/s]\n",
      "Remove ✅ s: 0it [00:00, ?it/s]\n",
      "Remove + s: 0it [00:00, ?it/s]\n",
      "Remove \" s: 0it [00:00, ?it/s]\n",
      "Remove -s: 0it [00:00, ?it/s]\n",
      "Remove : s: 0it [00:00, ?it/s]\n",
      "Remove – s: 0it [00:00, ?it/s]\n",
      "Remove \\\"s: 0it [00:00, ?it/s]\n",
      "Remove ﻿s: 0it [00:00, ?it/s]\n",
      "Remove |s: 0it [00:00, ?it/s]\n",
      "Remove )s: 0it [00:00, ?it/s]\n",
      "Remove *s: 100%|█████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00,  5.97it/s]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "for prefix_str in [' ', '- ', '· ', '• ', '•', ', ', '● ', '**', '* ', '— ', '✅ ', '+ ', '\" ', '-', ': ', '– ', '\\\\\"', '\\ufeff', '|', ')', '*']:\n",
    "    \n",
    "    # Replace child strings beginning with prefix_str in the quals dictionary\n",
    "    poppable_list = [child_str for child_str in basic_quals_df.qualification_str if child_str.startswith(prefix_str)]\n",
    "    progress_bar = tqdm(\n",
    "        poppable_list, total=len(poppable_list), desc=f\"Remove {prefix_str}s\"\n",
    "    )\n",
    "    for child_str in progress_bar:\n",
    "        new_child_str = child_str.replace(prefix_str, '', ).strip()\n",
    "        basic_quals_dict = nu.load_object('basic_quals_dict')\n",
    "        if child_str in basic_quals_dict:\n",
    "            basic_quals_dict[new_child_str] = basic_quals_dict[child_str]\n",
    "            basic_quals_dict.pop(child_str, None)\n",
    "            nu.store_objects(basic_quals_dict=basic_quals_dict, verbose=False)\n",
    "        \n",
    "        # Replace this particular child string in the database\n",
    "        def do_cypher_tx(tx, old_child_str, new_child_str):\n",
    "            cypher_str = '''\n",
    "                MATCH (qs:QualificationStrings {qualification_str: $old_child_str})\n",
    "                SET qs.qualification_str = $new_child_str;\n",
    "                '''\n",
    "            results_list = tx.run(query=cypher_str, parameters={'old_child_str': old_child_str, 'new_child_str': new_child_str})\n",
    "        \n",
    "            return [dict(record.items()) for record in results_list]\n",
    "        with cu.driver.session() as session:\n",
    "            row_objs_list = session.write_transaction(do_cypher_tx, old_child_str=child_str, new_child_str=new_child_str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 303,
   "id": "2aa16b78-0bba-4b4c-9cb4-24e1c24b516a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Remove <orq>s: 0it [00:00, ?it/s]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Replace child strings beginning with <orq> and a number and ending in </orq> in the quals dictionary\n",
    "pattern = r'<orq>\\d+ - ([^<]+)</orq>'\n",
    "poppable_list = [child_str for child_str in basic_quals_df.qualification_str if re.search(pattern, child_str)]\n",
    "progress_bar = tqdm(\n",
    "    poppable_list, total=len(poppable_list), desc=f\"Remove <orq>s\"\n",
    ")\n",
    "for child_str in progress_bar:\n",
    "    new_child_str = re.sub(pattern, r'\\g<1>', child_str).strip()\n",
    "    basic_quals_dict = nu.load_object('basic_quals_dict')\n",
    "    if child_str in basic_quals_dict:\n",
    "        basic_quals_dict[new_child_str] = basic_quals_dict[child_str]\n",
    "        basic_quals_dict.pop(child_str, None)\n",
    "        nu.store_objects(basic_quals_dict=basic_quals_dict, verbose=False)\n",
    "    \n",
    "    # Replace this particular child string in the database\n",
    "    def do_cypher_tx(tx, old_child_str, new_child_str):\n",
    "        cypher_str = '''\n",
    "            MATCH (qs:QualificationStrings {qualification_str: $old_child_str})\n",
    "            SET qs.qualification_str = $new_child_str;\n",
    "            '''\n",
    "        results_list = tx.run(query=cypher_str, parameters={'old_child_str': old_child_str, 'new_child_str': new_child_str})\n",
    "    \n",
    "        return [dict(record.items()) for record in results_list]\n",
    "    with cu.driver.session() as session:\n",
    "        row_objs_list = session.write_transaction(do_cypher_tx, old_child_str=child_str, new_child_str=new_child_str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 279,
   "id": "f7811ae0-21b7-4d50-ad48-259737aa5ccc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Remove <li>s: 0it [00:00, ?it/s]\n",
      "Remove <div>s: 0it [00:00, ?it/s]\n",
      "Remove <p>s: 0it [00:00, ?it/s]\n",
      "Remove <b>s: 0it [00:00, ?it/s]\n",
      "Remove <i>s: 0it [00:00, ?it/s]\n",
      "Remove <span>s: 0it [00:00, ?it/s]\n",
      "Remove <em>s: 0it [00:00, ?it/s]\n",
      "Remove <orq>s: 0it [00:00, ?it/s]\n",
      "Remove <strong>s: 100%|████████████████████████████████████████████████████████████████| 14/14 [00:02<00:00,  6.83it/s]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "for tag in ['li', 'div', 'p', 'b', 'i', 'span', 'em', 'orq', 'strong']:\n",
    "    \n",
    "    # Replace child strings beginning with <tag> and (possibly) ending with </tag> in the quals dictionary\n",
    "    pattern = f'<{tag}[^>]*>(.+)(?:</{tag}>)?'\n",
    "    poppable_list = [child_str for child_str in basic_quals_df.qualification_str if re.search(pattern, child_str)]\n",
    "    progress_bar = tqdm(\n",
    "        poppable_list, total=len(poppable_list), desc=f\"Remove <{tag}>s\"\n",
    "    )\n",
    "    for child_str in progress_bar:\n",
    "        new_child_str = re.sub(pattern, r'\\g<1>', child_str).strip()\n",
    "        basic_quals_dict = nu.load_object('basic_quals_dict')\n",
    "        if child_str in basic_quals_dict:\n",
    "            basic_quals_dict[new_child_str] = basic_quals_dict[child_str]\n",
    "            basic_quals_dict.pop(child_str, None)\n",
    "            nu.store_objects(basic_quals_dict=basic_quals_dict, verbose=False)\n",
    "        \n",
    "        # Replace this particular child string in the database\n",
    "        def do_cypher_tx(tx, old_child_str, new_child_str):\n",
    "            cypher_str = '''\n",
    "                MATCH (qs:QualificationStrings {qualification_str: $old_child_str})\n",
    "                SET qs.qualification_str = $new_child_str;\n",
    "                '''\n",
    "            results_list = tx.run(query=cypher_str, parameters={'old_child_str': old_child_str, 'new_child_str': new_child_str})\n",
    "        \n",
    "            return [dict(record.items()) for record in results_list]\n",
    "        with cu.driver.session() as session:\n",
    "            row_objs_list = session.write_transaction(do_cypher_tx, old_child_str=child_str, new_child_str=new_child_str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "33b949b5-c15b-4d4b-9810-dc706815b76f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Remove short quals: 100%|██████████████████████████████████████████████████████████████| 15/15 [00:00<00:00, 82.62it/s]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Remove child strings in the quals dictionary that are only one character long\n",
    "poppable_list = [child_str for child_str in flat_sentences if len(child_str) < 2]\n",
    "progress_bar = tqdm(\n",
    "    poppable_list, total=len(poppable_list), desc=\"Remove short quals\"\n",
    ")\n",
    "for child_str in progress_bar:\n",
    "    basic_quals_dict = nu.load_object('basic_quals_dict')\n",
    "    if child_str in basic_quals_dict:\n",
    "        basic_quals_dict.pop(child_str, None)\n",
    "        nu.store_objects(basic_quals_dict=basic_quals_dict, verbose=False)\n",
    "\n",
    "# Remove these particular child strings from the database\n",
    "def do_cypher_tx(tx, verbose=False):\n",
    "    cypher_str = '''\n",
    "        // Delete qualification strings that are one character long\n",
    "        MATCH (qs:QualificationStrings)\n",
    "        WHERE SIZE(qs.qualification_str) = 1\n",
    "        DETACH DELETE qs;\n",
    "        '''\n",
    "    results_list = tx.run(query=cypher_str)\n",
    "\n",
    "    return [dict(record.items()) for record in results_list]\n",
    "with cu.driver.session() as session:\n",
    "    row_objs_list = session.write_transaction(do_cypher_tx, verbose=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "31071a8b-52c9-492c-93ac-a69ee86418b4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['.', '.', '?', '?', '?', '?', '?', '?', '?', '?']"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "flat_sentences[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "135becd7-bd51-4920-a6c2-743cd8e96258",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Get the one-character databased quals\n",
    "cypher_str = '''\n",
    "    // Get all qualification strings in the database\n",
    "    MATCH (qs:QualificationStrings)\n",
    "    WHERE SIZE(qs.qualification_str) = 1\n",
    "    RETURN qs;'''\n",
    "row_objs_list = []\n",
    "with cu.driver.session() as session: row_objs_list = session.write_transaction(cu.do_cypher_tx, cypher_str)\n",
    "if row_objs_list:\n",
    "    df = DataFrame(\n",
    "        [{k: v for k, v in row_obj['qs'].items()} for row_obj in row_objs_list]\n",
    "    ).drop_duplicates()\n",
    "    print(f'df.shape: {df.shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "023d4c09-c6af-4d47-b561-7c82fdcd9c0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Identify and remove duplicate qualification_str entries\n",
    "def do_cypher_tx(tx):\n",
    "    cypher_str = '''\n",
    "        // Identify and remove duplicate qualification_str entries\n",
    "        MATCH (qs:QualificationStrings)\n",
    "        WITH qs.qualification_str AS qual, COLLECT(qs) AS nodes\n",
    "        WHERE SIZE(nodes) > 1\n",
    "        WITH qual, nodes[0] AS keepNode, nodes[1..] AS duplicateNodes\n",
    "        DETACH DELETE duplicateNodes\n",
    "        RETURN COUNT(duplicateNodes) AS removedDuplicates;\n",
    "        '''\n",
    "    results_list = tx.run(query=cypher_str)\n",
    "\n",
    "    return [dict(record.items()) for record in results_list]\n",
    "with cu.driver.session() as session:\n",
    "    row_objs_list = session.write_transaction(do_cypher_tx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33337ad3-3f60-4fc9-af3d-c23e2496f394",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Job Hunting (Python 3.10.9)",
   "language": "python",
   "name": "jh_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
