{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "---\n",
    "# Load needed libraries and functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pretty printing has been turned OFF\n",
      "C:\\Users\\dev\\Documents\\Repositories\\job-hunting\\ipynb\\Indeed Header Classifier Scores.ipynb\n",
      "['s.attempt_to_pickle', 's.data_csv_folder', 's.data_folder', 's.encoding_type', 's.load_csv', 's.load_dataframes', 's.load_object', 's.save_dataframes', 's.saves_csv_folder', 's.saves_folder', 's.saves_pickle_folder', 's.store_objects']\n",
      "No pickle exists at C:\\Users\\dev\\Documents\\Repositories\\job-hunting\\saves\\pickle\\basic_tags_dict.pickle - attempting to load as csv.\n",
      "No csv exists at C:\\Users\\dev\\Documents\\Repositories\\job-hunting\\saves\\csv\\basic_tags_dict.csv - attempting to download from URL.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['AdaBoostClassifier', 'BaggingClassifier', 'Config', 'CountVectorizer', 'ExtraTreesClassifier', 'GradientBoostingClassifier', 'In', 'LogisticRegression', 'Out', 'RandomForestClassifier', 'SVC', 'StackingClassifier', 'Storage', 'TfidfTransformer', 'VotingClassifier', '_', '__', '___', '__builtin__', '__builtins__', '__doc__', '__loader__', '__name__', '__nonzero__', '__package__', '__spec__', '_dh', '_i', '_i1', '_ih', '_ii', '_iii', '_oh', 'basic_tags_dict', 'csv', 'entropy', 'exit', 'get_all_files_containing', 'get_classifier', 'get_data_structs_dataframe', 'get_datastructure_prediction', 'get_dir_tree', 'get_importances', 'get_input_sample', 'get_ipython', 'get_module_version', 'get_modules_dataframe', 'get_notebook_path', 'get_struct_name', 'inspect', 'ipykernel', 'json', 'jupyter_config_dir', 'notebook_path', 'notebookapp', 'np', 'os', 'pd', 'pickle', 'plt', 'preprocess_data', 'quit', 're', 's', 'set_matplotlib_formats', 'subprocess', 'sys', 'time', 'urllib']"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "%run ../load_magic/storage.py\n",
    "%run ../load_magic/environment.py\n",
    "from scipy.stats import entropy\n",
    "from sklearn.ensemble import AdaBoostClassifier, BaggingClassifier, ExtraTreesClassifier, GradientBoostingClassifier, RandomForestClassifier\n",
    "from sklearn.ensemble import StackingClassifier, VotingClassifier\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "import inspect\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Use the following only if you are on a high definition device\n",
    "from IPython.display import set_matplotlib_formats\n",
    "set_matplotlib_formats('retina')\n",
    "\n",
    "import numpy as np\n",
    "import time\n",
    "%pprint\n",
    "\n",
    "notebook_path = get_notebook_path()\n",
    "print(notebook_path)\n",
    "\n",
    "s = Storage()\n",
    "print(['s.{}'.format(fn) for fn in dir(s) if not fn.startswith('_')])\n",
    "try:\n",
    "    basic_tags_dict = s.load_object('basic_tags_dict')\n",
    "except:\n",
    "    basic_tags_dict = {}\n",
    "    s.store_objects(basic_tags_dict=basic_tags_dict)\n",
    "dir()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "len(basic_tags_dict.keys())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "---\n",
    "# Needed extra functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "scanner_regex = re.compile(r'(</?|\\b)[1-9a-zA-Z][0-9a-zA-Z]*( *[#\\+]{1,2}|>|\\b)')\n",
    "def regex_tokenizer(corpus):\n",
    "    \n",
    "    return [match.group() for match in re.finditer(scanner_regex, corpus)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "---\n",
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pickling to C:\\Users\\dev\\Documents\\Repositories\\job-hunting\\saves\\pickle\\basic_tags_df.pickle\n",
      "Retraining complete\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Rebuild the datframe from the dictionary\n",
    "rows_list = [{'html_tag_str': html_tag_str, 'is_fit': is_fit} for html_tag_str, is_fit in basic_tags_dict.items()]\n",
    "basic_tags_df = pd.DataFrame(rows_list, columns=['html_tag_str', 'is_fit'])\n",
    "s.store_objects(basic_tags_df=basic_tags_df)\n",
    "\n",
    "# Re-transform the bag-of-words and tf-idf from the new manual scores\n",
    "sents_list = basic_tags_df.html_tag_str.tolist()\n",
    "if len(sents_list):\n",
    "    \n",
    "    # Bag-of-words\n",
    "    cv = CountVectorizer(lowercase=True, tokenizer=regex_tokenizer, ngram_range=(1, 3))\n",
    "    bow_matrix = cv.fit_transform(sents_list)\n",
    "    s.store_objects(bq_cv_vocab=cv.vocabulary_)\n",
    "    \n",
    "    # Tf-idf, must get from BOW first\n",
    "    tt = TfidfTransformer()\n",
    "    tfidf_matrix = tt.fit_transform(bow_matrix)\n",
    "    s.store_objects(bq_tt=tt)\n",
    "    \n",
    "    # Re-train the classifier\n",
    "    X = tfidf_matrix.toarray()\n",
    "    y = basic_tags_df.is_fit.to_numpy()\n",
    "    fit_estimators_dict = s.load_object('fit_estimators_dict')\n",
    "    #basic_tags_clf = RandomForestClassifier(n_estimators=997)\n",
    "    #basic_tags_clf = AdaBoostClassifier(algorithm='SAMME.R', base_estimator=None, learning_rate=1.0, n_estimators=50, random_state=None)\n",
    "    basic_tags_clf = fit_estimators_dict['LogisticRegression']\n",
    "    basic_tags_clf.fit(X, y)\n",
    "    s.store_objects(basic_tags_clf=basic_tags_clf)\n",
    "    \n",
    "    # Re-calibrate the inference engine\n",
    "    bq_cv_vocab = s.load_object('bq_cv_vocab')\n",
    "    bq_cv = CountVectorizer(vocabulary=bq_cv_vocab)\n",
    "    bq_cv._validate_vocabulary()\n",
    "    bq_tt = s.load_object('bq_tt')\n",
    "    def predict_percent_fit(quals_list):\n",
    "        y_predict_proba = np.array([])\n",
    "        if len(quals_list):\n",
    "            X_test = bq_tt.transform(bq_cv.transform(quals_list)).toarray()\n",
    "            y_predict_proba = basic_tags_clf.predict_proba(X_test)\n",
    "\n",
    "        return y_predict_proba\n",
    "print('Retraining complete')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "---\n",
    "# Rescore the quals dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Get the training data and models\n",
    "if len(sents_list):\n",
    "    X = tfidf_matrix.toarray()\n",
    "    y = basic_tags_df.is_fit.to_numpy()\n",
    "    estimators_list = [AdaBoostClassifier(algorithm='SAMME.R', base_estimator=None, learning_rate=1.0, n_estimators=50, random_state=None),\n",
    "                       BaggingClassifier(base_estimator=None, bootstrap=True, bootstrap_features=False, max_features=1.0, max_samples=1.0,\n",
    "                                         n_estimators=10, n_jobs=None, oob_score=False, random_state=None, verbose=0, warm_start=False),\n",
    "                       ExtraTreesClassifier(bootstrap=False, ccp_alpha=0.0, class_weight=None, criterion='gini', max_depth=None,\n",
    "                                            max_features='auto', max_leaf_nodes=None, max_samples=None, min_impurity_decrease=0.0,\n",
    "                                            min_impurity_split=None, min_samples_leaf=1, min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
    "                                            n_estimators=100, n_jobs=None, oob_score=False, random_state=None, verbose=0, warm_start=False),\n",
    "                       GradientBoostingClassifier(ccp_alpha=0.0, criterion='friedman_mse', init=None, learning_rate=0.1, loss='deviance',\n",
    "                                                  max_depth=3,\n",
    "                                                  max_features=None, max_leaf_nodes=None, min_impurity_decrease=0.0, min_impurity_split=None,\n",
    "                                                  min_samples_leaf=1, min_samples_split=2, min_weight_fraction_leaf=0.0, n_estimators=100,\n",
    "                                                  n_iter_no_change=None, presort='deprecated', random_state=None, subsample=1.0, tol=0.0001,\n",
    "                                                  validation_fraction=0.1, verbose=0, warm_start=False),\n",
    "                       RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight=None, criterion='gini', max_depth=None,\n",
    "                                              max_features='auto',\n",
    "                                              max_leaf_nodes=None, max_samples=None, min_impurity_decrease=0.0, min_impurity_split=None,\n",
    "                                              min_samples_leaf=1, min_samples_split=2, min_weight_fraction_leaf=0.0, n_estimators=100,\n",
    "                                              n_jobs=None, oob_score=False, random_state=None, verbose=0, warm_start=False),\n",
    "                       LogisticRegression(**{'C': 7.5, 'class_weight': 'balanced', 'dual': True, 'fit_intercept': False, 'max_iter': 64,\n",
    "                                             'penalty': 'l2', 'solver': 'liblinear', 'tol': 1e-09}),\n",
    "                       SVC(C=1.0, break_ties=False, cache_size=200, class_weight=None, coef0=0.0, decision_function_shape='ovr', degree=3,\n",
    "                           gamma='scale', kernel='rbf', max_iter=-1, probability=True, random_state=None, shrinking=True, tol=0.001,\n",
    "                           verbose=False)]\n",
    "\n",
    "    # Fit the data and add the duration and fitted models to lists\n",
    "    fit_estimators_list = []\n",
    "    training_durations_list = []\n",
    "    for clf in estimators_list:\n",
    "        start_time = time.time()\n",
    "        fit_estimators_list.append(clf.fit(X, y))\n",
    "        stop_time = time.time()\n",
    "        training_durations_list.append(stop_time - start_time)\n",
    "    s.store_objects(estimators_list=fit_estimators_list, training_durations_list=training_durations_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No pickle exists at C:\\Users\\dev\\Documents\\Repositories\\job-hunting\\saves\\pickle\\estimators_list.pickle - attempting to load as csv.\n",
      "No csv exists at C:\\Users\\dev\\Documents\\Repositories\\job-hunting\\saves\\csv\\estimators_list.csv - attempting to download from URL.\n",
      "Pickling to C:\\Users\\dev\\Documents\\Repositories\\job-hunting\\saves\\pickle\\estimators_list.pickle\n"
     ]
    }
   ],
   "source": [
    "\n",
    "try:\n",
    "    estimators_list = s.load_object('estimators_list')\n",
    "except:\n",
    "    estimators_list = []\n",
    "    s.store_objects(estimators_list=estimators_list)\n",
    "if len(estimators_list):\n",
    "    inference_durations_list = []\n",
    "    for clf in estimators_list:\n",
    "        clf_name = str(type(clf)).split('.')[-1].split(\"'\")[0]\n",
    "        basic_tags_df[clf_name] = np.nan\n",
    "        start_time = time.time()\n",
    "        for row_index, row_series in basic_tags_df.iterrows():\n",
    "            html_tag_str = row_series.html_tag_str\n",
    "            X_test = bq_tt.transform(bq_cv.transform([html_tag_str])).toarray()\n",
    "            y_predict_proba = clf.predict_proba(X_test)[0][1]\n",
    "            basic_tags_df.loc[row_index, clf_name] = y_predict_proba\n",
    "        stop_time = time.time()\n",
    "        inference_durations_list.append(stop_time - start_time)\n",
    "    s.store_objects(basic_tags_df=basic_tags_df, inference_durations_list=inference_durations_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "try:\n",
    "    estimators_list = s.load_object('estimators_list')\n",
    "except:\n",
    "    estimators_list = []\n",
    "    s.store_objects(estimators_list=estimators_list)\n",
    "if len(estimators_list):\n",
    "    clf = StackingClassifier(estimators=[(str(type(e)).split('.')[-1].split(\"'\")[0], e) for e in estimators_list],\n",
    "                             final_estimator=None, cv=None, stack_method='auto', n_jobs=None, passthrough=False, verbose=0)\n",
    "    clf_name = str(type(clf)).split('.')[-1].split(\"'\")[0]\n",
    "    basic_tags_df = s.load_object('basic_tags_df')\n",
    "    basic_tags_df[clf_name] = np.nan\n",
    "    fit_estimators_list = estimators_list.copy()\n",
    "    bq_cv_vocab = s.load_object('bq_cv_vocab')\n",
    "    bq_cv = CountVectorizer(vocabulary=bq_cv_vocab)\n",
    "    bq_cv._validate_vocabulary()\n",
    "    bq_tt = s.load_object('bq_tt')\n",
    "    X = bq_tt.transform(bq_cv.transform(basic_tags_df.html_tag_str.tolist())).toarray()\n",
    "    y = basic_tags_df.is_fit.to_numpy()\n",
    "    start_time = time.time()\n",
    "    fit_estimators_list.append(clf.fit(X, y))\n",
    "    stop_time = time.time()\n",
    "    training_durations_list = s.load_object('training_durations_list')\n",
    "    training_durations_list.append(stop_time - start_time)\n",
    "    s.store_objects(fit_estimators_list=fit_estimators_list, training_durations_list=training_durations_list)\n",
    "\n",
    "    # Re-score the quals dataframe\n",
    "    inference_durations_list = s.load_object('inference_durations_list')\n",
    "    start_time = time.time()\n",
    "    for row_index, row_series in basic_tags_df.iterrows():\n",
    "        html_tag_str = row_series.html_tag_str\n",
    "        X_test = bq_tt.transform(bq_cv.transform([html_tag_str])).toarray()\n",
    "        y_predict_proba = clf.predict_proba(X_test)[0][1]\n",
    "        basic_tags_df.loc[row_index, clf_name] = y_predict_proba\n",
    "    stop_time = time.time()\n",
    "    inference_durations_list.append(stop_time - start_time)\n",
    "    s.store_objects(basic_tags_df=basic_tags_df, inference_durations_list=inference_durations_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "if len(estimators_list):\n",
    "    clf = VotingClassifier(estimators=[(str(type(e)).split('.')[-1].split(\"'\")[0], e) for e in estimators_list],\n",
    "                           voting='soft', weights=None, n_jobs=None, flatten_transform=True)\n",
    "    clf_name = str(type(clf)).split('.')[-1].split(\"'\")[0]\n",
    "    basic_tags_df[clf_name] = np.nan\n",
    "    fit_estimators_list = s.load_object('fit_estimators_list')\n",
    "    start_time = time.time()\n",
    "    fit_estimators_list.append(clf.fit(X, y))\n",
    "    stop_time = time.time()\n",
    "    training_durations_list = s.load_object('training_durations_list')\n",
    "    training_durations_list.append(stop_time - start_time)\n",
    "    s.store_objects(fit_estimators_list=fit_estimators_list, training_durations_list=training_durations_list)\n",
    "\n",
    "    # Re-score the quals dataframe\n",
    "    bq_cv_vocab = s.load_object('bq_cv_vocab')\n",
    "    bq_cv = CountVectorizer(vocabulary=bq_cv_vocab)\n",
    "    bq_cv._validate_vocabulary()\n",
    "    bq_tt = s.load_object('bq_tt')\n",
    "    inference_durations_list = s.load_object('inference_durations_list')\n",
    "    start_time = time.time()\n",
    "    for row_index, row_series in basic_tags_df.iterrows():\n",
    "        html_tag_str = row_series.html_tag_str\n",
    "        X_test = bq_tt.transform(bq_cv.transform([html_tag_str])).toarray()\n",
    "        y_predict_proba = clf.predict_proba(X_test)[0][1]\n",
    "        basic_tags_df.loc[row_index, clf_name] = y_predict_proba\n",
    "    stop_time = time.time()\n",
    "    inference_durations_list.append(stop_time - start_time)\n",
    "    s.store_objects(basic_tags_df=basic_tags_df, inference_durations_list=inference_durations_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['html_tag_str', 'is_fit']\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>html_tag_str</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>is_fit</th>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: []\n",
       "Index: [html_tag_str, is_fit]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "try:\n",
    "    basic_tags_df = s.load_object('basic_tags_df')\n",
    "except:\n",
    "    columns_list = ['html_tag_str', 'is_fit', 'AdaBoostClassifier', 'BaggingClassifier', 'ExtraTreesClassifier', 'GradientBoostingClassifier',\n",
    "                    'RandomForestClassifier', 'LogisticRegression', 'SVC', 'StackingClassifier', 'VotingClassifier']\n",
    "    basic_tags_df = pd.DataFrame([], columns=columns_list)\n",
    "    s.store_objects(basic_tags_df=basic_tags_df)\n",
    "print(basic_tags_df.columns.tolist())\n",
    "basic_tags_df.head(5).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No pickle exists at C:\\Users\\dev\\Documents\\Repositories\\job-hunting\\saves\\pickle\\fit_estimators_list.pickle - attempting to load as csv.\n",
      "No csv exists at C:\\Users\\dev\\Documents\\Repositories\\job-hunting\\saves\\csv\\fit_estimators_list.csv - attempting to download from URL.\n",
      "Pickling to C:\\Users\\dev\\Documents\\Repositories\\job-hunting\\saves\\pickle\\fit_estimators_list.pickle\n"
     ]
    }
   ],
   "source": [
    "\n",
    "metrics_list = ['accuracy_score', 'adjusted_mutual_info_score', 'adjusted_rand_score', 'average_precision_score',\n",
    "                'balanced_accuracy_score', 'cohen_kappa_score', 'completeness_score', 'explained_variance_score',\n",
    "                'f1_score', 'fowlkes_mallows_score', 'homogeneity_score', 'jaccard_score', 'mutual_info_score',\n",
    "                'normalized_mutual_info_score', 'precision_score', 'r2_score', 'recall_score', 'roc_auc_score', 'v_measure_score']\n",
    "exec('from sklearn.metrics import {}'.format(', '.join(metrics_list)))\n",
    "try:\n",
    "    fit_estimators_list = s.load_object('fit_estimators_list')\n",
    "except:\n",
    "    fit_estimators_list = []\n",
    "    s.store_objects(fit_estimators_list=fit_estimators_list)\n",
    "if len(fit_estimators_list):\n",
    "    clf_name_list = [str(type(clf)).split('.')[-1].split(\"'\")[0] for clf in fit_estimators_list]\n",
    "    basic_tags_df = s.load_object('basic_tags_df')\n",
    "    y_true = basic_tags_df.is_fit.tolist()\n",
    "    fit_match_series = (basic_tags_df.is_fit == 1)\n",
    "    yes_list = basic_tags_df[fit_match_series].is_fit.tolist()\n",
    "    no_list = basic_tags_df[~fit_match_series].is_fit.tolist()\n",
    "    columns_list = ['clf_name', 'training_duration', 'inference_duration', 'boundary_diff', 'clf_yes_entropy',\n",
    "                    'relative_yes_entropy'] + metrics_list\n",
    "    rows_list = []\n",
    "    training_durations_list = s.load_object('training_durations_list')\n",
    "    inference_durations_list = s.load_object('inference_durations_list')\n",
    "    for column_name, training_duration, inference_duration in zip(clf_name_list, training_durations_list, inference_durations_list):\n",
    "        yes_series = basic_tags_df[fit_match_series][column_name]\n",
    "        upper_bound = yes_series.min()\n",
    "        no_series = basic_tags_df[~fit_match_series][column_name]\n",
    "        lower_bound = no_series.max()\n",
    "        y_pred = []\n",
    "        for p in basic_tags_df[column_name]:\n",
    "            if p > 0.5:\n",
    "                y_pred.append(1)\n",
    "            else:\n",
    "                y_pred.append(0)\n",
    "        row_dict = {}\n",
    "        row_dict['clf_name'] = column_name\n",
    "        row_dict['training_duration'] = training_duration\n",
    "        row_dict['inference_duration'] = inference_duration\n",
    "        row_dict['boundary_diff'] = upper_bound-lower_bound\n",
    "        row_dict['clf_yes_entropy'] = entropy(pk=yes_series.tolist())\n",
    "        row_dict['relative_yes_entropy'] = entropy(pk=yes_list, qk=yes_series.tolist())\n",
    "        for metric_str in metrics_list:\n",
    "            try:\n",
    "                row_dict[metric_str] = eval('{}(y_true, basic_tags_df[column_name].tolist())'.format(metric_str))\n",
    "            except Exception as e1:\n",
    "                try:\n",
    "                    row_dict[metric_str] = eval('{}(y_true, y_pred)'.format(metric_str))\n",
    "                except Exception as e2:\n",
    "                    row_dict[metric_str] = np.nan\n",
    "        rows_list.append(row_dict)\n",
    "    entropy_df = pd.DataFrame(rows_list, columns=columns_list).dropna(axis='columns', how='all')\n",
    "    entropy_df.set_index('clf_name', drop=True, inplace=True)\n",
    "    s.store_objects(entropy_df=entropy_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No pickle exists at C:\\Users\\dev\\Documents\\Repositories\\job-hunting\\saves\\pickle\\entropy_df.pickle - attempting to load as csv.\n",
      "No csv exists at C:\\Users\\dev\\Documents\\Repositories\\job-hunting\\saves\\csv\\entropy_df.csv - attempting to download from URL.\n",
      "Pickling to C:\\Users\\dev\\Documents\\Repositories\\job-hunting\\saves\\pickle\\entropy_df.pickle\n",
      "Pickling to C:\\Users\\dev\\Documents\\Repositories\\job-hunting\\saves\\pickle\\metrics_list.pickle\n",
      "Pickling to C:\\Users\\dev\\Documents\\Repositories\\job-hunting\\saves\\pickle\\description_dict.pickle\n"
     ]
    }
   ],
   "source": [
    "\n",
    "if len(metrics_list):\n",
    "    description_dict = {name: fn.__doc__.strip().split('\\n')[0] for name, fn in inspect.getmembers(sys.modules[__name__],\n",
    "                                                                                                   inspect.isfunction) if name in metrics_list}\n",
    "    try:\n",
    "        entropy_df = s.load_object('entropy_df')\n",
    "    except:\n",
    "        columns_list = ['training_duration', 'inference_duration', 'balanced_accuracy_score', 'accuracy_score', 'adjusted_mutual_info_score',\n",
    "                        'adjusted_rand_score', 'average_precision_score', 'balanced_accuracy_score', 'cohen_kappa_score', 'completeness_score',\n",
    "                        'explained_variance_score', 'f1_score', 'fowlkes_mallows_score', 'homogeneity_score', 'jaccard_score',\n",
    "                        'mutual_info_score', 'normalized_mutual_info_score', 'precision_score', 'r2_score', 'recall_score', 'roc_auc_score',\n",
    "                        'v_measure_score', 'boundary_diff', 'clf_yes_entropy', 'relative_yes_entropy']\n",
    "        entropy_df = pd.DataFrame([], columns=columns_list)\n",
    "        s.store_objects(entropy_df=entropy_df)\n",
    "    for name, cls in inspect.getmembers(sys.modules[__name__], inspect.isclass):\n",
    "        if name in entropy_df.index:\n",
    "            description_dict[name] = cls.__doc__.strip().split('\\n')[0]\n",
    "    s.store_objects(metrics_list=metrics_list, description_dict=description_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['training_duration', 'inference_duration', 'balanced_accuracy_score', 'accuracy_score', 'adjusted_mutual_info_score', 'adjusted_rand_score', 'average_precision_score', 'balanced_accuracy_score', 'cohen_kappa_score', 'completeness_score', 'explained_variance_score', 'f1_score', 'fowlkes_mallows_score', 'homogeneity_score', 'jaccard_score', 'mutual_info_score', 'normalized_mutual_info_score', 'precision_score', 'r2_score', 'recall_score', 'roc_auc_score', 'v_measure_score', 'boundary_diff', 'clf_yes_entropy', 'relative_yes_entropy']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['accuracy_score', 'adjusted_mutual_info_score', 'adjusted_rand_score', 'average_precision_score', 'balanced_accuracy_score', 'cohen_kappa_score', 'completeness_score', 'explained_variance_score', 'f1_score', 'fowlkes_mallows_score', 'homogeneity_score', 'jaccard_score', 'mutual_info_score', 'normalized_mutual_info_score', 'precision_score', 'r2_score', 'recall_score', 'roc_auc_score', 'v_measure_score']"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "print(entropy_df.columns.tolist())\n",
    "metrics_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>training_duration</th>\n",
       "      <th>inference_duration</th>\n",
       "      <th>balanced_accuracy_score</th>\n",
       "      <th>r2_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [training_duration, inference_duration, balanced_accuracy_score, r2_score]\n",
       "Index: []"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "columns_list = ['training_duration', 'inference_duration', 'balanced_accuracy_score', 'r2_score']\n",
    "entropy_df[columns_list].sort_values('balanced_accuracy_score', ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pickling to C:\\Users\\dev\\Documents\\Repositories\\job-hunting\\saves\\pickle\\fit_estimators_dict.pickle\n"
     ]
    }
   ],
   "source": [
    "\n",
    "fit_estimators_list = s.load_object('fit_estimators_list')\n",
    "fit_estimators_dict = {str(type(clf)).split('.')[-1].split(\"'\")[0]: clf for clf in fit_estimators_list}\n",
    "s.store_objects(fit_estimators_dict=fit_estimators_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "entropy_df = s.load_object('entropy_df')\n",
    "if entropy_df.shape[0]:\n",
    "    metrics_list = s.load_object('metrics_list')\n",
    "    custom_metrics_list = ['boundary_diff', 'clf_yes_entropy', 'relative_yes_entropy']\n",
    "    columns_list = metrics_list + custom_metrics_list\n",
    "    columns_list = [cn for cn, s in sorted([(cn, entropy_df[cn].std()) for cn in columns_list], key=lambda x: x[1], reverse=True)][:3]\n",
    "    description_dict = s.load_object('description_dict')\n",
    "    for metric in columns_list:\n",
    "        print('{}: {}'.format(metric, description_dict[metric]))\n",
    "    AxesSubplot_obj = entropy_df[columns_list].sort_values('r2_score', ascending=True).plot.line(rot=45, figsize=(18, 8))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "if entropy_df.shape[0]:\n",
    "    description_dict = s.load_object('description_dict')\n",
    "    columns_list = ['training_duration', 'inference_duration', 'balanced_accuracy_score', 'r2_score']\n",
    "    for metric in columns_list:\n",
    "        if metric in description_dict:\n",
    "            print('{}: {}'.format(metric, description_dict[metric]))\n",
    "    entropy_df = s.load_object('entropy_df')\n",
    "    fig = plt.figure(figsize=(18, 8))\n",
    "    ax = fig.add_subplot(1, 1, 1)\n",
    "    ax.set_yscale('log')\n",
    "    AxesSubplot_obj = entropy_df[columns_list].sort_values('training_duration', ascending=True).plot.line(rot=45, ax=ax)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "if entropy_df.shape[0]:\n",
    "    metrics_list = s.load_object('metrics_list')\n",
    "    columns_list = [cn for cn in metrics_list if 'accur' in cn.lower()]\n",
    "    for metric in columns_list:\n",
    "        if metric in description_dict:\n",
    "            print('{}: {}'.format(metric, description_dict[metric]))\n",
    "    AxesSubplot_obj = entropy_df[columns_list].sort_values('accuracy_score', ascending=True).plot.line(rot=45, figsize=(18, 8))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>boundary_diff</th>\n",
       "      <th>clf_yes_entropy</th>\n",
       "      <th>relative_yes_entropy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [boundary_diff, clf_yes_entropy, relative_yes_entropy]\n",
       "Index: []"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "entropy_df[custom_metrics_list].sort_values('boundary_diff', ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "if entropy_df.shape[0]:\n",
    "    for metric in custom_metrics_list:\n",
    "        if metric in description_dict:\n",
    "            print('{}: {}'.format(metric, description_dict[metric]))\n",
    "    AxesSubplot_obj = entropy_df[custom_metrics_list].sort_values('boundary_diff', ascending=True).plot.line(rot=45, figsize=(18, 8))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "entropy_df = s.load_object('entropy_df')\n",
    "if entropy_df.shape[0]:\n",
    "    columns_list = ['average_precision_score', 'precision_score', 'recall_score']\n",
    "    description_dict = s.load_object('description_dict')\n",
    "    for metric in columns_list:\n",
    "        print('{}: {}'.format(metric, description_dict[metric]))\n",
    "    AxesSubplot_obj = entropy_df[columns_list].sort_values('precision_score', ascending=True).plot.line(rot=45, figsize=(18, 8))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>training_duration</th>\n",
       "      <th>inference_duration</th>\n",
       "      <th>average_precision_score</th>\n",
       "      <th>precision_score</th>\n",
       "      <th>recall_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [training_duration, inference_duration, average_precision_score, precision_score, recall_score]\n",
       "Index: []"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "columns_list = ['average_precision_score', 'precision_score', 'recall_score']\n",
    "extended_columns_list = ['training_duration', 'inference_duration'] + columns_list\n",
    "entropy_df[extended_columns_list].sort_values('precision_score', ascending=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Job Hunting (Python 3.9.0)",
   "language": "python",
   "name": "jh"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
