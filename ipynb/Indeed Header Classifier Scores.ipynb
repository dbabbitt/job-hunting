{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "---\n",
    "# Load needed libraries and functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "%pprint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "%run ../load_magic/storage.py\n",
    "%run ../load_magic/environment.py\n",
    "from matplotlib_inline.backend_inline import set_matplotlib_formats\n",
    "from scipy.stats import entropy\n",
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "from sklearn.ensemble import AdaBoostClassifier, BaggingClassifier, ExtraTreesClassifier\n",
    "from sklearn.ensemble import GradientBoostingClassifier, RandomForestClassifier\n",
    "from sklearn.ensemble import StackingClassifier, VotingClassifier\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "import gensim\n",
    "import inspect\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import time\n",
    "\n",
    "set_matplotlib_formats('retina')\n",
    "notebook_path = get_notebook_path()\n",
    "s = Storage()\n",
    "assert s.pickle_exists('NAVIGABLE_PARENT_IS_HEADER_DICT')\n",
    "NAVIGABLE_PARENT_IS_HEADER_DICT = s.load_object('NAVIGABLE_PARENT_IS_HEADER_DICT')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "print(notebook_path)\n",
    "print(['s.{}'.format(fn) for fn in dir(s) if not fn.startswith('_')])\n",
    "print([fn for fn in dir() if not fn.startswith('_')])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "len(NAVIGABLE_PARENT_IS_HEADER_DICT.keys())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "---\n",
    "# Needed extra functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "%run ../py/html_analysis.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "---\n",
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Rebuild the datframe from the dictionary\n",
    "rows_list = [{'navigable_parent': navigable_parent,\n",
    "              'is_header': is_header} for navigable_parent, is_header in NAVIGABLE_PARENT_IS_HEADER_DICT.items()]\n",
    "basic_tags_df = pd.DataFrame(rows_list, columns=['navigable_parent', 'is_header'])\n",
    "s.store_objects(basic_tags_df=basic_tags_df)\n",
    "\n",
    "# Re-transform the bag-of-words and tf-idf from the new manual scores\n",
    "sents_list = basic_tags_df.navigable_parent.tolist()\n",
    "assert len(sents_list)\n",
    "\n",
    "# Bag-of-words\n",
    "cv = CountVectorizer(**{'analyzer': 'word', 'binary': False, 'decode_error': 'strict', 'lowercase': False, 'max_df': 1.0,\n",
    "                        'max_features': None, 'min_df': 0.0, 'ngram_range': (1, 5), 'stop_words': None, 'strip_accents': 'ascii',\n",
    "                        'tokenizer': regex_tokenizer})\n",
    "bow_matrix = cv.fit_transform(sents_list)\n",
    "s.store_objects(bq_cv_vocab=cv.vocabulary_)\n",
    "\n",
    "# Tf-idf, must get from BOW first\n",
    "tt = TfidfTransformer(**{'norm': 'l1', 'smooth_idf': True, 'sublinear_tf': False, 'use_idf': True})\n",
    "tfidf_matrix = tt.fit_transform(bow_matrix)\n",
    "s.store_objects(bq_tt=tt)\n",
    "\n",
    "# Re-train the classifier\n",
    "X = tfidf_matrix.toarray()\n",
    "y = basic_tags_df.is_header.to_numpy()\n",
    "# Best score: 0.850\n",
    "basic_tags_clf = LogisticRegression(C=10.0, class_weight='balanced', dual=False, fit_intercept=True, intercept_scaling=1, l1_ratio=None,\n",
    "                                    max_iter=1000, multi_class='auto', n_jobs=None, penalty='l1', random_state=None, solver='liblinear',\n",
    "                                    tol=0.0001, verbose=0, warm_start=False)\n",
    "basic_tags_clf.fit(X, y)\n",
    "s.store_objects(basic_tags_clf=basic_tags_clf)\n",
    "\n",
    "# Re-calibrate the inference engine\n",
    "bq_cv_vocab = s.load_object('bq_cv_vocab')\n",
    "bq_cv = CountVectorizer(vocabulary=bq_cv_vocab)\n",
    "bq_cv._validate_vocabulary()\n",
    "bq_tt = s.load_object('bq_tt')\n",
    "CLF_NAME = 'LogisticRegression'\n",
    "print('Retraining complete')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "---\n",
    "# Rescore the tags dataframe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Add the Standard Models to the Fit Estimators and Training Durations List"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "counts_dict = basic_tags_df.groupby('is_header').count()['navigable_parent'].to_dict()\n",
    "np.array([counts_dict[False]/(counts_dict[False]+counts_dict[True]), counts_dict[True]/(counts_dict[False]+counts_dict[True])])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Get the training data and models\n",
    "assert len(sents_list)\n",
    "X = tfidf_matrix.toarray()\n",
    "y = basic_tags_df.is_header.to_numpy()\n",
    "estimators_list = [\n",
    "                   # done in 3.035s\n",
    "                   # Best score: 0.734\n",
    "                   AdaBoostClassifier(algorithm='SAMME.R', base_estimator=None, learning_rate=1.0, n_estimators=5, random_state=None),\n",
    "\n",
    "                   # done in 332.301s\n",
    "                   # Best score: 0.814\n",
    "                   BaggingClassifier(base_estimator=None, bootstrap=True, bootstrap_features=False, max_features=0.5, max_samples=0.75,\n",
    "                                     n_estimators=10, n_jobs=None, oob_score=False, random_state=None, verbose=0, warm_start=False),\n",
    "\n",
    "                   # done in 221.215s\n",
    "                   # Best score: 0.683\n",
    "                   ExtraTreesClassifier(bootstrap=False, ccp_alpha=0.0, class_weight=None, criterion='gini', max_depth=100,\n",
    "                                        max_features='sqrt', max_leaf_nodes=None, max_samples=None, min_impurity_decrease=0.0,\n",
    "                                        min_samples_leaf=1, min_samples_split=2, min_weight_fraction_leaf=0.0, n_estimators=100,\n",
    "                                        n_jobs=None, oob_score=False, random_state=None, verbose=0, warm_start=False),\n",
    "\n",
    "                   # done in 135.438s\n",
    "                   # Best score: 0.698\n",
    "                   GradientBoostingClassifier(ccp_alpha=0.0, criterion='friedman_mse', init='zero', learning_rate=1.0, loss='deviance',\n",
    "                                              max_depth=3, max_features=None, max_leaf_nodes=None, min_impurity_decrease=0.0,\n",
    "                                              min_samples_leaf=1, min_samples_split=2, min_weight_fraction_leaf=0.0, n_estimators=1000,\n",
    "                                              n_iter_no_change=None, random_state=None, subsample=1.0, tol=0.0001, validation_fraction=0.1,\n",
    "                                              verbose=0, warm_start=False),\n",
    "\n",
    "                   # done in 289.938s\n",
    "                   # Best score: 0.779\n",
    "                   RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight=None, criterion='entropy', max_depth=None,\n",
    "                                          max_features=None, max_leaf_nodes=None, max_samples=None, min_impurity_decrease=0.0,\n",
    "                                          min_impurity_split=None, min_samples_leaf=2, min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
    "                                          n_estimators=1000, n_jobs=None, oob_score=False, random_state=None, verbose=0, warm_start=False),\n",
    "\n",
    "                   # done in 13.410s\n",
    "                   # Best score: 0.850\n",
    "                   LogisticRegression(C=10.0, class_weight='balanced', dual=False, fit_intercept=True, intercept_scaling=1, l1_ratio=None,\n",
    "                                      max_iter=1000, multi_class='auto', n_jobs=None, penalty='l1', random_state=None, solver='liblinear',\n",
    "                                      tol=0.0001, verbose=0, warm_start=False),\n",
    "\n",
    "                   # done in 6.014s\n",
    "                   # Best score: 0.764\n",
    "                   SVC(C=1.0, break_ties=False, cache_size=200, class_weight=None, coef0=0.0, decision_function_shape='ovr', degree=3,\n",
    "                       gamma='scale', kernel='rbf', max_iter=-1, probability=True, random_state=None, shrinking=True, tol=0.001,\n",
    "                       verbose=False),\n",
    "                   ]\n",
    "\n",
    "# Fit the data and add the duration and fitted models to lists\n",
    "fit_estimators_list = []\n",
    "training_durations_list = []\n",
    "for clf in estimators_list:\n",
    "    start_time = time.time()\n",
    "    fit_estimators_list.append(clf.fit(X, y))\n",
    "    stop_time = time.time()\n",
    "    training_durations_list.append(stop_time - start_time)\n",
    "s.store_objects(estimators_list=fit_estimators_list, training_durations_list=training_durations_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Add the LDA Model to the Fit Estimators and Training Durations List"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "\n",
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "\n",
    "# Train the model on the corpus\n",
    "lda = LatentDirichletAllocation(n_components=2, doc_topic_prior=None, topic_word_prior=None, learning_method='batch',\n",
    "                                learning_decay=0.7, learning_offset=10.0, max_iter=10, batch_size=128, evaluate_every=-1,\n",
    "                                total_samples=1000000.0, perp_tol=0.1, mean_change_tol=0.001, max_doc_update_iter=100, n_jobs=None,\n",
    "                                verbose=0, random_state=None)\n",
    "lda.fit(X)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "\n",
    "from gensim.corpora.dictionary import Dictionary\n",
    "from gensim.models.ldamodel import LdaModel\n",
    "\n",
    "assert len(sents_list)\n",
    "fit_estimators_list = s.load_object('estimators_list')\n",
    "start_time = time.time()\n",
    "\n",
    "# Build model with tokenized words\n",
    "tokenized_sents_list = [regex_tokenizer(sent_str) for sent_str in sents_list]\n",
    "\n",
    "# Create a corpus from a list of texts\n",
    "headers_dictionary = Dictionary(tokenized_sents_list)\n",
    "headers_corpus = [headers_dictionary.doc2bow(tag_str) for tag_str in tokenized_sents_list]\n",
    "\n",
    "# Train the model on the corpus\n",
    "lda = LdaModel(corpus=headers_corpus, num_topics=2)\n",
    "fit_estimators_list.append(lda)\n",
    "\n",
    "stop_time = time.time()\n",
    "training_durations_list.append(stop_time - start_time)\n",
    "s.store_objects(estimators_list=fit_estimators_list, training_durations_list=training_durations_list)\n",
    "\n",
    "# Save model as binary file\n",
    "#child_str_model = gensim.models.FastText(tokenized_sents_list)\n",
    "#model_folder = os.path.join(s.data_folder, 'bin')\n",
    "#os.makedirs(name=model_folder, exist_ok=True)\n",
    "#file_path = os.path.join(model_folder, 'child_str_model.bin')\n",
    "#child_str_model.save(file_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Create the Inference Durations List"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "inference_durations_list = []\n",
    "for clf in fit_estimators_list:\n",
    "    clf_name = str(type(clf)).split('.')[-1].split(\"'\")[0]\n",
    "    basic_tags_df[clf_name] = np.nan\n",
    "    start_time = time.time()\n",
    "    for row_index, row_series in basic_tags_df.iterrows():\n",
    "        navigable_parent = row_series.navigable_parent\n",
    "        if(clf_name == 'LdaModel'):\n",
    "            X_test = headers_dictionary.doc2bow(regex_tokenizer(navigable_parent))\n",
    "            result_list = lda[X_test]\n",
    "            if len(result_list) == 1:\n",
    "                result_tuple = result_list[0]\n",
    "            elif len(result_list) == 2:\n",
    "                result_tuple = result_list[1]\n",
    "                \n",
    "            # Assume it's the probability of the larger topic\n",
    "            y_predict_proba = 1.0 - result_tuple[1]\n",
    "            \n",
    "        else:\n",
    "            X_test = bq_tt.transform(bq_cv.transform([navigable_parent])).toarray()\n",
    "            y_predict_proba = clf.predict_proba(X_test)[0][1]\n",
    "        basic_tags_df.loc[row_index, clf_name] = y_predict_proba\n",
    "    stop_time = time.time()\n",
    "    inference_durations_list.append(stop_time - start_time)\n",
    "s.store_objects(basic_tags_df=basic_tags_df, inference_durations_list=inference_durations_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "clf_name"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Add the Stacking Classifier to the Estimators, Training and Durations List"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "clf = StackingClassifier(estimators=[(str(type(e)).split('.')[-1].split(\"'\")[0], e) for e in estimators_list],\n",
    "                         final_estimator=None, cv=None, stack_method='auto', n_jobs=None, passthrough=False, verbose=0)\n",
    "clf_name = str(type(clf)).split('.')[-1].split(\"'\")[0]\n",
    "basic_tags_df = s.load_object('basic_tags_df')\n",
    "basic_tags_df[clf_name] = np.nan\n",
    "bq_cv_vocab = s.load_object('bq_cv_vocab')\n",
    "bq_cv = CountVectorizer(vocabulary=bq_cv_vocab)\n",
    "bq_cv._validate_vocabulary()\n",
    "bq_tt = s.load_object('bq_tt')\n",
    "X = bq_tt.transform(bq_cv.transform(basic_tags_df.navigable_parent.tolist())).toarray()\n",
    "y = basic_tags_df.is_header.to_numpy()\n",
    "start_time = time.time()\n",
    "fit_estimators_list.append(clf.fit(X, y))\n",
    "stop_time = time.time()\n",
    "training_durations_list = s.load_object('training_durations_list')\n",
    "training_durations_list.append(stop_time - start_time)\n",
    "s.store_objects(fit_estimators_list=fit_estimators_list, training_durations_list=training_durations_list)\n",
    "\n",
    "# Re-score the tags dataframe\n",
    "inference_durations_list = s.load_object('inference_durations_list')\n",
    "start_time = time.time()\n",
    "for row_index, row_series in basic_tags_df.iterrows():\n",
    "    navigable_parent = row_series.navigable_parent\n",
    "    X_test = bq_tt.transform(bq_cv.transform([navigable_parent])).toarray()\n",
    "    y_predict_proba = clf.predict_proba(X_test)[0][1]\n",
    "    basic_tags_df.loc[row_index, clf_name] = y_predict_proba\n",
    "stop_time = time.time()\n",
    "inference_durations_list.append(stop_time - start_time)\n",
    "s.store_objects(basic_tags_df=basic_tags_df, inference_durations_list=inference_durations_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Add the Voting Classifier to the Estimators, Training and Durations List"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "clf = VotingClassifier(estimators=[(str(type(e)).split('.')[-1].split(\"'\")[0], e) for e in estimators_list],\n",
    "                       voting='soft', weights=None, n_jobs=None, flatten_transform=True)\n",
    "clf_name = str(type(clf)).split('.')[-1].split(\"'\")[0]\n",
    "basic_tags_df[clf_name] = np.nan\n",
    "fit_estimators_list = s.load_object('fit_estimators_list')\n",
    "start_time = time.time()\n",
    "fit_estimators_list.append(clf.fit(X, y))\n",
    "stop_time = time.time()\n",
    "training_durations_list = s.load_object('training_durations_list')\n",
    "training_durations_list.append(stop_time - start_time)\n",
    "s.store_objects(fit_estimators_list=fit_estimators_list, training_durations_list=training_durations_list)\n",
    "\n",
    "# Re-score the html tags dataframe for prediction comparisons\n",
    "bq_cv_vocab = s.load_object('bq_cv_vocab')\n",
    "bq_cv = CountVectorizer(vocabulary=bq_cv_vocab)\n",
    "bq_cv._validate_vocabulary()\n",
    "bq_tt = s.load_object('bq_tt')\n",
    "inference_durations_list = s.load_object('inference_durations_list')\n",
    "start_time = time.time()\n",
    "for row_index, row_series in basic_tags_df.iterrows():\n",
    "    navigable_parent = row_series.navigable_parent\n",
    "    X_test = bq_tt.transform(bq_cv.transform([navigable_parent])).toarray()\n",
    "    y_predict_proba = clf.predict_proba(X_test)[0][1]\n",
    "    basic_tags_df.loc[row_index, clf_name] = y_predict_proba\n",
    "stop_time = time.time()\n",
    "inference_durations_list.append(stop_time - start_time)\n",
    "s.store_objects(basic_tags_df=basic_tags_df, inference_durations_list=inference_durations_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Create and store a dictionary of all the fitted classifiers\n",
    "fit_estimators_list = s.load_object('fit_estimators_list')\n",
    "fit_estimators_dict = {str(type(clf)).split('.')[-1].split(\"'\")[0]: clf for clf in fit_estimators_list}\n",
    "s.store_objects(FIT_ESTIMATORS_DICT=fit_estimators_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "print(basic_tags_df.columns.tolist())\n",
    "basic_tags_df.sample(5).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "metrics_list = ['accuracy_score', 'adjusted_mutual_info_score', 'adjusted_rand_score', 'average_precision_score',\n",
    "                'balanced_accuracy_score', 'cohen_kappa_score', 'completeness_score', 'explained_variance_score',\n",
    "                'f1_score', 'fowlkes_mallows_score', 'homogeneity_score', 'jaccard_score', 'mutual_info_score',\n",
    "                'normalized_mutual_info_score', 'precision_score', 'r2_score', 'recall_score', 'roc_auc_score', 'v_measure_score']\n",
    "exec('from sklearn.metrics import {}'.format(', '.join(metrics_list)))\n",
    "fit_estimators_list = s.load_object('fit_estimators_list')\n",
    "clf_name_list = [str(type(clf)).split('.')[-1].split(\"'\")[0] for clf in fit_estimators_list]\n",
    "basic_tags_df = s.load_object('basic_tags_df')\n",
    "y_true = basic_tags_df.is_header.tolist()\n",
    "fit_match_series = (basic_tags_df.is_header == True)\n",
    "yes_list = basic_tags_df[fit_match_series].is_header.tolist()\n",
    "no_list = basic_tags_df[~fit_match_series].is_header.tolist()\n",
    "columns_list = ['clf_name', 'training_duration', 'inference_duration', 'boundary_diff', 'clf_yes_entropy',\n",
    "                'relative_yes_entropy'] + metrics_list\n",
    "rows_list = []\n",
    "training_durations_list = s.load_object('training_durations_list')\n",
    "inference_durations_list = s.load_object('inference_durations_list')\n",
    "for column_name, training_duration, inference_duration in zip(clf_name_list, training_durations_list, inference_durations_list):\n",
    "    yes_series = basic_tags_df[fit_match_series][column_name]\n",
    "    upper_bound = yes_series.min()\n",
    "    no_series = basic_tags_df[~fit_match_series][column_name]\n",
    "    lower_bound = no_series.max()\n",
    "    y_pred = []\n",
    "    for p in basic_tags_df[column_name]:\n",
    "        if p > 0.5:\n",
    "            y_pred.append(1)\n",
    "        else:\n",
    "            y_pred.append(0)\n",
    "    row_dict = {}\n",
    "    row_dict['clf_name'] = column_name\n",
    "    row_dict['training_duration'] = training_duration\n",
    "    row_dict['inference_duration'] = inference_duration\n",
    "    row_dict['boundary_diff'] = upper_bound-lower_bound\n",
    "    row_dict['clf_yes_entropy'] = entropy(pk=yes_series.tolist(), base=2)\n",
    "    row_dict['relative_yes_entropy'] = entropy(pk=yes_list, qk=yes_series.tolist(), base=2)\n",
    "    for metric_str in metrics_list:\n",
    "        try:\n",
    "            row_dict[metric_str] = eval('{}(y_true, basic_tags_df[column_name].tolist())'.format(metric_str))\n",
    "        except Exception as e1:\n",
    "            try:\n",
    "                row_dict[metric_str] = eval('{}(y_true, y_pred)'.format(metric_str))\n",
    "            except Exception as e2:\n",
    "                row_dict[metric_str] = np.nan\n",
    "    rows_list.append(row_dict)\n",
    "entropy_df = pd.DataFrame(rows_list, columns=columns_list).dropna(axis='columns', how='all')\n",
    "entropy_df.set_index('clf_name', drop=True, inplace=True)\n",
    "s.store_objects(entropy_df=entropy_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "description_dict = {name: fn.__doc__.strip().split('\\n')[0] for name, fn in inspect.getmembers(sys.modules[__name__],\n",
    "                                                                                               inspect.isfunction) if name in metrics_list}\n",
    "assert s.pickle_exists('entropy_df')\n",
    "entropy_df = s.load_object('entropy_df')\n",
    "for name, cls in inspect.getmembers(sys.modules[__name__], inspect.isclass):\n",
    "    if name in entropy_df.index:\n",
    "        description_dict[name] = cls.__doc__.strip().split('\\n')[0]\n",
    "s.store_objects(metrics_list=metrics_list, description_dict=description_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "print(entropy_df.columns.tolist())\n",
    "metrics_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# 1 - second topic: LdaModel\t3.973441\t1.990886\t0.798208\t-0.819588\n",
    "# 1 - first topic:  LdaModel\t3.973441\t2.222728\t0.211814\t-4.210144\n",
    "# first topic:      LdaModel\t3.973441\t2.371093\t0.787072\t-0.987907\n",
    "# second topic:     LdaModel\t3.973441\t2.371093\t0.200678\t-4.379083\n",
    "columns_list = ['training_duration', 'inference_duration', 'balanced_accuracy_score', 'r2_score']\n",
    "entropy_df[columns_list].sort_values('balanced_accuracy_score', ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "description_dict = s.load_object('description_dict')\n",
    "if 'training_duration' not in description_dict:\n",
    "    description_dict['training_duration'] = 'The average training time in seconds'\n",
    "    s.store_objects(description_dict=description_dict)\n",
    "if 'inference_duration' not in description_dict:\n",
    "    description_dict['inference_duration'] = 'The average inference time in seconds'\n",
    "    s.store_objects(description_dict=description_dict)\n",
    "if 'clf_yes_entropy' not in description_dict:\n",
    "    description_dict['clf_yes_entropy'] = 'The entropy of the distribution for True probability values'\n",
    "    s.store_objects(description_dict=description_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "----"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Assume the entropy dataframe has been populated\n",
    "entropy_df = s.load_object('entropy_df')\n",
    "assert entropy_df.shape[0]\n",
    "description_dict = s.load_object('description_dict')\n",
    "columns_list = ['training_duration', 'inference_duration']\n",
    "for metric in columns_list:\n",
    "    if metric in description_dict:\n",
    "        print('{}: {}'.format(metric, description_dict[metric]))\n",
    "fig = plt.figure(figsize=(18, 8))\n",
    "ax = fig.add_subplot(1, 1, 1)\n",
    "AxesSubplot_obj = entropy_df.sort_values(columns_list[0], ascending=True)[columns_list].plot.bar(rot=45, ax=ax)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Assume the entropy dataframe has been populated\n",
    "entropy_df = s.load_object('entropy_df')\n",
    "assert entropy_df.shape[0]\n",
    "description_dict = s.load_object('description_dict')\n",
    "columns_list = ['balanced_accuracy_score', 'r2_score']\n",
    "for metric in columns_list:\n",
    "    if metric in description_dict:\n",
    "        print('{}: {}'.format(metric, description_dict[metric]))\n",
    "fig = plt.figure(figsize=(18, 8))\n",
    "ax = fig.add_subplot(1, 1, 1)\n",
    "AxesSubplot_obj = entropy_df.sort_values(columns_list[0], ascending=True)[columns_list].plot.bar(rot=45, ax=ax)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Assume the entropy dataframe has been populated\n",
    "entropy_df = s.load_object('entropy_df')\n",
    "assert entropy_df.shape[0]\n",
    "description_dict = s.load_object('description_dict')\n",
    "columns_list = ['training_duration', 'inference_duration', 'balanced_accuracy_score']\n",
    "for metric in columns_list:\n",
    "    if metric in description_dict:\n",
    "        print('{}: {}'.format(metric, description_dict[metric]))\n",
    "fig = plt.figure(figsize=(18, 8))\n",
    "ax = fig.add_subplot(1, 1, 1)\n",
    "ax.set_yscale('log')\n",
    "AxesSubplot_obj = entropy_df.sort_values(columns_list[0], ascending=True)[columns_list].plot.bar(rot=45, ax=ax)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "entropy_df = s.load_object('entropy_df')\n",
    "assert entropy_df.shape[0]\n",
    "metrics_list = s.load_object('metrics_list')\n",
    "custom_metrics_list = ['boundary_diff', 'clf_yes_entropy', 'relative_yes_entropy']\n",
    "columns_list = metrics_list + custom_metrics_list\n",
    "columns_list = [cn for cn, s in sorted([(cn, entropy_df[cn].std()) for cn in columns_list], key=lambda x: x[1], reverse=True)][:3]\n",
    "description_dict = s.load_object('description_dict')\n",
    "for metric in columns_list:\n",
    "    if metric in description_dict:\n",
    "        print('{}: {}'.format(metric, description_dict[metric]))\n",
    "df = entropy_df.sort_values(columns_list[0], ascending=True)[columns_list]\n",
    "AxesSubplot_obj = df.plot.bar(rot=45, figsize=(18, 8))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from IPython.display import display\n",
    "\n",
    "row_dict = {}\n",
    "for column_name in df.columns:\n",
    "    row_dict[column_name] = df[column_name].std()\n",
    "display(df.append(pd.DataFrame([row_dict], index=['Standard Deviation'])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "assert entropy_df.shape[0]\n",
    "metrics_list = s.load_object('metrics_list')\n",
    "columns_list = [cn for cn in metrics_list if 'accur' in cn.lower()]\n",
    "for metric in columns_list:\n",
    "    if metric in description_dict:\n",
    "        print('{}: {}'.format(metric, description_dict[metric]))\n",
    "AxesSubplot_obj = entropy_df.sort_values(columns_list[0], ascending=True)[columns_list].plot.bar(rot=45, figsize=(18, 8))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "entropy_df.sort_values('boundary_diff', ascending=False)[custom_metrics_list]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "assert entropy_df.shape[0]\n",
    "for metric in custom_metrics_list:\n",
    "    if metric in description_dict:\n",
    "        print('{}: {}'.format(metric, description_dict[metric]))\n",
    "AxesSubplot_obj = entropy_df.sort_values('boundary_diff', ascending=True)[custom_metrics_list].plot.bar(rot=45, figsize=(18, 8))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "entropy_df = s.load_object('entropy_df')\n",
    "assert entropy_df.shape[0]\n",
    "columns_list = ['average_precision_score', 'precision_score', 'recall_score']\n",
    "description_dict = s.load_object('description_dict')\n",
    "for metric in columns_list:\n",
    "    print('{}: {}'.format(metric, description_dict[metric]))\n",
    "AxesSubplot_obj = entropy_df.sort_values('precision_score', ascending=True)[columns_list].plot.bar(rot=45, figsize=(18, 8))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "columns_list = ['average_precision_score', 'precision_score', 'recall_score']\n",
    "extended_columns_list = ['training_duration', 'inference_duration'] + columns_list\n",
    "entropy_df.sort_values('precision_score', ascending=True)[extended_columns_list]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "----"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "[(str(type(e)).split('.')[-1].split(\"'\")[0], e) for e in fit_estimators_list]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "idx = 'LdaModel'\n",
    "mask_series = (entropy_df.index == idx)\n",
    "entropy_df[mask_series].T.to_dict()[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "clf_name = 'LdaModel'\n",
    "FIT_ESTIMATORS_DICT = s.load_object('FIT_ESTIMATORS_DICT')\n",
    "clf = FIT_ESTIMATORS_DICT[clf_name]\n",
    "basic_tags_df[clf_name] = np.nan\n",
    "for row_index, row_series in basic_tags_df.iterrows():\n",
    "    navigable_parent = row_series.navigable_parent\n",
    "    X_test = headers_dictionary.doc2bow(regex_tokenizer(navigable_parent))\n",
    "    result_list = lda[X_test]\n",
    "    if len(result_list) == 1:\n",
    "        result_tuple = result_list[0]\n",
    "    elif len(result_list) == 2:\n",
    "        \n",
    "        # Assume it's the first topic\n",
    "        result_tuple = result_list[0]\n",
    "        \n",
    "    y_predict_proba = result_tuple[1]\n",
    "    \n",
    "    basic_tags_df.loc[row_index, clf_name] = y_predict_proba\n",
    "s.store_objects(basic_tags_df=basic_tags_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Job Hunting (Python 3.9.0)",
   "language": "python",
   "name": "jh"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
