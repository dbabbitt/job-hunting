{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pretty printing has been turned OFF\n"
     ]
    }
   ],
   "source": [
    "\n",
    "%pprint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[1;31mInit signature:\u001b[0m\n",
       "\u001b[0mmodels\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mLdaModel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m\n",
       "\u001b[0m    \u001b[0mcorpus\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
       "\u001b[0m    \u001b[0mnum_topics\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m100\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
       "\u001b[0m    \u001b[0mid2word\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
       "\u001b[0m    \u001b[0mdistributed\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
       "\u001b[0m    \u001b[0mchunksize\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m2000\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
       "\u001b[0m    \u001b[0mpasses\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
       "\u001b[0m    \u001b[0mupdate_every\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
       "\u001b[0m    \u001b[0malpha\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'symmetric'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
       "\u001b[0m    \u001b[0meta\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
       "\u001b[0m    \u001b[0mdecay\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0.5\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
       "\u001b[0m    \u001b[0moffset\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1.0\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
       "\u001b[0m    \u001b[0meval_every\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m10\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
       "\u001b[0m    \u001b[0miterations\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m50\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
       "\u001b[0m    \u001b[0mgamma_threshold\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0.001\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
       "\u001b[0m    \u001b[0mminimum_probability\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0.01\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
       "\u001b[0m    \u001b[0mrandom_state\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
       "\u001b[0m    \u001b[0mns_conf\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
       "\u001b[0m    \u001b[0mminimum_phi_value\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0.01\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
       "\u001b[0m    \u001b[0mper_word_topics\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
       "\u001b[0m    \u001b[0mcallbacks\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
       "\u001b[0m    \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m<\u001b[0m\u001b[1;32mclass\u001b[0m \u001b[1;34m'numpy.float32'\u001b[0m\u001b[1;33m>\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
       "\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
       "\u001b[1;31mDocstring:\u001b[0m     \n",
       "Train and use Online Latent Dirichlet Allocation (OLDA) models as presented in\n",
       "`Hoffman et al. :\"Online Learning for Latent Dirichlet Allocation\" <https://www.di.ens.fr/~fbach/mdhnips2010.pdf>`_.\n",
       "\n",
       "Examples\n",
       "-------\n",
       "Initialize a model using a Gensim corpus\n",
       "\n",
       ".. sourcecode:: pycon\n",
       "\n",
       "    >>> from gensim.test.utils import common_corpus\n",
       "    >>>\n",
       "    >>> lda = LdaModel(common_corpus, num_topics=10)\n",
       "\n",
       "You can then infer topic distributions on new, unseen documents.\n",
       "\n",
       ".. sourcecode:: pycon\n",
       "\n",
       "    >>> doc_bow = [(1, 0.3), (2, 0.1), (0, 0.09)]\n",
       "    >>> doc_lda = lda[doc_bow]\n",
       "\n",
       "The model can be updated (trained) with new documents.\n",
       "\n",
       ".. sourcecode:: pycon\n",
       "\n",
       "    >>> # In practice (corpus =/= initial training corpus), but we use the same here for simplicity.\n",
       "    >>> other_corpus = common_corpus\n",
       "    >>>\n",
       "    >>> lda.update(other_corpus)\n",
       "\n",
       "Model persistency is achieved through :meth:`~gensim.models.ldamodel.LdaModel.load` and\n",
       ":meth:`~gensim.models.ldamodel.LdaModel.save` methods.\n",
       "\u001b[1;31mInit docstring:\u001b[0m\n",
       "Parameters\n",
       "----------\n",
       "corpus : {iterable of list of (int, float), scipy.sparse.csc}, optional\n",
       "    Stream of document vectors or sparse matrix of shape (`num_documents`, `num_terms`).\n",
       "    If not given, the model is left untrained (presumably because you want to call\n",
       "    :meth:`~gensim.models.ldamodel.LdaModel.update` manually).\n",
       "num_topics : int, optional\n",
       "    The number of requested latent topics to be extracted from the training corpus.\n",
       "id2word : {dict of (int, str), :class:`gensim.corpora.dictionary.Dictionary`}\n",
       "    Mapping from word IDs to words. It is used to determine the vocabulary size, as well as for\n",
       "    debugging and topic printing.\n",
       "distributed : bool, optional\n",
       "    Whether distributed computing should be used to accelerate training.\n",
       "chunksize :  int, optional\n",
       "    Number of documents to be used in each training chunk.\n",
       "passes : int, optional\n",
       "    Number of passes through the corpus during training.\n",
       "update_every : int, optional\n",
       "    Number of documents to be iterated through for each update.\n",
       "    Set to 0 for batch learning, > 1 for online iterative learning.\n",
       "alpha : {numpy.ndarray, str}, optional\n",
       "    Can be set to an 1D array of length equal to the number of expected topics that expresses\n",
       "    our a-priori belief for the each topics' probability.\n",
       "    Alternatively default prior selecting strategies can be employed by supplying a string:\n",
       "\n",
       "        * 'asymmetric': Uses a fixed normalized asymmetric prior of `1.0 / topicno`.\n",
       "        * 'auto': Learns an asymmetric prior from the corpus (not available if `distributed==True`).\n",
       "eta : {float, np.array, str}, optional\n",
       "    A-priori belief on word probability, this can be:\n",
       "\n",
       "        * scalar for a symmetric prior over topic/word probability,\n",
       "        * vector of length num_words to denote an asymmetric user defined probability for each word,\n",
       "        * matrix of shape (num_topics, num_words) to assign a probability for each word-topic combination,\n",
       "        * the string 'auto' to learn the asymmetric prior from the data.\n",
       "decay : float, optional\n",
       "    A number between (0.5, 1] to weight what percentage of the previous lambda value is forgotten\n",
       "    when each new document is examined. Corresponds to Kappa from\n",
       "    `Matthew D. Hoffman, David M. Blei, Francis Bach:\n",
       "    \"Online Learning for Latent Dirichlet Allocation NIPS'10\" <https://www.di.ens.fr/~fbach/mdhnips2010.pdf>`_.\n",
       "offset : float, optional\n",
       "    Hyper-parameter that controls how much we will slow down the first steps the first few iterations.\n",
       "    Corresponds to Tau_0 from `Matthew D. Hoffman, David M. Blei, Francis Bach:\n",
       "    \"Online Learning for Latent Dirichlet Allocation NIPS'10\" <https://www.di.ens.fr/~fbach/mdhnips2010.pdf>`_.\n",
       "eval_every : int, optional\n",
       "    Log perplexity is estimated every that many updates. Setting this to one slows down training by ~2x.\n",
       "iterations : int, optional\n",
       "    Maximum number of iterations through the corpus when inferring the topic distribution of a corpus.\n",
       "gamma_threshold : float, optional\n",
       "    Minimum change in the value of the gamma parameters to continue iterating.\n",
       "minimum_probability : float, optional\n",
       "    Topics with a probability lower than this threshold will be filtered out.\n",
       "random_state : {np.random.RandomState, int}, optional\n",
       "    Either a randomState object or a seed to generate one. Useful for reproducibility.\n",
       "ns_conf : dict of (str, object), optional\n",
       "    Key word parameters propagated to :func:`gensim.utils.getNS` to get a Pyro4 Nameserved.\n",
       "    Only used if `distributed` is set to True.\n",
       "minimum_phi_value : float, optional\n",
       "    if `per_word_topics` is True, this represents a lower bound on the term probabilities.\n",
       "per_word_topics : bool\n",
       "    If True, the model also computes a list of topics, sorted in descending order of most likely topics for\n",
       "    each word, along with their phi values multiplied by the feature length (i.e. word count).\n",
       "callbacks : list of :class:`~gensim.models.callbacks.Callback`\n",
       "    Metric callbacks to log and visualize evaluation metrics of the model during training.\n",
       "dtype : {numpy.float16, numpy.float32, numpy.float64}, optional\n",
       "    Data-type to use during calculations inside model. All inputs are also converted.\n",
       "\u001b[1;31mFile:\u001b[0m           c:\\users\\dev\\documents\\repositories\\job-hunting\\jh\\lib\\site-packages\\gensim\\models\\ldamodel.py\n",
       "\u001b[1;31mType:\u001b[0m           type\n",
       "\u001b[1;31mSubclasses:\u001b[0m     LdaMulticore, AuthorTopicModel\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "from gensim import models\n",
    "\n",
    "models.LdaModel?"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "\n",
    "import sys\n",
    "\n",
    "# After this has completed, run these commands in a PowerShell window:\n",
    "# cd $Env:UserProfile\\Documents\\Repositories\\job-hunting\\ps1\n",
    "# clear\n",
    "# .\\create_job_hunting_temp_environment_yml_file.ps1\n",
    "command_str = f'{sys.executable} -m pip install --upgrade statsmodels'\n",
    "print(command_str)\n",
    "!{command_str}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\Users\\dev\\Documents\\Repositories\\job-hunting\\ipynb\\OS Path Navigation.ipynb\n",
      "['s.attempt_to_pickle', 's.data_csv_folder', 's.data_folder', 's.encoding_type', 's.load_csv', 's.load_dataframes', 's.load_object', 's.pickle_exists', 's.save_dataframes', 's.saves_csv_folder', 's.saves_folder', 's.saves_pickle_folder', 's.store_objects']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['Config', 'In', 'Out', 'RandomForestClassifier', 'SequenceMatcher', 'Storage', 'check_4_doubles', 'check_for_typos', 'command_str', 'conjunctify_list', 'copyfile', 'csv', 'encoding', 'example_iterrows', 'exit', 'filepath_regex', 'get_all_files_containing', 'get_classifier', 'get_column_descriptions', 'get_data_structs_dataframe', 'get_datastructure_prediction', 'get_dir_tree', 'get_git_lfs_track_commands', 'get_importances', 'get_input_sample', 'get_ipython', 'get_max_rsquared_adj', 'get_module_version', 'get_modules_dataframe', 'get_notebook_path', 'get_page_tables', 'get_specific_gitignore_files', 'get_struct_name', 'humanize_bytes', 'io', 'ipykernel', 'json', 'jupyter_config_dir', 'math', 'notebook_path', 'notebookapp', 'np', 'os', 'pd', 'pickle', 'plt', 'preprocess_data', 'print_all_files_ending_starting_with', 'print_all_files_ending_with', 'print_all_files_starting_with', 'quit', 're', 'remove_empty_folders', 's', 'similar', 'sm', 'sns', 'stats', 'subprocess', 'sys', 'time', 'url_regex', 'urllib']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "%run ../load_magic/storage.py\n",
    "%run ../load_magic/dataframes.py\n",
    "%run ../load_magic/paths.py\n",
    "%run ../load_magic/lists.py\n",
    "%run ../load_magic/environment.py\n",
    "notebook_path = get_notebook_path()\n",
    "print(notebook_path)\n",
    "s = Storage()\n",
    "print(['s.{}'.format(fn) for fn in dir(s) if not fn.startswith('_')])\n",
    "[fn for fn in dir() if not fn.startswith('_')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>first_item</th>\n",
       "      <th>second_item</th>\n",
       "      <th>first_bytes</th>\n",
       "      <th>second_bytes</th>\n",
       "      <th>max_similarity</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>IsI!</td>\n",
       "      <td>IsI!</td>\n",
       "      <td>73-115-73-33</td>\n",
       "      <td>73-115-73-33</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  first_item second_item   first_bytes  second_bytes  max_similarity\n",
       "0       IsI!        IsI!  73-115-73-33  73-115-73-33             1.0"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "password_str = 'IsI!'\n",
    "check_4_doubles([password_str, password_str], verbose=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def get_all_directories_containing(root_dir=r'C:\\Users\\dev\\anaconda3\\envs', contains_str='activate',\n",
    "                                   black_list=['$RECYCLE.BIN', '$Recycle.Bin', '.git']):\n",
    "    dir_path_list = []\n",
    "    if type(root_dir) == list:\n",
    "        root_dir_list = root_dir\n",
    "    else:\n",
    "        root_dir_list = [root_dir]\n",
    "    if type(contains_str) == list:\n",
    "        contains_list = contains_str\n",
    "    else:\n",
    "        contains_list = [contains_str]\n",
    "    for root_dir in root_dir_list:\n",
    "        for sub_directory, directories_list, files_list in os.walk(root_dir):\n",
    "            if all(map(lambda x: x not in sub_directory, black_list)):\n",
    "                for dir_name in directories_list:\n",
    "                    contains_bool = False\n",
    "                    for contains_str in contains_list:\n",
    "                        contains_bool = contains_bool or (contains_str in dir_name)\n",
    "                    if contains_bool:\n",
    "                        dir_path = os.path.join(sub_directory, dir_name)\n",
    "                        dir_path_list.append(dir_path)\n",
    "    \n",
    "    return dir_path_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['C:\\\\\\\\Program Files\\\\WindowsApps\\\\Microsoft.MinecraftUWP_1.16.1002.0_x64__8wekyb3d8bbwe\\\\UAP.Assets\\\\minecraft', 'C:\\\\\\\\Program Files\\\\WindowsApps\\\\Microsoft.MinecraftUWP_1.16.1002.0_x64__8wekyb3d8bbwe\\\\UAP.Win10.Assets\\\\minecraft', 'C:\\\\\\\\Users\\\\Dave Babbitt\\\\AppData\\\\Roaming\\\\.minecraft', 'C:\\\\\\\\Users\\\\dev\\\\AppData\\\\Roaming\\\\.minecraft', 'C:\\\\\\\\Users\\\\dev\\\\AppData\\\\Roaming\\\\.minecraft\\\\libraries\\\\net\\\\minecraft', 'C:\\\\\\\\Users\\\\dev\\\\AppData\\\\Roaming\\\\.minecraft\\\\libraries\\\\net\\\\minecraftforge', 'C:\\\\\\\\Users\\\\dev\\\\Documents\\\\Repositories\\\\mine-hack\\\\java\\\\org\\\\numenta\\\\minecraft']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "get_all_directories_containing(root_dir=r'C:\\\\', contains_str='minecraft', black_list=['$RECYCLE.BIN', '$Recycle.Bin', '.git'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "!start %windir%\\explorer.exe \"os.path.abspath('C:\\\\\\\\Users\\\\Dave Babbitt\\\\AppData\\\\Roaming\\\\.minecraft')\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\\\Users\\dev\\anaconda3\\Scripts\\speedtest-cli.exe\n",
      "C:\\\\Users\\dev\\anaconda3\\Scripts\\speedtest.exe\n",
      "C:\\\\Users\\dev\\Downloads\\ookla-speedtest-1.0.0-win64\\speedtest.exe\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print_all_files_ending_starting_with(\n",
    "    root_dir=[r'C:\\\\'],\n",
    "    ends_with='.exe',\n",
    "    starts_with='speedtest',\n",
    "    black_list=['$RECYCLE.BIN', '$Recycle.Bin', '.git'],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\Users\\dev\\Documents\\Repositories\\animation_nodes\\animation_nodes\\libs\\FastNoiseSIMD\\source\\compile_windows.bat\n",
      "C:\\Users\\dev\\Documents\\Repositories\\Asset-Flinger\\add_mesh_asset_flinger\\thumbnailer\\Windows\\Thumbnailer - METAL.bat\n",
      "C:\\Users\\dev\\Documents\\Repositories\\Asset-Flinger\\add_mesh_asset_flinger\\thumbnailer\\Windows\\Thumbnailer.bat\n",
      "C:\\Users\\dev\\Documents\\Repositories\\basemap\\geos-3.3.3\\autogen.bat\n",
      "C:\\Users\\dev\\Documents\\Repositories\\BioBlender\\data\\runAPBS.bat\n",
      "C:\\Users\\dev\\Documents\\Repositories\\blenderseed\\docs\\make.bat\n",
      "C:\\Users\\dev\\Documents\\Repositories\\FACSvatar\\docs\\make.bat\n",
      "C:\\Users\\dev\\Documents\\Repositories\\notebooks\\bible\\data\\txt\\kjvtxt\\combine.bat\n",
      "C:\\Users\\dev\\Documents\\Repositories\\notebooks\\cookbook-code-master\\notebooks\\chapter05_hpc\\_launch_notebook.bat\n",
      "C:\\Users\\dev\\Documents\\Repositories\\notebooks\\statsintro_python-master\\ipynb\\make.bat\n",
      "C:\\Users\\dev\\Documents\\Repositories\\numenta-apps\\taurus-mobile\\android\\gradlew.bat\n",
      "C:\\Users\\dev\\Documents\\Repositories\\rpc\\bat\\get_monthly_sermonaudio_stats.bat\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print_all_files_ending_with(\n",
    "    root_dir=[r'C:\\Users\\dev\\Documents\\Repositories'],\n",
    "    ends_with='.bat',\n",
    "    black_list=['$RECYCLE.BIN', '$Recycle.Bin', '.git', 'pkgs'],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "contains_str = 'job_hunting'\n",
    "file_path_list = get_all_files_containing(root_dir=r'C:\\Users\\dev\\Documents\\Repositories\\rpc\\ps1', contains_str=contains_str)\n",
    "for file_path in file_path_list:\n",
    "    os.rename(file_path, file_path.replace(contains_str, 'rpc'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "APPDATA C:\\Users\\dev\\AppData\\Roaming\n",
      "HOMEPATH \\Users\\dev\n",
      "LOCALAPPDATA C:\\Users\\dev\\AppData\\Local\n",
      "ONEDRIVE C:\\Users\\dev\\OneDrive\n",
      "ONEDRIVECONSUMER C:\\Users\\dev\\OneDrive\n",
      "PATH C:\\Users\\dev\\anaconda3;C:\\Users\\dev\\anaconda3\\Library\\mingw-w64\\bin;C:\\Users\\dev\\anaconda3\\Library\\usr\\bin;C:\\Users\\dev\\anaconda3\\Library\\bin;C:\\Users\\dev\\anaconda3\\Scripts;C:\\Program Files (x86)\\Common Files\\Oracle\\Java\\javapath;C:\\ffmpeg\\bin;C:\\ProgramData\\Oracle\\Java\\javapath;C:\\Windows\\system32;C:\\Windows;C:\\Windows\\System32\\Wbem;C:\\Windows\\System32\\WindowsPowerShell\\v1.0\\;C:\\Users\\dev\\AppData\\Local\\Microsoft\\WindowsApps;C:\\Users\\dev\\AppData\\Local\\GitHubDesktop\\bin;C:\\WINDOWS\\system32;C:\\WINDOWS;C:\\WINDOWS\\System32\\Wbem;C:\\WINDOWS\\System32\\WindowsPowerShell\\v1.0\\;C:\\WINDOWS\\System32\\OpenSSH\\;C:\\Program Files\\MiKTeX 2.9\\miktex\\bin\\x64\\;C:\\Program Files\\dotnet\\;C:\\libav\\usr\\bin;C:\\Program Files (x86)\\Microsoft Visual Studio 14.0;C:\\ProgramData\\chocolatey\\bin;C:\\Program Files\\nodejs\\;C:\\Program Files (x86)\\Gow\\bin;C:\\Program Files\\PowerShell\\7\\;C:\\Program Files\\Git\\cmd;C:\\Program Files\\Graphviz 2.44.1\\bin;C:\\Program Files (x86)\\WinGraphviz;C:\\Program Files\\R\\R-4.0.3\\bin\\x64;C:\\Users\\dev\\anaconda3;C:\\Users\\dev\\anaconda3\\Library\\mingw-w64\\bin;C:\\Users\\dev\\anaconda3\\Library\\usr\\bin;C:\\Users\\dev\\anaconda3\\Library\\bin;C:\\Users\\dev\\anaconda3\\Scripts;C:\\Program Files (x86)\\Nmap;C:\\Users\\dev\\AppData\\Roaming\\npm\n",
      "PYSPARK_DRIVER_PYTHON C:\\Users\\dev\\Anaconda3\\Scripts\\jupyter-lab.exe\n",
      "PYWIKIBOT_DIR C:\\Users\\dev\\Documents\\repositories\\notebooks\\Miscellaneous\\py\n",
      "TEMP C:\\Users\\dev\\AppData\\Local\\Temp\n",
      "TMP C:\\Users\\dev\\AppData\\Local\\Temp\n",
      "USERNAME dev\n",
      "USERPROFILE C:\\Users\\dev\n",
      "CONDA_PREFIX C:\\Users\\dev\\anaconda3\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import os\n",
    "\n",
    "for key, value in dict(os.environ).items():\n",
    "    if 'dev' in value:\n",
    "        print(key, value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "%windir%\\System32\\cmd.exe \"/K\" C:\\Users\\dev\\anaconda3\\Scripts\\activate.bat C:\\Users\\dev\\anaconda3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import subprocess\n",
    "\n",
    "repos_list = [r'C:\\Users\\dev\\Documents\\Repositories\\covid19', r'C:\\Users\\dev\\Documents\\Repositories\\notebooks\\flask',\n",
    "              r'C:\\Users\\dev\\Documents\\Repositories\\notebooks\\pt', r'C:\\Users\\dev\\Documents\\Repositories\\notebooks\\pymc3',\n",
    "              r'C:\\Users\\dev\\Documents\\Repositories\\notebooks\\rpc', r'C:\\Users\\dev\\Documents\\Repositories\\notebooks\\test',\n",
    "              r'C:\\Users\\dev\\Documents\\Repositories\\notebooks\\tf', r'C:\\Users\\dev\\Documents\\Repositories\\notebooks\\x']\n",
    "envs_list = ['covid19', 'flask', 'pt', 'pymc3', 'rpc', 'test', 'tf', 'x']\n",
    "for repo_path, env_name in zip(repos_list, envs_list):\n",
    "    print()\n",
    "    print('-'*len(env_name))\n",
    "    print(env_name)\n",
    "    print('-'*len(env_name))\n",
    "    command_str = f'''\n",
    "cd \"{repo_path}\"\n",
    "conda activate {env_name}\n",
    "conda env export --name {env_name} -f tmp_environment.yml\n",
    "conda deactivate\n",
    "'''\n",
    "    process = subprocess.Popen(r'C:\\Program Files\\PowerShell\\7\\pwsh.exe', stdin=subprocess.PIPE, stdout=subprocess.PIPE)\n",
    "    std_out, std_err = process.communicate(command_str.encode(encoding='utf-8'))\n",
    "    #print(std_out.decode())\n",
    "    if std_err is not None:\n",
    "        print(std_err.decode())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "env_pkgs_dict = {}\n",
    "for repo_path, env_name in zip(repos_list, envs_list):\n",
    "    file_path = os.path.join(repo_path, 'tmp_environment.yml')\n",
    "    with open(file_path, 'r') as file:\n",
    "        lines_list = file.read().split('\\n')[5:-2]\n",
    "        pkgs_list = [line_str.split('=')[0].split(' ')[-1] for line_str in lines_list]\n",
    "        env_pkgs_dict[env_name] = set(pkgs_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "union_set = set()\n",
    "for env_name, pkgs_set in env_pkgs_dict.items():\n",
    "    union_set = union_set.union(pkgs_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "intersection_set = union_set.copy()\n",
    "for env_name, pkgs_set in env_pkgs_dict.items():\n",
    "    intersection_set = intersection_set.intersection(pkgs_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  - prompt-toolkit\n",
      "  - vs2015_runtime\n",
      "  - backcall\n",
      "  - jedi\n",
      "  - parso\n",
      "(prompt-toolkit|vs2015_runtime|backcall|jedi|parso)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "repo_path = r'C:\\Users\\dev\\Documents\\Repositories\\job-hunting'\n",
    "file_path = os.path.join(repo_path, 'tmp_environment.yml')\n",
    "with open(file_path, 'r') as file:\n",
    "    lines_list = file.read().split('\\n')[5:-2]\n",
    "    pkgs_list = [line_str.split('=')[0].split(' ')[-1] for line_str in lines_list]\n",
    "    pkgs_set = set(pkgs_list)\n",
    "missing_set = intersection_set - pkgs_set\n",
    "for pkg_name in missing_set:\n",
    "    print(f'  - {pkg_name}')\n",
    "print(f'({\"|\".join(missing_set)})')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\n",
      "C:\\ProgramData\\Microsoft\\Windows\\Start Menu\\Programs\\Anaconda3 (64-bit)\n",
      "C:\\Users\\577342\\AppData\\Roaming\\Microsoft\\Windows\\Start Menu\\Programs\\Anaconda3 (64-bit)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "for root_dir in ['C:\\\\', 'D:\\\\']:\n",
    "    for sub_directory, directories_list, files_list in os.walk(root_dir):\n",
    "        if 'Anaconda3' in sub_directory.split(os.sep)[-1]:\n",
    "            print(sub_directory)\n",
    "            !start %windir%\\explorer.exe \"os.path.abspath(sub_directory)\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "----"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# Ignore big files (GitHub will warn you when pushing files larger than 50 MB. You will not be allowed to\n",
      "# push files larger than 100 MB.) Tip: If you regularly push large files to GitHub, consider introducing\n",
      "# Git Large File Storage (Git LFS) as part of your workflow.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "get_specific_gitignore_files('data-foundations', repository_dir=r'D:\\Documents\\Repositories')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "import collections\n",
    "\n",
    "columns_list = jira_df.columns.tolist()\n",
    "[item for item, count in collections.Counter(columns_list).items() if count > 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>column_name</th>\n",
       "      <th>dtype</th>\n",
       "      <th>count_blanks</th>\n",
       "      <th>count_uniques</th>\n",
       "      <th>count_zeroes</th>\n",
       "      <th>has_dates</th>\n",
       "      <th>min_value</th>\n",
       "      <th>max_value</th>\n",
       "      <th>only_integers</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>Created</td>\n",
       "      <td>object</td>\n",
       "      <td>0</td>\n",
       "      <td>26</td>\n",
       "      <td>0</td>\n",
       "      <td>True</td>\n",
       "      <td>2019-04-19 11:37:00</td>\n",
       "      <td>2020-01-13 09:25:00</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>Updated</td>\n",
       "      <td>object</td>\n",
       "      <td>0</td>\n",
       "      <td>19</td>\n",
       "      <td>0</td>\n",
       "      <td>True</td>\n",
       "      <td>2019-05-29 12:10:00</td>\n",
       "      <td>2020-04-14 08:09:00</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>Resolved</td>\n",
       "      <td>object</td>\n",
       "      <td>2</td>\n",
       "      <td>25</td>\n",
       "      <td>0</td>\n",
       "      <td>True</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>Fix Version/s</td>\n",
       "      <td>object</td>\n",
       "      <td>16</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>True</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      column_name   dtype  count_blanks  count_uniques  count_zeroes  \\\n",
       "10        Created  object             0             26             0   \n",
       "12        Updated  object             0             19             0   \n",
       "13       Resolved  object             2             25             0   \n",
       "15  Fix Version/s  object            16              3             0   \n",
       "\n",
       "    has_dates            min_value            max_value only_integers  \n",
       "10       True  2019-04-19 11:37:00  2020-01-13 09:25:00           NaN  \n",
       "12       True  2019-05-29 12:10:00  2020-04-14 08:09:00           NaN  \n",
       "13       True                  NaN                  NaN           NaN  \n",
       "15       True                  NaN                  NaN           NaN  "
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "column_descriptions_df = get_column_descriptions(jira_df)\n",
    "mask_series = column_descriptions_df.has_dates & column_descriptions_df.only_integers.isnull()\n",
    "column_descriptions_df[mask_series]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "----"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "D:\\Documents\\Repositories\\notebooks\\age-of-empires-ii\\ipynb\\AoE2 Civilization Bonus Feature Selection.ipynb\n"
     ]
    }
   ],
   "source": [
    "\n",
    "plt_regex = re.compile(r'([\\r\\n]+)    \"import matplotlib\\.pyplot as plt\\\\n\",')\n",
    "notebooks_dir = r'D:\\Documents\\Repositories\\notebooks'\n",
    "stopped = False\n",
    "for sub_directory, directories_list, files_list in os.walk(notebooks_dir):\n",
    "    if stopped:\n",
    "        break\n",
    "    for file_name in files_list:\n",
    "        if stopped:\n",
    "            break\n",
    "        if file_name.endswith('.ipynb'):\n",
    "            file_path = os.path.join(sub_directory, file_name)\n",
    "            try:\n",
    "                with open(file_path, 'r') as f:\n",
    "                    file_str = f.read()\n",
    "                    if plt_regex.search(file_str):\n",
    "                        print(file_path)\n",
    "                        stopped = True\n",
    "                        break\n",
    "            except UnicodeDecodeError as e:\n",
    "                try:\n",
    "                    with open(file_path, 'rb') as f:\n",
    "                        file_str = f.read().decode('utf-8')\n",
    "                        if plt_regex.search(file_str):\n",
    "                            print(file_path)\n",
    "                except Exception as e:\n",
    "                    message = str(e).strip()\n",
    "                    print()\n",
    "                    print('{} had an error after trying to decode: {}'.format(file_path, message))\n",
    "                    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import pyperclip\n",
    "\n",
    "pyperclip.copy(str(file_str))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "gi = get_ipython()\n",
    "#print([fn for fn in dir(gi) if not fn.startswith('_')])\n",
    "gi.run_line_magic('who', '')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "remove_empty_folders(folder_path=r'D:\\VirtualBox VMs')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "magic_dict_list = magic_dict['test.py']\n",
    "print(len(magic_dict_list))\n",
    "subprocess.run([comparator_path, os.path.abspath(magic_dict_list[0]), os.path.abspath(magic_dict_list[1])])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "get_specific_gitignore_files('notebooks', repository_dir=r'C:\\Users\\dev\\Documents\\repositories')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import sys\n",
    "\n",
    "for key, value in sys.modules.items():\n",
    "    if 'xdist' in key.lower():\n",
    "        #print('{}: {}'.format(key, value))\n",
    "        print('{}'.format(key))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Job Hunting (Python 3.7.9)",
   "language": "python",
   "name": "jh"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
