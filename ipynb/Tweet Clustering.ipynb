{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import tweepy\n",
    "from tweepy import OAuthHandler\n",
    " \n",
    "consumer_key = ''\n",
    "consumer_secret = ''\n",
    " \n",
    "access_token = ''\n",
    "access_secret = ''\n",
    " \n",
    "auth = OAuthHandler(consumer_key, consumer_secret)\n",
    "auth.set_access_token(access_token, access_secret)\n",
    " \n",
    "api = tweepy.API(auth)\n",
    " \n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import datetime\n",
    "import sys\n",
    "import codecs\n",
    "import re\n",
    "import urllib, urllib2\n",
    "import itertools, collections\n",
    " \n",
    "import nltk  # Natural Language Processing\n",
    "#nltk.download('punkt')\n",
    "#nltk.download('all')\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "from nltk.corpus import stopwords # list of words\n",
    "from collections import Counter  # optimized way to do this\n",
    "import string  # list(string.punctuation) - produces a list of punctuations\n",
    "import copy\n",
    "from itertools import product, tee, combinations, chain\n",
    "from nltk.stem import PorterStemmer\n",
    "from operator import itemgetter # help with dataframes\n",
    " \n",
    "from scipy.spatial.distance import cosine\n",
    " \n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics import pairwise_distances_argmin\n",
    "from sklearn.utils import shuffle\n",
    " \n",
    "from tweepy import Stream\n",
    " \n",
    "encodingTot = sys.stdout.encoding or 'utf-8'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Similarity Measure *******************************************************************************************\n",
    "def cosine_sim(v1, v2):\n",
    "         \n",
    "    rho = round(1.0 - cosine(v1, v2), 3)\n",
    "    rho = rho if(not np.isnan(rho)) else 0.0\n",
    "    return rho\n",
    " \n",
    "# Words Replacement ***************************************************************************************\n",
    "def replace_all(text, dic):\n",
    "    for i, j in dic.iteritems():\n",
    "        text = text.replace(i, j)\n",
    "    return text\n",
    " \n",
    "# Function to find element with Maximum Frequency in TDM  *******************************************************************\n",
    "def nanargmax(a):\n",
    "    idx = np.argmax(a, axis=None)\n",
    "    multi_idx = np.unravel_index(idx, a.shape)\n",
    "    if np.isnan(a[multi_idx]):\n",
    "        nan_count = np.sum(np.isnan(a))\n",
    " \n",
    "        idx = np.argpartition(a, -nan_count-1, axis=None)[-nan_count-1]\n",
    "        multi_idx = np.unravel_index(idx, a.shape)\n",
    "    return multi_idx\n",
    " \n",
    "# Define Top K Neighbours to the WORD or TWEET ***************************************************************************\n",
    "def K_neighbor(k, term, list_t):\n",
    "     \n",
    "    # list_t - a list of tuples\n",
    "    # term - value of criteria (tweet or word)\n",
    "     \n",
    "    neighbor = []\n",
    "    neighbor = [item for item in list_t if term in item] \n",
    "    neighbor.append(item) \n",
    "     \n",
    "    neighbor.sort(key = itemgetter(0), reverse=True)\n",
    "       \n",
    "    print 'Top ', k, ' elements for ', term   \n",
    "    print '**********************************************'\n",
    "         \n",
    "    for i in xrange(k):\n",
    "        print neighbor[i]\n",
    "     \n",
    "    return neighbor[:k]\n",
    " \n",
    "# Determine Pair of Words Counter method ******************************************************************************\n",
    "def Pair_words(word_list, tweet_clean_fin, n_top):\n",
    " \n",
    "    pairs = list(itertools.combinations(word_list, 2)) # order does not matter\n",
    " \n",
    "    #pairs = set(map(tuple, map(sorted, _pairs)))\n",
    "    pairs = set(pairs)\n",
    "    c = collections.Counter()\n",
    " \n",
    "    for tweet in tweet_clean_fin:\n",
    "        for pair in pairs:\n",
    "            if pair[0] == pair[1]: \n",
    "                pass\n",
    "            elif pair[0] in tweet and pair[1] in tweet:\n",
    "                #c.update({pair: 1})\n",
    "                c[pair] +=1\n",
    "  \n",
    "    return c.most_common(n_top)\n",
    " \n",
    "# BIC score function ********************************************************************************\n",
    " \n",
    "from sklearn import cluster\n",
    "from scipy.spatial import distance\n",
    "from sklearn.preprocessing import StandardScaler\n",
    " \n",
    "def compute_bic(kmeans,X):\n",
    "    \"\"\"\n",
    "    Computes the BIC metric for given clusters\n",
    " \n",
    "    Parameters:\n",
    "    -----------------------------------------\n",
    "    kmeans:  List of clustering object from scikit learn\n",
    " \n",
    "    X     :  multidimension np array of data points\n",
    " \n",
    "    \"\"\"\n",
    "    # assign centers and labels\n",
    "    centers = [kmeans.cluster_centers_]\n",
    "    labels  = kmeans.labels_\n",
    "    #number of clusters\n",
    "    m = kmeans.n_clusters\n",
    "    # size of the clusters\n",
    "    n = np.bincount(labels)\n",
    "    #size of data set\n",
    "    N, d = X.shape\n",
    " \n",
    "    #compute variance for all clusters beforehand\n",
    "    cl_var =  (1.0 / (N - m) / d) * sum([sum(distance.cdist(X[np.where(labels == i)], [centers[0][i]], 'euclidean')**2) for i in range(m)])\n",
    "    const_term = 0.5 * m * np.log(N) * (d+1)\n",
    " \n",
    "    BIC = np.sum([n[i] * np.log(n[i]) -\n",
    "               n[i] * np.log(N) -\n",
    "             ((n[i] * d) / 2) * np.log(2*np.pi*cl_var) -\n",
    "             ((n[i] - 1) * d/ 2) for i in range(m)]) - const_term\n",
    " \n",
    "    return(BIC)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Store to file text, who and time\n",
    " \n",
    "columns = ['Screen_Name', 'Time_Stamp', 'Tweet']\n",
    "todays_date = datetime.datetime.now().date()\n",
    " \n",
    "tweetDF = pd.DataFrame(columns=columns)\n",
    " \n",
    "num_tweets = 500\n",
    " \n",
    "for tweet in tweepy.Cursor(api.search, q=\"google\", lang=\"en\").items(num_tweets):\n",
    "     \n",
    "    lenDF = len(tweetDF)\n",
    " \n",
    "    tweetDF.loc[lenDF] = [tweet.user.screen_name, tweet.created_at, tweet.text]\n",
    "         \n",
    "tweetDF.to_csv(\"C:/Users/Toly/Documents/Big Data/DeZyre/Data Science/IBM Project/tweetDF\", sep='\\t', encoding = encodingTot)\n",
    " \n",
    "#tweetDF = pd.read_csv(\"C:/Users/Toly/Documents/Big Data/DeZyre/Data Science/IBM Project/tweetDF\", sep='\\t')\n",
    " \n",
    "tweetDF = pd.read_csv(open('C:/Users/Toly/Documents/Big Data/DeZyre/Data Science/IBM Project/tweetDF','rU'), sep='\\t', engine='c')\n",
    " \n",
    "tweetDF[\"Tweet\"].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "tweet_list_org = tweetDF['Tweet'].tolist() # convert DF to list (tweets only) NOT a nested list\n",
    " \n",
    "# Regex from Gagan ************************************************************\n",
    " \n",
    "emoticons_str = r\"\"\"\n",
    "    (?:\n",
    "        [:=;] # Eyes\n",
    "        [oO\\-]? # Nose (optional)\n",
    "        [D\\)\\]\\(\\]/\\\\OpP] # Mouth\n",
    "    )\"\"\"\n",
    " \n",
    "# Regex_str is used to GET text from CSV file\n",
    " \n",
    "regex_str = [\n",
    "     \n",
    "    r'<[^>]+>', # HTML tags\n",
    "    r'(?:@[\\w_]+)', # @-signs\n",
    "    r\"(?:\\#+[\\w_]+[\\w\\'_\\-]*[\\w_]+)\", # hash-tags\n",
    "    r'http[s]?://(?:[a-z]|[0-9]|[$-_@.&+]|[!*\\(\\),]|(?:%[0-9a-f][0-9a-f]))+', # URLs \n",
    "    r'(?:(?:\\d+,?)+(?:\\.?\\d+)?)', # numbers\n",
    "    r\"(?:[a-z][a-z'\\-_]+[a-z])\", # words with - and '\n",
    "    r'(?:[\\w_]+)' # other words\n",
    "]\n",
    " \n",
    "# These Regex are used to EXCLUDE items from the text AFTER IMPORTING from csv with regex_str\n",
    " \n",
    "numbers = r'(?:(?:\\d+,?)+(?:\\.?\\d+)?)'\n",
    "URL = r'http[s]?://(?:[a-z]|[0-9]|[$-_@.&+]|[!*\\(\\),]|(?:%[0-9a-f][0-9a-f]))+'\n",
    "html_tag = r'<[^>]+>'\n",
    "hash_tag = r\"(?:\\#+[\\w_]+[\\w\\'_\\-]*[\\w_]+)\"\n",
    "at_sign = r'(?:@[\\w_]+)'\n",
    "dash_quote = r\"(?:[a-z][a-z'\\-_]+[a-z])\"\n",
    "other_word = r'(?:[\\w_]+)'\n",
    "other_stuff = r'(?:\\S)' # anything else - NOT USED\n",
    "start_pound = r\"([#?])(\\w+)\" # Start with #\n",
    "start_quest_pound = r\"(?:^|\\s)([#?])(\\w+)\" # Start with ? or with #\n",
    "cont_number = r'(\\w*\\d\\w*)' # Words containing numbers\n",
    " \n",
    "# My REGEX **************************************************************************\n",
    " \n",
    "#      Remove '[' and ']' brackets\n",
    " \n",
    "sq_br_f = r'(?:[[\\w_]+)' # removes '['\n",
    "sq_br_b = r'(?:][\\w_]+)' # removes ']'\n",
    " \n",
    "rem_bracket = r'(' + '|'.join([sq_br_f, sq_br_b]) +')'\n",
    "rem_bracketC = re.compile(rem_bracket, re.VERBOSE)\n",
    " \n",
    "# Removes all words of 3 characters or less *****************************************************\n",
    " \n",
    "short_words = r'\\W*\\b\\w{1,3}\\b' # Short words of 3 character or less\n",
    "short_wordsC = re.compile(short_words, re.VERBOSE | re.IGNORECASE)\n",
    " \n",
    "# REGEX remove all words with \\ and / combinations\n",
    " \n",
    "slash_back =  r'\\s*(?:[\\w_]*\\\\(?:[\\w_]*\\\\)*[\\w_]*)'\n",
    "slash_fwd = r'\\s*(?:[\\w_]*/(?:[\\w_]*/)*[\\w_]*)'\n",
    "slash_all = r'\\s*(?:[\\w_]*[/\\\\](?:[\\w_]*[/\\\\])*[\\w_]*)'\n",
    " \n",
    "# REGEX numbers, short words and URL only to EXCLUDE +++++++++++++++++++++++++++++++++++++++++++++++++++\n",
    " \n",
    "num_url_short = r'(' + '|'.join([numbers, URL, short_words + sq_br_f + sq_br_b]) +')'  # Exclude from tweets\n",
    "comp_num_url_short = re.compile(num_url_short, re.VERBOSE | re.IGNORECASE)\n",
    " \n",
    "# Master REGEX to INCLUDE from the original tweets ++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++\n",
    " \n",
    "list_regex = r'(' + '|'.join(regex_str) + ')'\n",
    " \n",
    "master_regex = re.compile(list_regex, re.VERBOSE | re.IGNORECASE) # TAKE from tweets INITIALLY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Filters IMPORTED from csv file data\n",
    " \n",
    "def filterPick(list, filter):\n",
    "    return [ ( l, m.group(1) ) for l in list for m in (filter(l),) if m]\n",
    " \n",
    "search_regex = re.compile(list_regex, re.VERBOSE | re.IGNORECASE).search\n",
    " \n",
    "# Use tweetList -  that is a list from DF (using .tolist())\n",
    " \n",
    "outlist_init = filterPick(tweet_list_org, search_regex) # It is a tuple: initial list from all tweets\n",
    " \n",
    "char_remove = [']', '[', '(', ')', '{', '}'] # characters to be removed\n",
    "words_keep = ['old', 'new', 'age', 'lot', 'bag', 'top', 'cat', 'bat', 'sap', 'jda', 'tea', 'dog', 'lie', 'law', 'lab',\\\n",
    "             'mob', 'map', 'car', 'fat', 'sea', 'saw', 'raw', 'rob', 'win', 'can', 'get', 'fan', 'fun', 'big',\\\n",
    "             'use', 'pea', 'pit','pot', 'pat', 'ear', 'eye', 'kit', 'pot', 'pen', 'bud', 'bet', 'god', 'tax', 'won', 'run',\\\n",
    "              'lid', 'log', 'pr', 'pd', 'cop', 'nyc', 'ny', 'la', 'toy', 'war', 'law', 'lax', 'jfk', 'fed', 'cry', 'ceo',\\\n",
    "              'pay', 'pet', 'fan', 'fun', 'usd', 'rio']\n",
    " \n",
    "emotion_list = [':)', ';)', '(:', '(;', '}', '{','}']\n",
    "word_garb = ['here', 'there', 'where', 'when', 'would', 'should', 'could','thats', 'youre', 'thanks', 'hasn',\\\n",
    "             'thank', 'https', 'since', 'wanna', 'gonna', 'aint', 'http', 'unto', 'onto', 'into', 'havent',\\\n",
    "             'dont', 'done', 'cant', 'werent', 'https', 'u', 'isnt', 'go', 'theyre', 'each', 'every', 'shes', 'youve', 'youll',\\\n",
    "            'weve', 'theyve']\n",
    " \n",
    "# Dictionary with Replacement Pairs ******************************************************************************\n",
    "repl_dict = {'googleele': 'goog', 'lyin': 'lie', 'googles': 'goog', 'aapl':'apple',\\\n",
    "             'msft':'microsoft', 'google': 'goog', 'googl':'goog'}\n",
    " \n",
    "exclude = list(string.punctuation) + emotion_list + word_garb\n",
    " \n",
    "# Convert tuple to a list, then to a string; Remove the characters; Stays as a STRING. Porter Stemmer\n",
    " \n",
    "stemmer=PorterStemmer()\n",
    "lmtzr = WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Convert tuple to a list, then to a string; Remove the characters; Stays as a STRING. Porter Stemmer\n",
    " \n",
    "# Preparing CLEAN tweets tp keep SEPARATELY from WORDS in TWEETS\n",
    " \n",
    "tweet_clean_fin = [] # Cleaned Tweets - Final Version\n",
    "for tweet in outlist_init:\n",
    " \n",
    "    tw_clean = []\n",
    "    tw_clean = [ch for ch in tweet if ch not in char_remove]\n",
    " \n",
    "    tw_clean = re.sub(URL, \"\", str(tw_clean))\n",
    "    tw_clean = re.sub(html_tag, \"\",str(tw_clean))\n",
    "    tw_clean = re.sub(hash_tag, \"\",str(tw_clean))\n",
    "    tw_clean = re.sub(slash_all,\"\", str(tw_clean))\n",
    "    tw_clean = re.sub(cont_number, \"\",str(tw_clean))\n",
    "    tw_clean = re.sub(numbers, \"\",str(tw_clean))\n",
    "    tw_clean = re.sub(start_pound, \"\",str(tw_clean))\n",
    "    tw_clean = re.sub(start_quest_pound, \"\",str(tw_clean))\n",
    "    tw_clean = re.sub(at_sign, \"\",str(tw_clean))\n",
    "    tw_clean = re.sub(\"'\", \"\",str(tw_clean))\n",
    "    tw_clean = re.sub('\"', \"\",str(tw_clean))\n",
    "    tw_clean = re.sub(r'(?:^|\\s)[@#].*?(?=[,;:.!?]|\\s|$)', r'', tw_clean) # Removes # and @ in words (lookahead)\n",
    " \n",
    "    tw_clean = lmtzr.lemmatize(str(tw_clean))\n",
    "    #tw_clean = stemmer.stem(str(tw_clean))\n",
    "     \n",
    "    tw_clean_lst = re.findall(r'\\w+', str(tw_clean))\n",
    "     \n",
    "    tw_clean_lst = [tw.lower() for tw in tw_clean_lst if tw.lower() not in stopwords.words('english')]\n",
    "    tw_clean_lst = [word for word in tw_clean_lst if word not in exclude]\n",
    "    tw_clean_lst = str([word for word in tw_clean_lst if len(word)>3 or word.lower() in words_keep])\n",
    "     \n",
    "    tw_clean_lst = re.findall(r'\\w+', str(tw_clean_lst))\n",
    "    tw_clean_lst = [replace_all(word, repl_dict) for word in tw_clean_lst]\n",
    "     \n",
    "    tweet_clean_fin.append(list(tw_clean_lst))\n",
    " \n",
    "# Delete various elements from the text (LIST OF WORDS)\n",
    " \n",
    "out_list_fin = []\n",
    "out_string_temp = ''.join([ch for ch in str(list(outlist_init)) if ch not in char_remove])\n",
    " \n",
    "out_string_temp = re.sub(URL, \"\", out_string_temp)\n",
    "out_string_temp = re.sub(html_tag, \"\", out_string_temp)\n",
    "out_string_temp = re.sub(hash_tag, \"\", out_string_temp)\n",
    "out_string_temp = re.sub(slash_all,\"\", str(out_string_temp))\n",
    "out_string_temp = re.sub(cont_number, \"\", out_string_temp) \n",
    "out_string_temp = re.sub(numbers, \"\", out_string_temp)\n",
    "out_string_temp = re.sub(start_pound, \"\", out_string_temp)\n",
    "out_string_temp = re.sub(start_quest_pound, \"\", out_string_temp)\n",
    "out_string_temp = re.sub(at_sign, \"\", out_string_temp)\n",
    "out_string_temp = re.sub(\"'\", \"\", out_string_temp)\n",
    "out_string_temp = re.sub('\"', \"\", out_string_temp)\n",
    "out_string_temp = re.sub(r'(?:^|\\s)[@#].*?(?=[,;:.!?]|\\s|$)', r'', out_string_temp) # Removes # and @ in words (lookahead)\n",
    " \n",
    "out_list_w = re.findall(r'\\w+', out_string_temp)\n",
    " \n",
    "out_string_short = str([word.lower() for word in out_list_w if len(word)>3 or word.lower() in words_keep])\n",
    " \n",
    "out_list_w = re.findall(r'\\w+', out_string_short)   \n",
    " \n",
    "out_list_w = [lmtzr.lemmatize(word) for word in out_list_w]\n",
    "#out_list_w = [stemmer.stem(word) for word in out_list_w]\n",
    "out_list_w = [word.lower() for word in out_list_w if word.lower() not in stopwords.words('english')]  # Remove stopwords\n",
    "out_list_w = str([word.lower() for word in out_list_w if word not in exclude])\n",
    "out_string_rpl = replace_all(out_list_w, repl_dict) # replace all words from dictionary\n",
    " \n",
    "# Convert \"Cleaned\" STRING to a LIST\n",
    " \n",
    "out_list_fin = re.findall(r'\\w+', out_string_rpl)\n",
    " \n",
    "list_len = len(out_list_fin)\n",
    "word_list = set(out_list_fin) # list of unique words from all tweets - SET\n",
    "word_list_len = len(word_list)\n",
    " \n",
    "print \"Set = \", word_list_len, \"Original Qty = \", list_len\n",
    "print word_list\n",
    "print '********************************************************************************************************'\n",
    "print tweet_clean_fin\n",
    "print len(tweet_clean_fin)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Create a matrix of frequencies for word pairs\n",
    " \n",
    "words = {v:k for (k, v) in enumerate(word_list)}\n",
    "keys = words.keys() # List all UNIQUE words in the dictionary from all CLEANED tweets   \n",
    "l_keys = len(keys) \n",
    " \n",
    "matrix_pair = np.zeros([l_keys, l_keys]) # store all combination of keys\n",
    " \n",
    "for tweet in tweet_clean_fin:\n",
    "    word_l = []\n",
    "     \n",
    "    for word in tweet:\n",
    "        word_l.append(word)         # List of words from ONE CLEANED tweet\n",
    "     \n",
    "    items = set(word_l)  #set of words in from ONE CLEANED tweet\n",
    "    items = [term for term in items if term in keys] # take only words from a tweet that are in keys\n",
    "    index = [words[pos] for pos in items] # positions of the words\n",
    " \n",
    "    for i1 in index: \n",
    "        for i2 in index:\n",
    "            if i1< i2:\n",
    "                matrix_pair[i1][i2] += 1  #frequency\n",
    "                 \n",
    "print \"Frequency Matrix *********************************************\"\n",
    "print matrix_pair\n",
    "print \"                                                              \"\n",
    " \n",
    "print 'Maximum Frequency', np.max(matrix_pair)\n",
    "print \"                                                             \"\n",
    " \n",
    "idx1, idx2 = nanargmax(matrix_pair)\n",
    " \n",
    "print \"Indexes for a pair with max frequency - \", idx1, idx2\n",
    "print \"Pair of Words with Max Frequency: Word1 - \", words.keys()[idx1], \"  Word2 - \", words.keys()[idx2]\n",
    "print \"                                                            \"\n",
    " \n",
    "# Selecting TOP N elements from the Matrix ##########################################################################\n",
    " \n",
    "n_top = 10\n",
    " \n",
    "matrix_pairF = matrix_pair.flatten()\n",
    "idx_f = matrix_pairF.argsort()[-n_top:]\n",
    "x_idx, y_idx = np.unravel_index(idx_f, matrix_pair.shape)\n",
    " \n",
    "for x, y, in zip(x_idx, y_idx):\n",
    "    print(\"Frequency = \", matrix_pair[x][y], \"index1 = \", x, \"index2 = \", y, \"Word1 - \", words.keys()[x], \"  Word2 - \", words.keys()[y])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Create document-term-matrix\n",
    " \n",
    "columns = word_list\n",
    "ncols = word_list_len + 1\n",
    " \n",
    "term_doc = pd.DataFrame(columns = columns)\n",
    "term_doc.insert(0, \"Tweet\", \" \")\n",
    "term_doc[\"Tweet\"] = tweetDF[\"Tweet\"]\n",
    "term_doc.fillna(0, inplace=True)\n",
    " \n",
    "i_row = 0\n",
    "for line in tweet_clean_fin:\n",
    "     \n",
    "    for word in line:\n",
    " \n",
    "        for col in xrange(1, ncols-1):\n",
    "            if word == term_doc.columns[col]: term_doc.iloc[i_row, col] += 1\n",
    " \n",
    "    i_row += 1\n",
    " \n",
    "# DataFrame for Statistics with Totals by Row and by Column\n",
    "     \n",
    "statDF = copy.deepcopy(term_doc)\n",
    "columns_cl = [\"Tweet\", \"Sim\"]\n",
    "tweet_sim = pd.DataFrame(columns = columns_cl)\n",
    "tweet_sim = tweetDF[\"Tweet\"]\n",
    "tweet_sim.fillna(0.0, inplace=True)\n",
    " \n",
    "# Sum Rows by Columns\n",
    "row_sum = statDF.sum(axis=1)\n",
    "statDF[\"Total\"] = row_sum\n",
    "print 'Row Max Value = ', row_sum.max()\n",
    "print \"Max Value DF = \", statDF[\"Total\"].max(axis=0)\n",
    " \n",
    "# Sum Columns by Row:\n",
    "col_list = list(statDF)\n",
    "col_list.remove('Tweet')\n",
    " \n",
    "rsum = {col: statDF[col].sum() for col in col_list}\n",
    "# Turn the sums into a DataFrame with one row with an index of 'Total':\n",
    "sum_df = pd.DataFrame(rsum, index=[\"Total\"])\n",
    "# Now append the row:\n",
    "statDF = statDF.append(sum_df)\n",
    " \n",
    "# Calculate Similarity of Unique Words\n",
    "tup_word = [] # need to pull column headers and rows af words\n",
    "sim_word = np.zeros((ncols, ncols))\n",
    " \n",
    "for i in xrange(ncols-1):\n",
    "     \n",
    "    v1 = [0.0]*ncols\n",
    "    v1 = term_doc.iloc[:, i+1]\n",
    "     \n",
    "    for k in xrange(ncols-1):\n",
    "         \n",
    "        v2 = [0.0]*ncols \n",
    "        if i >= k: pass\n",
    "        else:\n",
    "            v2 = term_doc.iloc[:, k+1]\n",
    "            similar = cosine_sim(v1, v2)\n",
    "            tup_w = (similar, list(columns)[i], list(columns)[k])\n",
    " \n",
    "            tup_word.append(tup_w)\n",
    "            sim_word[i,k] = similar\n",
    "            sim_word[k,i] = similar\n",
    "     \n",
    "    sim_word[i,i] = 1.0\n",
    " \n",
    "sim_word[ncols-1,ncols-1] = 1.0\n",
    " \n",
    "print 'Similarity for Words: Words = ', word_list_len\n",
    "print sim_word\n",
    " \n",
    "# SIMILARITY for TWEETS\n",
    "tu_tweet = []\n",
    "sim_tweet = np.zeros((num_tweets, num_tweets))\n",
    " \n",
    "for i in xrange(num_tweets):\n",
    "     \n",
    "    v1 = [0.0]*num_tweets\n",
    "    v1 = term_doc.iloc[i, 1:]\n",
    "     \n",
    "    for k in xrange(num_tweets):\n",
    "         \n",
    "        v2 = [0.0]*num_tweets\n",
    "        if i >= k: pass\n",
    "        else:\n",
    "            v2 = term_doc.iloc[k, 1:]\n",
    "            similar = cosine_sim(v1, v2)\n",
    "            tup_twe = (similar, term_doc['Tweet'][i], term_doc['Tweet'][k])\n",
    "            tu_tweet.append(tup_twe)\n",
    "             \n",
    "            sim_tweet[i, k] = similar\n",
    "            sim_tweet[k, i] = similar\n",
    "    sim_tweet[i,i] = 1.0\n",
    " \n",
    "print '                                                                                         '\n",
    "print \"Similarity for Tweets: Tweets = \", num_tweets\n",
    "print sim_tweet\n",
    " \n",
    "statDF.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Determine Top N TWEETS / WORDS\n",
    " \n",
    "K_neighbor(n_top, tweetDF['Tweet'][498], tu_tweet)  #Top tweets for a given tweet\n",
    " \n",
    "K_neighbor(n_top, 'accelerator', tup_word)  #Top words for a given word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def tweet_prep(df):\n",
    "     \n",
    "    tweet_list = df['Tweet'].tolist()\n",
    "    tweet_list_clean = df['Clean_Tweet'].tolist()\n",
    "    word_list_cl = [[word for word in str(line).split()] for line in tweet_list_clean]\n",
    "    word_list_tot = list(chain.from_iterable(word_list_cl))\n",
    "     \n",
    "    set_word = set(word_list_tot) # from clean tweets\n",
    "     \n",
    "    return Pair_words(set_word, tweet_list_clean, n_top)\n",
    " \n",
    "print \"Top \", n_top, \" pairs of words\"\n",
    " \n",
    "most_comm = Pair_words(word_list, tweet_clean_fin, n_top)\n",
    " \n",
    "print most_comm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.feature_extraction.text import HashingVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    " \n",
    "from sklearn.preprocessing import Normalizer\n",
    "from sklearn import metrics\n",
    " \n",
    "# K-Means Processing\n",
    "n_clst = 200\n",
    "cluster_doc = term_doc.drop(['Tweet'], axis=1)\n",
    " \n",
    "kmeans = KMeans(n_clusters=n_clst, init='k-means++', random_state=0, max_iter=100, n_init=10, verbose=True)\n",
    "print(\"Clustering sparse data with %s\" % kmeans)\n",
    " \n",
    "kmeans.fit(cluster_doc)\n",
    " \n",
    "cluster_num = kmeans.predict(cluster_doc)\n",
    "tweet_clean_list = [\" \".join(tweet) for tweet in tweet_clean_fin]\n",
    " \n",
    "labels = kmeans.labels_\n",
    "cluster_centers = kmeans.cluster_centers_\n",
    "labels_unique = np.unique(labels)\n",
    " \n",
    "lenlb = len(labels_unique)\n",
    "label_elem = np.zeros([lenlb])\n",
    " \n",
    "print len(cluster_num), len(term_doc), len(tweet_clean_list), len(tweet_clean_fin)\n",
    " \n",
    "cluster_tweet = pd.DataFrame({\"Tweet\": term_doc[\"Tweet\"], \"Cluster_Num\": cluster_num, \"Clean_Tweet\": tweet_clean_list})\n",
    "tweet_prep(cluster_tweet)\n",
    " \n",
    "cluster_top_pair = cluster_tweet.groupby(\"Cluster_Num\").apply(tweet_prep)\n",
    "elem_cluster = np.bincount(labels) # Number of elements per Cluster\n",
    "print \"Top Cluster Pair\"\n",
    "print cluster_top_pair\n",
    " \n",
    "for i in labels_unique:\n",
    "    label_elem[i] = 0\n",
    "     \n",
    "    for l in labels:\n",
    "        if l == i: label_elem[i] +=1\n",
    "    print \"Label = \", i, \"  Number of Elements = \", label_elem[i] \n",
    " \n",
    "samp_size = min(num_tweets, 300) \n",
    " \n",
    "silh_score = metrics.silhouette_score(cluster_doc, labels, metric='euclidean', sample_size=samp_size)\n",
    "print \"Silhouette score = \", round(silh_score, 3), \"  for Sample Size = \", samp_size\n",
    " \n",
    "cluster_arr = cluster_doc.as_matrix()\n",
    "BIC = compute_bic(kmeans,cluster_arr)\n",
    "print 'BIC Score = ', round(BIC, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Job Hunting (Python 3.9.0)",
   "language": "python",
   "name": "jh"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
