{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "fit_set = set(['sklearn.calibration.CalibratedClassifierCV', 'sklearn.cluster.AffinityPropagation', \n",
    "               'sklearn.cluster.AgglomerativeClustering', 'sklearn.cluster.Birch', \n",
    "               'sklearn.cluster.DBSCAN', 'sklearn.cluster.FeatureAgglomeration', \n",
    "               'sklearn.cluster.KMeans', 'sklearn.cluster.MeanShift', \n",
    "               'sklearn.cluster.MiniBatchKMeans', 'sklearn.cluster.OPTICS', \n",
    "               'sklearn.cluster.SpectralBiclustering', 'sklearn.cluster.SpectralClustering', \n",
    "               'sklearn.cluster.SpectralCoclustering', 'sklearn.compose.ColumnTransformer', \n",
    "               'sklearn.compose.TransformedTargetRegressor', 'sklearn.covariance.EllipticEnvelope', \n",
    "               'sklearn.covariance.EmpiricalCovariance', 'sklearn.covariance.GraphicalLasso', \n",
    "               'sklearn.covariance.GraphicalLassoCV', 'sklearn.covariance.LedoitWolf', \n",
    "               'sklearn.covariance.MinCovDet', 'sklearn.covariance.OAS', \n",
    "               'sklearn.covariance.ShrunkCovariance', 'sklearn.cross_decomposition.CCA', \n",
    "               'sklearn.cross_decomposition.PLSCanonical', 'sklearn.cross_decomposition.PLSRegression', \n",
    "               'sklearn.cross_decomposition.PLSSVD', 'sklearn.decomposition.DictionaryLearning', \n",
    "               'sklearn.decomposition.FactorAnalysis', 'sklearn.decomposition.FastICA', \n",
    "               'sklearn.decomposition.IncrementalPCA', 'sklearn.decomposition.KernelPCA', \n",
    "               'sklearn.decomposition.LatentDirichletAllocation', \n",
    "               'sklearn.decomposition.MiniBatchDictionaryLearning', \n",
    "               'sklearn.decomposition.MiniBatchSparsePCA', 'sklearn.decomposition.NMF', \n",
    "               'sklearn.decomposition.PCA', 'sklearn.decomposition.SparseCoder', \n",
    "               'sklearn.decomposition.SparsePCA', 'sklearn.decomposition.TruncatedSVD', \n",
    "               'sklearn.discriminant_analysis.LinearDiscriminantAnalysis', \n",
    "               'sklearn.discriminant_analysis.QuadraticDiscriminantAnalysis', \n",
    "               'sklearn.dummy.DummyClassifier', 'sklearn.dummy.DummyRegressor', \n",
    "               'sklearn.ensemble.AdaBoostClassifier', 'sklearn.ensemble.AdaBoostRegressor', \n",
    "               'sklearn.ensemble.BaggingClassifier', 'sklearn.ensemble.BaggingRegressor', \n",
    "               'sklearn.ensemble.ExtraTreesClassifier', 'sklearn.ensemble.ExtraTreesRegressor', \n",
    "               'sklearn.ensemble.GradientBoostingClassifier', \n",
    "               'sklearn.ensemble.GradientBoostingRegressor', \n",
    "               'sklearn.ensemble.HistGradientBoostingClassifier', \n",
    "               'sklearn.ensemble.HistGradientBoostingRegressor', 'sklearn.ensemble.IsolationForest', \n",
    "               'sklearn.ensemble.RandomForestClassifier', 'sklearn.ensemble.RandomForestRegressor', \n",
    "               'sklearn.ensemble.RandomTreesEmbedding', 'sklearn.ensemble.StackingClassifier', \n",
    "               'sklearn.ensemble.StackingRegressor', 'sklearn.ensemble.VotingClassifier', \n",
    "               'sklearn.ensemble.VotingRegressor', 'sklearn.feature_extraction.DictVectorizer', \n",
    "               'sklearn.feature_extraction.FeatureHasher', \n",
    "               'sklearn.feature_extraction.image.PatchExtractor', \n",
    "               'sklearn.feature_extraction.text.CountVectorizer', \n",
    "               'sklearn.feature_extraction.text.HashingVectorizer', \n",
    "               'sklearn.feature_extraction.text.TfidfTransformer', \n",
    "               'sklearn.feature_extraction.text.TfidfVectorizer', \n",
    "               'sklearn.feature_selection.GenericUnivariateSelect', 'sklearn.feature_selection.RFE', \n",
    "               'sklearn.feature_selection.RFECV', 'sklearn.feature_selection.SelectFdr', \n",
    "               'sklearn.feature_selection.SelectFpr', 'sklearn.feature_selection.SelectFromModel', \n",
    "               'sklearn.feature_selection.SelectFwe', 'sklearn.feature_selection.SelectKBest', \n",
    "               'sklearn.feature_selection.SelectPercentile', \n",
    "               'sklearn.feature_selection.VarianceThreshold', \n",
    "               'sklearn.gaussian_process.GaussianProcessClassifier', \n",
    "               'sklearn.gaussian_process.GaussianProcessRegressor', 'sklearn.impute.IterativeImputer', \n",
    "               'sklearn.impute.KNNImputer', 'sklearn.impute.MissingIndicator', \n",
    "               'sklearn.impute.SimpleImputer', 'sklearn.isotonic.IsotonicRegression', \n",
    "               'sklearn.kernel_approximation.AdditiveChi2Sampler', \n",
    "               'sklearn.kernel_approximation.Nystroem', 'sklearn.kernel_approximation.RBFSampler', \n",
    "               'sklearn.kernel_approximation.SkewedChi2Sampler', 'sklearn.kernel_ridge.KernelRidge', \n",
    "               'sklearn.linear_model.ARDRegression', 'sklearn.linear_model.BayesianRidge', \n",
    "               'sklearn.linear_model.ElasticNet', 'sklearn.linear_model.ElasticNetCV', \n",
    "               'sklearn.linear_model.GammaRegressor', 'sklearn.linear_model.HuberRegressor', \n",
    "               'sklearn.linear_model.Lars', 'sklearn.linear_model.LarsCV', \n",
    "               'sklearn.linear_model.Lasso', 'sklearn.linear_model.LassoCV', \n",
    "               'sklearn.linear_model.LassoLars', 'sklearn.linear_model.LassoLarsCV', \n",
    "               'sklearn.linear_model.LassoLarsIC', 'sklearn.linear_model.LinearRegression', \n",
    "               'sklearn.linear_model.LogisticRegression', 'sklearn.linear_model.LogisticRegressionCV', \n",
    "               'sklearn.linear_model.MultiTaskElasticNet', \n",
    "               'sklearn.linear_model.MultiTaskElasticNetCV', 'sklearn.linear_model.MultiTaskLasso', \n",
    "               'sklearn.linear_model.MultiTaskLassoCV', \n",
    "               'sklearn.linear_model.OrthogonalMatchingPursuit', \n",
    "               'sklearn.linear_model.OrthogonalMatchingPursuitCV', \n",
    "               'sklearn.linear_model.PassiveAggressiveClassifier', 'sklearn.linear_model.Perceptron', \n",
    "               'sklearn.linear_model.PoissonRegressor', 'sklearn.linear_model.RANSACRegressor', \n",
    "               'sklearn.linear_model.Ridge', 'sklearn.linear_model.RidgeClassifier', \n",
    "               'sklearn.linear_model.RidgeClassifierCV', 'sklearn.linear_model.RidgeCV', \n",
    "               'sklearn.linear_model.SGDClassifier', 'sklearn.linear_model.SGDRegressor', \n",
    "               'sklearn.linear_model.TheilSenRegressor', 'sklearn.linear_model.TweedieRegressor', \n",
    "               'sklearn.manifold.Isomap', 'sklearn.manifold.LocallyLinearEmbedding', \n",
    "               'sklearn.manifold.MDS', 'sklearn.manifold.SpectralEmbedding', 'sklearn.manifold.TSNE', \n",
    "               'sklearn.mixture.BayesianGaussianMixture', 'sklearn.mixture.GaussianMixture', \n",
    "               'sklearn.model_selection.GridSearchCV', 'sklearn.model_selection.RandomizedSearchCV', \n",
    "               'sklearn.multiclass.OneVsOneClassifier', 'sklearn.multiclass.OneVsRestClassifier', \n",
    "               'sklearn.multiclass.OutputCodeClassifier', 'sklearn.multioutput.ClassifierChain', \n",
    "               'sklearn.multioutput.MultiOutputClassifier', 'sklearn.multioutput.MultiOutputRegressor', \n",
    "               'sklearn.multioutput.RegressorChain', 'sklearn.naive_bayes.BernoulliNB', \n",
    "               'sklearn.naive_bayes.CategoricalNB', 'sklearn.naive_bayes.ComplementNB', \n",
    "               'sklearn.naive_bayes.GaussianNB', 'sklearn.naive_bayes.MultinomialNB', \n",
    "               'sklearn.neighbors.KernelDensity', 'sklearn.neighbors.KNeighborsClassifier', \n",
    "               'sklearn.neighbors.KNeighborsRegressor', 'sklearn.neighbors.KNeighborsTransformer', \n",
    "               'sklearn.neighbors.LocalOutlierFactor', 'sklearn.neighbors.NearestCentroid', \n",
    "               'sklearn.neighbors.NearestNeighbors', \n",
    "               'sklearn.neighbors.NeighborhoodComponentsAnalysis', \n",
    "               'sklearn.neighbors.RadiusNeighborsClassifier', \n",
    "               'sklearn.neighbors.RadiusNeighborsRegressor', \n",
    "               'sklearn.neighbors.RadiusNeighborsTransformer', 'sklearn.neural_network.BernoulliRBM', \n",
    "               'sklearn.neural_network.MLPClassifier', 'sklearn.neural_network.MLPRegressor', \n",
    "               'sklearn.pipeline.FeatureUnion', 'sklearn.pipeline.Pipeline', \n",
    "               'sklearn.preprocessing.Binarizer', 'sklearn.preprocessing.FunctionTransformer', \n",
    "               'sklearn.preprocessing.KBinsDiscretizer', 'sklearn.preprocessing.KernelCenterer', \n",
    "               'sklearn.preprocessing.LabelBinarizer', 'sklearn.preprocessing.LabelEncoder', \n",
    "               'sklearn.preprocessing.MaxAbsScaler', 'sklearn.preprocessing.MinMaxScaler', \n",
    "               'sklearn.preprocessing.MultiLabelBinarizer', 'sklearn.preprocessing.Normalizer', \n",
    "               'sklearn.preprocessing.OneHotEncoder', 'sklearn.preprocessing.OrdinalEncoder', \n",
    "               'sklearn.preprocessing.PolynomialFeatures', 'sklearn.preprocessing.PowerTransformer', \n",
    "               'sklearn.preprocessing.QuantileTransformer', 'sklearn.preprocessing.RobustScaler', \n",
    "               'sklearn.preprocessing.StandardScaler', \n",
    "               'sklearn.random_projection.GaussianRandomProjection', \n",
    "               'sklearn.random_projection.SparseRandomProjection', \n",
    "               'sklearn.semi_supervised.LabelPropagation', 'sklearn.semi_supervised.LabelSpreading', \n",
    "               'sklearn.svm.LinearSVC', 'sklearn.svm.LinearSVR', 'sklearn.svm.NuSVC', \n",
    "               'sklearn.svm.NuSVR', 'sklearn.svm.OneClassSVM', 'sklearn.svm.SVC', 'sklearn.svm.SVR', \n",
    "               'sklearn.tree.DecisionTreeClassifier', 'sklearn.tree.DecisionTreeRegressor', \n",
    "               'sklearn.tree.ExtraTreeClassifier', 'sklearn.tree.ExtraTreeRegressor'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "predict_proba_set = set(['sklearn.calibration.CalibratedClassifierCV', 'sklearn.discriminant_analysis.LinearDiscriminantAnalysis',\n",
    "                         'sklearn.discriminant_analysis.QuadraticDiscriminantAnalysis', 'sklearn.dummy.DummyClassifier',\n",
    "                         'sklearn.ensemble.AdaBoostClassifier', 'sklearn.ensemble.BaggingClassifier', 'sklearn.ensemble.ExtraTreesClassifier',\n",
    "                         'sklearn.ensemble.GradientBoostingClassifier', 'sklearn.ensemble.HistGradientBoostingClassifier',\n",
    "                         'sklearn.ensemble.RandomForestClassifier', 'sklearn.ensemble.StackingClassifier', 'sklearn.ensemble.VotingClassifier',\n",
    "                         'sklearn.feature_selection.RFE', 'sklearn.feature_selection.RFECV', 'sklearn.gaussian_process.GaussianProcessClassifier',\n",
    "                         'sklearn.linear_model.LogisticRegression', 'sklearn.linear_model.LogisticRegressionCV',\n",
    "                         'sklearn.linear_model.SGDClassifier', 'sklearn.mixture.BayesianGaussianMixture', 'sklearn.mixture.GaussianMixture',\n",
    "                         'sklearn.model_selection.GridSearchCV', 'sklearn.model_selection.RandomizedSearchCV',\n",
    "                         'sklearn.multiclass.OneVsRestClassifier', 'sklearn.multioutput.ClassifierChain',\n",
    "                         'sklearn.multioutput.MultiOutputClassifier', 'sklearn.naive_bayes.BernoulliNB', 'sklearn.naive_bayes.CategoricalNB',\n",
    "                         'sklearn.naive_bayes.ComplementNB', 'sklearn.naive_bayes.GaussianNB', 'sklearn.naive_bayes.MultinomialNB',\n",
    "                         'sklearn.neighbors.KNeighborsClassifier', 'sklearn.neighbors.RadiusNeighborsClassifier',\n",
    "                         'sklearn.neural_network.MLPClassifier', 'sklearn.pipeline.Pipeline', 'sklearn.semi_supervised.LabelPropagation',\n",
    "                         'sklearn.semi_supervised.LabelSpreading', 'sklearn.svm.NuSVC', 'sklearn.svm.SVC', 'sklearn.tree.DecisionTreeClassifier',\n",
    "                         'sklearn.tree.ExtraTreeClassifier'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "estimators_list_str = '''\n",
    "score_file_path = os.path.join(saves_folder, 'txt', 'new_estimators_scores.txt')\n",
    "os.makedirs(name=os.path.dirname(score_file_path), exist_ok=True)\n",
    "with open(score_file_path, 'w') as f:\n",
    "    print('', file=f)\n",
    "\n",
    "# ##################################################################\n",
    "# Define a pipeline combining a text feature extractor with a simple\n",
    "# classifier\n",
    "# ##################################################################\n",
    "estimators_list = [\n",
    "                   # done in xxxx\n",
    "                   # Best score: xxxx\n",
    "                   {}\n",
    "                   ]'''\n",
    "estimators_infix_str = ''',\n",
    "\n",
    "                   # done in xxxx\n",
    "                   # Best score: xxxx\n",
    "                   '''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "file_path = '../py/grid_search_params.py'\n",
    "with open(file_path, 'w') as f:\n",
    "    print(r\"\"\"\n",
    "#!/usr/bin/env python\n",
    "# coding: utf-8\n",
    "\n",
    "# From Anaconda Prompt (anaconda3), type:\n",
    "# cd C:\\Users\\dev\\Documents\\Repositories\\job-hunting\\py\n",
    "# conda activate jh\n",
    "# clear\n",
    "# python grid_search_params.py\n",
    "# conda deactivate\n",
    "\n",
    "from sklearn.experimental import enable_hist_gradient_boosting\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer\n",
    "\"\"\", file=f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "infix_str = '''\n",
    "from sklearn.utils import Bunch\n",
    "import logging\n",
    "import numpy as np\n",
    "import os\n",
    "import pandas as pd\n",
    "import pickle\n",
    "import re\n",
    "import sys\n",
    "import time\n",
    "\n",
    "\n",
    "\n",
    "# Display progress logs on stdout\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s %(levelname)s %(message)s')\n",
    "\n",
    "\n",
    "\n",
    "saves_folder = r'../saves/'\n",
    "saves_pickle_folder = os.path.join(saves_folder, 'pickle')\n",
    "os.makedirs(name=saves_pickle_folder, exist_ok=True)\n",
    "saves_csv_folder = os.path.join(saves_folder, 'csv')\n",
    "os.makedirs(name=saves_csv_folder, exist_ok=True)\n",
    "encoding_type = ['latin1', 'iso8859-1', 'utf-8'][2]\n",
    "\n",
    "\n",
    "\n",
    "def load_object(obj_name, download_url=None):\n",
    "    pickle_path = os.path.join(saves_pickle_folder, '{}.pickle'.format(obj_name))\n",
    "    if not os.path.isfile(pickle_path):\n",
    "        print('No pickle exists at {} - attempting to load as csv.'.format(os.path.abspath(pickle_path)))\n",
    "        csv_path = os.path.join(saves_csv_folder, '{}.csv'.format(obj_name))\n",
    "        if not os.path.isfile(csv_path):\n",
    "            print('No csv exists at {} - attempting to download from URL.'.format(os.path.abspath(csv_path)))\n",
    "            object = pd.read_csv(download_url, low_memory=False,\n",
    "                                 encoding=encoding_type)\n",
    "        else:\n",
    "            object = pd.read_csv(csv_path, low_memory=False,\n",
    "                                 encoding=encoding_type)\n",
    "        if isinstance(object, pd.DataFrame):\n",
    "            attempt_to_pickle(object, pickle_path, raise_exception=False)\n",
    "        else:\n",
    "            with open(pickle_path, 'wb') as handle:\n",
    "                \n",
    "                # Protocal 4 is not handled in python 2\n",
    "                if sys.version_info.major == 2:\n",
    "                    pickle.dump(object, handle, 2)\n",
    "                elif sys.version_info.major == 3:\n",
    "                    pickle.dump(object, handle, pickle.HIGHEST_PROTOCOL)\n",
    "    else:\n",
    "        try:\n",
    "            object = pd.read_pickle(pickle_path)\n",
    "        except:\n",
    "            with open(pickle_path, 'rb') as handle:\n",
    "                object = pickle.load(handle)\n",
    "    \n",
    "    return(object)\n",
    "\n",
    "\n",
    "\n",
    "def attempt_to_pickle(df, pickle_path, raise_exception=False, verbose=True):\n",
    "    try:\n",
    "        if verbose:\n",
    "            print('Pickling to {}'.format(os.path.abspath(pickle_path)))\n",
    "        \n",
    "        # Protocal 4 is not handled in python 2\n",
    "        if sys.version_info.major == 2:\n",
    "            df.to_pickle(pickle_path, protocol=2)\n",
    "        elif sys.version_info.major == 3:\n",
    "            df.to_pickle(pickle_path, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "    \n",
    "    except Exception as e:\n",
    "        os.remove(pickle_path)\n",
    "        if verbose:\n",
    "            print(e, \": Couldn't save {:,} cells as a pickle.\".format(df.shape[0]*df.shape[1]))\n",
    "        if raise_exception:\n",
    "            raise\n",
    "\n",
    "\n",
    "\n",
    "SCANNER_REGEX = re.compile(r'</?\\w+|\\w+[#\\+]*|:|\\.')\n",
    "def regex_tokenizer(corpus):\n",
    "    \n",
    "    return [match.group() for match in re.finditer(SCANNER_REGEX, corpus)]\n",
    "\n",
    "\n",
    "\n",
    "def get_value_str(best_parameters_dict, params_dict, param_name):\n",
    "    value = best_parameters_dict[param_name]\n",
    "    example_param_obj = params_dict[param_name][0]\n",
    "    if type(example_param_obj) == str:\n",
    "        value_str = f\"'{str(value)}'\"\n",
    "    elif hasattr(example_param_obj, '__call__'):\n",
    "        value_str = example_param_obj.__name__\n",
    "    else:\n",
    "        value_str = str(value)\n",
    "    \n",
    "    return value_str\n",
    "\n",
    "\n",
    "\n",
    "basic_tags_dict = load_object('basic_tags_dict')\n",
    "rows_list = [{'navigable_parent': navigable_parent,\n",
    "              'is_header': is_header} for navigable_parent, is_header in basic_tags_dict.items()]\n",
    "child_str_df = pd.DataFrame(rows_list)\n",
    "data = Bunch(data=child_str_df.navigable_parent.tolist(), target=child_str_df.is_header.to_numpy())\n",
    "\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from inspect import signature\n",
    "from sklearn.experimental import enable_hist_gradient_boosting\n",
    "\n",
    "file_path = '../py/grid_search_params.py'\n",
    "with open(file_path, 'a') as f:\n",
    "    print('', file=f)\n",
    "    method_name_list = []\n",
    "    for module_name in sorted(predict_proba_set.intersection(fit_set)):\n",
    "        method_name = module_name.split('.')[-1]\n",
    "        super_name = '.'.join(module_name.split('.')[:-1])\n",
    "        import_exec_str = f'from {super_name} import {method_name}'\n",
    "        try:\n",
    "            exec(import_exec_str)\n",
    "            print(import_exec_str, file=f)\n",
    "            method_name_list.append(method_name)\n",
    "        except Exception as e:\n",
    "            print(f'# The executed string \"{import_exec_str}\" gets this error: {str(e).strip()}', file=f)\n",
    "    print(infix_str, file=f)\n",
    "    print('', file=f)\n",
    "    print('clf_params_dict = {}', file=f)\n",
    "    function_str_list = []\n",
    "    for method_name in sorted(method_name_list):\n",
    "        sig_eval_str = f'signature({method_name})'\n",
    "        function_str = f'{method_name}('\n",
    "        signature_list = []\n",
    "        try:\n",
    "            sig = eval(sig_eval_str)\n",
    "            params_dict = dict(sig.parameters)\n",
    "            print(f\"clf_params_dict['{method_name}'] = {{\", file=f)\n",
    "            clf_str_list = []\n",
    "            for parameter_obj in params_dict.values():\n",
    "                name = parameter_obj.name\n",
    "                default = parameter_obj.default\n",
    "                kind = parameter_obj.kind\n",
    "                if default is None:\n",
    "                    clf_str_list.append(f\"        'clf__{name}': (None,),\")\n",
    "                    #signature_list.append(f'{name}=None')\n",
    "                elif str(default) == 'nan':\n",
    "                    clf_str_list.append(f\"        'clf__{name}': (np.nan,),\")\n",
    "                    #signature_list.append(f'{name}=np.nan')\n",
    "                elif type(default) is str:\n",
    "                    clf_str_list.append(f\"        'clf__{name}': ('{default}',),\")\n",
    "                    #signature_list.append(f\"{name}='{default}'\")\n",
    "                elif kind is parameter_obj.VAR_KEYWORD:\n",
    "                    pass\n",
    "                elif type(default) is type:\n",
    "                    #clf_str_list.append(f\"        'clf__{name}': ({name}_obj,),\")\n",
    "                    signature_list.append(f'{name}={name}_obj')\n",
    "                else:\n",
    "                    clf_str_list.append(f\"        'clf__{name}': ({default},),\")\n",
    "                    #signature_list.append(f'{name}={default}')\n",
    "            for clf_str in sorted(clf_str_list):\n",
    "                print(clf_str, file=f)\n",
    "            print('    }', file=f)\n",
    "            function_str += ', '.join(signature_list)\n",
    "            function_str += ')'\n",
    "            function_str_list.append(function_str)\n",
    "        except Exception as e:\n",
    "            print(f'        # The evaluated string \"{sig_eval_str}\" gets this error: {str(e).strip()}', file=f)\n",
    "            print('    }', file=f)\n",
    "    print(estimators_list_str.format(estimators_infix_str.join(function_str_list)), file=f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "file_path = '../py/grid_search_params.py'\n",
    "with open(file_path, 'a') as f:\n",
    "    print('''\n",
    "for estimator in estimators_list:\n",
    "    steps_list = [\n",
    "        ('vect', CountVectorizer()),\n",
    "        ('tfidf', TfidfTransformer()),\n",
    "        ('clf', estimator),\n",
    "    ]\n",
    "    pipeline = Pipeline(steps_list)\n",
    "    clf_name = str(estimator.__class__).split('.')[-1].split(\"'\")[0]\n",
    "    params_dict = clf_params_dict[clf_name]\n",
    "    params_dict.update({\n",
    "        'tfidf__norm': ('l1',),\n",
    "        'tfidf__smooth_idf': (True,),\n",
    "        'tfidf__sublinear_tf': (False,),\n",
    "        'tfidf__use_idf': (True,),\n",
    "        'vect__analyzer': ('word',),\n",
    "        'vect__binary': (False,),\n",
    "        'vect__decode_error': ('strict',),\n",
    "        'vect__lowercase': (False,),\n",
    "        'vect__max_df': (1.0,),\n",
    "        'vect__max_features': (None,),\n",
    "        'vect__min_df': (0.0,),\n",
    "        'vect__ngram_range': ((1, 5),),\n",
    "        'vect__stop_words': (None,),\n",
    "        'vect__strip_accents': ('ascii',),\n",
    "        'vect__tokenizer': (regex_tokenizer,),\n",
    "    })\n",
    "    \n",
    "    \n",
    "    \n",
    "    # Find the best parameters for both the feature extraction and the\n",
    "    # classifier\n",
    "    grid_search = GridSearchCV(pipeline, params_dict, n_jobs=-1, verbose=1, scoring='f1')\n",
    "    \n",
    "    # Add inital pipeline parameters\n",
    "    with open(score_file_path, 'a') as f:\n",
    "        print('', file=f)\n",
    "        print('_'*len(clf_name), file=f)\n",
    "        print(clf_name, file=f)\n",
    "        print('^'*len(clf_name), file=f)\n",
    "        print(\"Performing grid search...\", file=f)\n",
    "        print(\"pipeline:\", [name for name, _ in pipeline.steps], file=f)\n",
    "        print(\"parameters:\", file=f)\n",
    "        print('{', file=f)\n",
    "        for key, value in params_dict.items():\n",
    "            print(f\"\"\"        '{key}': {str(value)},\"\"\", file=f)\n",
    "        print('}', file=f)\n",
    "    \n",
    "    # Add scores\n",
    "    t0 = time.time()\n",
    "    print()\n",
    "    print(time.ctime(t0))\n",
    "    print(clf_name)\n",
    "    grid_search.fit(data.data, data.target)\n",
    "    with open(score_file_path, 'a') as f:\n",
    "        print(\"done in %0.3fs\" % (time.time() - t0), file=f)\n",
    "        print('', file=f)\n",
    "        print(\"Best %s score: %0.3f\" % clf_name, grid_search.best_score_, file=f)\n",
    "        print(\"Best parameters set:\", file=f)\n",
    "    best_parameters_dict = grid_search.best_estimator_.get_params()\n",
    "    for param_name in sorted(params_dict.keys()):\n",
    "        with open(score_file_path, 'a') as f:\n",
    "            #print(\"    %s: %r\" % (param_name, best_parameters_dict[param_name]), file=f)\n",
    "            value_str = get_value_str(best_parameters_dict, params_dict, param_name)\n",
    "            print(f\"        '{param_name}': ({value_str},),\", file=f)\n",
    "    with open(score_file_path, 'a') as f:\n",
    "        print('', file=f)\n",
    "    \n",
    "    # Add function signatures\n",
    "    for step_tuple in steps_list:\n",
    "        step_prefix = step_tuple[0]\n",
    "        step_name = str(step_tuple[1].__class__).split('.')[-1].split(\"'\")[0]\n",
    "        function_str = f'{step_name}('\n",
    "        signature_list = []\n",
    "        for param_name in sorted(params_dict.keys()):\n",
    "            if param_name.startswith(step_prefix) and (param_name in best_parameters_dict):\n",
    "                value_str = get_value_str(best_parameters_dict, params_dict, param_name)\n",
    "                signature_list.append(f'{param_name[len(step_prefix)+2:]}={value_str}')\n",
    "        function_str += ', '.join(signature_list)\n",
    "        function_str += '),'\n",
    "        with open(score_file_path, 'a') as f:\n",
    "            print(function_str, file=f)''', file=f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Job Hunting (Python 3.9.0)",
   "language": "python",
   "name": "jh"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
