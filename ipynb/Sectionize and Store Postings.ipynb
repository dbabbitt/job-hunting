{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "---\n",
    "# Load needed libraries and functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pretty printing has been turned OFF\n"
     ]
    }
   ],
   "source": [
    "\n",
    "%pprint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pickling to C:\\Users\\dev\\Documents\\Repositories\\job-hunting\\saves\\pickle\\CHILD_STR_CLF.pickle\n",
      "Wall time: 32.8 s\n"
     ]
    }
   ],
   "source": [
    "\n",
    "%%time\n",
    "%run ../py/html_analysis.py\n",
    "hc = HeaderCategories()\n",
    "ha = HeaderAnalysis()\n",
    "ea = ElementAnalysis()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def get_pos_and_consecutives(file_name):\n",
    "    child_strs_list = ha.get_child_strs_from_file(file_name)\n",
    "    child_tags_list = ha.get_child_tags_list(child_strs_list)\n",
    "    is_header_list = ha.get_is_header_list(child_strs_list)\n",
    "    feature_dict_list = hc.get_feature_dict_list(child_tags_list, is_header_list, child_strs_list)\n",
    "    feature_tuple_list = [hc.get_feature_tuple(feature_dict) for feature_dict in feature_dict_list]\n",
    "    crf_list = ea.CRF.predict_single(ea.sent2features(feature_tuple_list))\n",
    "    pos_list = []\n",
    "    for pos, feature_tuple in zip(crf_list, feature_tuple_list):\n",
    "        navigable_parent = feature_tuple[1]\n",
    "        if navigable_parent in ha.NAVIGABLE_PARENT_IS_HEADER_DICT:\n",
    "            pos_list = hc.append_parts_of_speech_list(navigable_parent, pos_list)\n",
    "        else:\n",
    "            pos_list.append(pos)\n",
    "    consecutives_list = []\n",
    "    for pos, v in groupby(pos_list):\n",
    "        consecutives_count = len(list(v))\n",
    "        consecutives_tuple = (pos, consecutives_count)\n",
    "        consecutives_list.append(consecutives_tuple)\n",
    "    \n",
    "    return consecutives_list, pos_list, child_strs_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "TITLE_SEPARATOR = ' - '\n",
    "PARENTH_REGEX = re.compile(r'[\\(\\[][^\\)\\]]+[\\)\\]]')\n",
    "def get_posting_location(file_name):\n",
    "    title_str = ' '.join(file_name.split('_')[:-1])\n",
    "    title_str = re.sub(' - *| *- ', TITLE_SEPARATOR, title_str)\n",
    "    matched_text_list = []\n",
    "    for match_obj in PARENTH_REGEX.finditer(title_str):\n",
    "        matched_text = match_obj.group()\n",
    "        matched_text_list.append(matched_text)\n",
    "    for matched_text in matched_text_list:\n",
    "        replacement_text = matched_text.replace(TITLE_SEPARATOR, '-')\n",
    "        replacement_text = replacement_text[1:-1].strip()\n",
    "        replacement_text = replacement_text.replace('. ', TITLE_SEPARATOR).replace(' + ', TITLE_SEPARATOR)\n",
    "        replacement_text = TITLE_SEPARATOR + replacement_text + TITLE_SEPARATOR\n",
    "        title_str = title_str.replace(matched_text, replacement_text)\n",
    "    posting_title_list = title_str.split(TITLE_SEPARATOR)\n",
    "    posting_title_list = [posting_title_str.strip() for posting_title_str in posting_title_list]\n",
    "    posting_title_list = list(filter(len, posting_title_list))\n",
    "    posting_location = posting_title_list.pop()\n",
    "    if posting_location == 'Indeed.com':\n",
    "        posting_location = posting_title_list.pop()\n",
    "    \n",
    "    return posting_location, posting_title_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "REMOTE_STRS_LIST = ['remote', 'telecommute', 'home-based', 'wfh', 'work from home']\n",
    "QUAL_STRS_LIST = ['citizen', 'clearance', 'usc', 'gc', 'ead']\n",
    "US_STRS_LIST = ['united states', 'us', 'usa', 'us citizens only']\n",
    "PAY_STRS_LIST = ['b2b', 'up to $']\n",
    "DURATION_STRS_LIST = ['direct hire']\n",
    "def create_deatails_lists(posting_title_list):\n",
    "    LOCATION_DETAILS_LIST = []\n",
    "    REQ_QUALS_LIST = []\n",
    "    job_title_list = []\n",
    "    PAY_DETAILS_LIST = []\n",
    "    duration_details = []\n",
    "    while len(posting_title_list):\n",
    "        posting_title_str = posting_title_list.pop()\n",
    "        if any(map(lambda x: x in posting_title_str.lower(), REMOTE_STRS_LIST)):\n",
    "            LOCATION_DETAILS_LIST.append(posting_title_str)\n",
    "        elif any(map(lambda x: re.search(f'\\\\b{x}\\\\b', posting_title_str.lower()), QUAL_STRS_LIST)):\n",
    "            REQ_QUALS_LIST.append(posting_title_str)\n",
    "        elif any(map(lambda x: x == posting_title_str.lower(), US_STRS_LIST)):\n",
    "            LOCATION_DETAILS_LIST.append(posting_title_str)\n",
    "        elif any(map(lambda x: x in posting_title_str.lower(), PAY_STRS_LIST)):\n",
    "            PAY_DETAILS_LIST.append(posting_title_str)\n",
    "        elif any(map(lambda x: x in posting_title_str.lower(), DURATION_STRS_LIST)):\n",
    "            duration_details.append(posting_title_str)\n",
    "        else:\n",
    "            posting_title_str = posting_title_str.strip()\n",
    "            if posting_title_str.startswith(','):\n",
    "                posting_title_str = posting_title_str[1:].strip()\n",
    "            job_title_list.append(posting_title_str)\n",
    "    \n",
    "    return LOCATION_DETAILS_LIST, REQ_QUALS_LIST, PAY_DETAILS_LIST, duration_details, job_title_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "COMMAS_REGEX = re.compile(r', ([^,]+),')\n",
    "REPLACEMENT_STR = f'{TITLE_SEPARATOR}\\\\1{TITLE_SEPARATOR}'\n",
    "def extract_job_title_info(file_name):\n",
    "    posting_location, posting_title_list = get_posting_location(file_name)\n",
    "    commas_list = []\n",
    "    while len(posting_title_list):\n",
    "        posting_title_str = posting_title_list.pop()\n",
    "        posting_title_str = re.sub(COMMAS_REGEX, REPLACEMENT_STR, posting_title_str)\n",
    "        if any(map(lambda x: x in posting_title_str.lower(), REMOTE_STRS_LIST)):\n",
    "            posting_title_str = re.sub(f\"-({'|'.join(REMOTE_STRS_LIST)})\", f'{TITLE_SEPARATOR}\\\\1', posting_title_str, 0, re.IGNORECASE)\n",
    "        commas_list.append(posting_title_str)\n",
    "    title_str = TITLE_SEPARATOR.join(commas_list)\n",
    "    posting_title_list = title_str.split(TITLE_SEPARATOR)\n",
    "    assert len(posting_title_list), f'{title_str}: {title_str.split(TITLE_SEPARATOR)}'\n",
    "    LOCATION_DETAILS_LIST, REQ_QUALS_LIST, PAY_DETAILS_LIST, duration_details, job_title_list = create_deatails_lists(posting_title_list)\n",
    "    \n",
    "    return posting_location, LOCATION_DETAILS_LIST, REQ_QUALS_LIST, PAY_DETAILS_LIST, duration_details, job_title_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def get_section_list(pos):\n",
    "    if pos == 'H-JD':\n",
    "        return JOB_DURATION_LIST\n",
    "    elif pos == 'H-ER':\n",
    "        return EDUCATION_REQUIREMENTS_LIST\n",
    "    elif pos == 'H-SP':\n",
    "        return PAY_DETAILS_LIST\n",
    "    elif pos == 'H-PQ':\n",
    "        return PREFF_QUALS_LIST\n",
    "    elif pos == 'H-TS':\n",
    "        return TASK_SCOPE_LIST\n",
    "    elif pos == 'H-OL':\n",
    "        return LOCATION_DETAILS_LIST\n",
    "    elif pos == 'H-IP':\n",
    "        return INTERVIEW_DETAILS_LIST\n",
    "    elif pos == 'H-LN':\n",
    "        return LEGAL_NOTIFS_LIST\n",
    "    elif pos == 'H-RQ':\n",
    "        return REQ_QUALS_LIST\n",
    "    elif pos == 'H-CS':\n",
    "        return CORP_SCOPE_LIST\n",
    "    elif pos == 'H-O':\n",
    "        return OTHER_LIST\n",
    "    \n",
    "    return []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import random\n",
    "\n",
    "files_list = os.listdir(ha.SAVES_HTML_FOLDER)\n",
    "file_name = random.choice(files_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "EDUCATION_REQUIREMENTS_LIST = []\n",
    "PREFF_QUALS_LIST = []\n",
    "TASK_SCOPE_LIST = []\n",
    "INTERVIEW_DETAILS_LIST = []\n",
    "LEGAL_NOTIFS_LIST = []\n",
    "CORP_SCOPE_LIST = []\n",
    "OTHER_LIST = []\n",
    "posting_location, LOCATION_DETAILS_LIST, REQ_QUALS_LIST, PAY_DETAILS_LIST, JOB_DURATION_LIST, job_title_list = extract_job_title_info(file_name)\n",
    "def get_pos_list(file_name):\n",
    "    child_strs_list = ha.get_child_strs_from_file(file_name)\n",
    "    child_tags_list = ha.get_child_tags_list(child_strs_list)\n",
    "    is_header_list = ha.get_is_header_list(child_strs_list)\n",
    "    feature_dict_list = hc.get_feature_dict_list(child_tags_list, is_header_list, child_strs_list)\n",
    "    feature_tuple_list = [hc.get_feature_tuple(feature_dict) for feature_dict in feature_dict_list]\n",
    "    crf_list = ea.CRF.predict_single(ea.sent2features(feature_tuple_list))\n",
    "    pos_list = []\n",
    "    for pos, feature_tuple in zip(crf_list, feature_tuple_list):\n",
    "        navigable_parent = feature_tuple[1]\n",
    "        if navigable_parent in ha.NAVIGABLE_PARENT_IS_HEADER_DICT:\n",
    "            pos_list = hc.append_parts_of_speech_list(navigable_parent, pos_list)\n",
    "        else:\n",
    "            pos_list.append(pos)\n",
    "    \n",
    "    return pos_list, child_strs_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('H-TS', 1), ('O', 17), ('H-LN', 1), ('O', 20), ('H-LN', 1), ('O', 1)]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from itertools import groupby\n",
    "\n",
    "#for file_name in files_list:\n",
    "pos_list, child_strs_list = get_pos_list(file_name)\n",
    "consecutives_list = []\n",
    "for key_pos, consecutive_value in groupby(pos_list):\n",
    "    value_list = list(consecutive_value)\n",
    "    consecutives_count = len(value_list)\n",
    "    consecutives_tuple = (key_pos, consecutives_count)\n",
    "    consecutives_list.append(consecutives_tuple)\n",
    "for pos in list(set(pos_list) - set(['O'])):\n",
    "    section_list = get_section_list(pos)\n",
    "    speech_parts_idx_list = ea.get_idx_list(pos_list, pos)\n",
    "    consecutives_idx_list = ea.get_idx_list(consecutives_list, (pos, 1))\n",
    "    for speech_part_idx, consecutives_idx in zip(speech_parts_idx_list, consecutives_idx_list):\n",
    "        if consecutives_idx+1 < len(consecutives_list):\n",
    "            o_count = consecutives_list[consecutives_idx+1][1]\n",
    "            if pos == 'H-RQ':\n",
    "                o_list = []\n",
    "                for child_str in child_strs_list[speech_part_idx:speech_part_idx+o_count+1]:\n",
    "                    if child_str not in ha.NAVIGABLE_PARENT_IS_QUAL_DICT:\n",
    "                        o_list.append(child_str)\n",
    "                    elif ha.NAVIGABLE_PARENT_IS_QUAL_DICT[child_str]:\n",
    "                        o_list.append(child_str)\n",
    "            else:\n",
    "                 o_list = child_strs_list[speech_part_idx:speech_part_idx+o_count+1]\n",
    "            section_list.extend(o_list)\n",
    "print(consecutives_list)\n",
    "#break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "H-LN Legal Notifications Header [18, 39]\n",
      "H-TS Task Scope Header [0]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "for pos in list(set(pos_list) - set(['O'])):\n",
    "    print(pos, hc.POS_EXPLANATION_DICT[pos], ea.get_idx_list(pos_list, pos))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "ZMQInteractiveShell_obj = get_ipython()\n",
    "NAVIGABLE_PARENT_IS_QUAL_DICT = s.load_object('NAVIGABLE_PARENT_IS_QUAL_DICT')\n",
    "def get_pos_code():\n",
    "    output_str = ''\n",
    "    for pos in list(set(pos_list) - set(['O'])):\n",
    "        output_str += '\\n'\n",
    "        output_str += '# '\n",
    "        output_str += pos\n",
    "        output_str += ' '\n",
    "        output_str += hc.POS_EXPLANATION_DICT[pos]\n",
    "        output_str += '\\n'\n",
    "        section_list = get_section_list(pos)\n",
    "        output_str += 'for tag_str in [\\n'\n",
    "        for child_str in section_list:\n",
    "            if \"'\" in child_str:\n",
    "                output_str += f'        \"{child_str}\",\\n'\n",
    "            else:\n",
    "                output_str += f\"        '{child_str}',\\n\"\n",
    "        output_str += '    ]:\\n'\n",
    "        if pos in ['H-RQ', 'H-PQ']:\n",
    "            output_str += '    NAVIGABLE_PARENT_IS_QUAL_DICT[tag_str] = True\\n'\n",
    "        else:\n",
    "            output_str += '    NAVIGABLE_PARENT_IS_QUAL_DICT[tag_str] = False\\n'\n",
    "        output_str += '''print(len(NAVIGABLE_PARENT_IS_QUAL_DICT.keys()))\n",
    "s.store_objects(NAVIGABLE_PARENT_IS_QUAL_DICT=NAVIGABLE_PARENT_IS_QUAL_DICT)\\n'''\n",
    "    \n",
    "    return output_str"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "---\n",
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "ZMQInteractiveShell_obj.set_next_input(text=get_pos_code(), replace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# H-LN Legal Notifications Header\n",
    "for tag_str in [\n",
    "        '<b>Desirable Skills</b>',\n",
    "        '<li>Demonstrated ability and problem solving skills to develop innovative machine learning applications</li>',\n",
    "        '<b>About AIS</b>',\n",
    "        '<b>AIS, Dedicated to Our People</b>',\n",
    "        \"AIS employees can spend their entire career at AIS doing challenging, rewarding work and reach their desired level of achievement and responsibility. We offer the opportunity to move up, without the obligation to move out of a position where one excels. We are committed to our employee's success; however, they define it.\",\n",
    "        \"It's our dedication to our employees that inspired our leadership to invest in our future and become partially employee-owned through an Employee Stock Ownership Program (ESOP).\",\n",
    "        'Our employees are our greatest strength, and we do all that we can to serve them. We invest in technology as early adopters, allowing us to create transformative and innovative solutions for our customers while exposing our team to cutting edge technology.',\n",
    "        'We hire outstanding individuals who are committed to curiosity, passionate about emerging technology, and who are excited to find innovative solutions for the biggest tech challenges facing international brands and government agencies today.',\n",
    "        '<b>We Invest in Individuals Committed to Innovation</b>',\n",
    "        'AIS is seeking professionals of a certain character and level of excellence. People that we can learn from and that we can help grow to achieve their personal career goals. We are looking for:',\n",
    "        '<li>Smart people with a passion for technology</li>',\n",
    "        '<li>Strong technical capabilities with a consultancy mindset</li>',\n",
    "        '<li>Close involvement with local technical communities</li>',\n",
    "        '<li>A willingness to think outside of the box to provide innovative solutions to clients</li>',\n",
    "        '<li>Ability to solve challenging technical business problems</li>',\n",
    "        '<li>Self-directed professionals</li>',\n",
    "        '<b>Our Core Values</b>',\n",
    "        '<li>Client Success</li>',\n",
    "        '<li>Continued Learning and Technical Excellence</li>',\n",
    "        '<li>Strong Client Relationships</li>',\n",
    "        '<li>Citizenship and Community</li>',\n",
    "        '<b>AIS is an Equal Opportunity Employer</b>',\n",
    "        'Applied Information Sciences is an Equal Opportunity Employer and does not discriminate on the basis of race, national origin, religion, color, gender, sexual orientation, age, disability, protected veteran status or any other basis covered by law. Employment decisions are based solely on qualifications merit, and business need.',\n",
    "    ]:\n",
    "    NAVIGABLE_PARENT_IS_QUAL_DICT[tag_str] = False\n",
    "print(len(NAVIGABLE_PARENT_IS_QUAL_DICT.keys()))\n",
    "s.store_objects(NAVIGABLE_PARENT_IS_QUAL_DICT=NAVIGABLE_PARENT_IS_QUAL_DICT)\n",
    "\n",
    "# H-TS Task Scope Header\n",
    "for tag_str in [\n",
    "        '<b>Now Hiring an AI/ML Engineer</b>',\n",
    "        'As an',\n",
    "        '<b>AI/ML Engineer,</b>',\n",
    "        'you will get hands on with translating the vision from solution architects and work with data scientists, date engineers, loT specialists, and software developers to build complete end-to-end solutions. This is a remote position.',\n",
    "        '<li>Knowledge and Experience in designing and implementing AI applications using Microsoft Azure Cognitive Services, Azure Bot Service and Azure Machine Learning</li>',\n",
    "        '<li>Analyzing requirements for AI solutions</li>',\n",
    "        '<li>Experience recommending the appropriate tools and technologies for AI solutions</li>',\n",
    "        '<li>Experience in designing and implementing AI solutions that meet scalability and performance requirements</li>',\n",
    "        '<li>Understanding when a custom model API should be developed to meet specific requirements</li>',\n",
    "        '<li>Experience in understanding of programming language such as Python</li>',\n",
    "        '<li>Ability to work autonomously and collaboratively as part of a team to both teach and learn every day</li>',\n",
    "        '<li>Continuously looking for opportunities to learn, build skills and share learning.</li>',\n",
    "        '<li>Excellent written and verbal communication skills</li>',\n",
    "        '<li>Must have experience in application development</li>',\n",
    "        '<b>Profile of Success</b>',\n",
    "        '<li>Understanding of ML frameworks</li>',\n",
    "        '<li>Proven experience as a Machine Learning Engineer or similar role</li>',\n",
    "        '<li>Experience with setting up DevOps for Machine Learning to enable data science teams and IT teams to collaborate</li>',\n",
    "    ]:\n",
    "    NAVIGABLE_PARENT_IS_QUAL_DICT[tag_str] = False\n",
    "print(len(NAVIGABLE_PARENT_IS_QUAL_DICT.keys()))\n",
    "s.store_objects(NAVIGABLE_PARENT_IS_QUAL_DICT=NAVIGABLE_PARENT_IS_QUAL_DICT)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "---\n",
    "# Inference"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "----\n",
    "# Create the Header Pattern Dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "child_strs_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "child_str = '<b>About AIS</b>'\n",
    "child_str in ha.NAVIGABLE_PARENT_IS_HEADER_DICT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pickling to C:\\Users\\dev\\Documents\\Repositories\\job-hunting\\saves\\pickle\\NAVIGABLE_PARENT_IS_HEADER_DICT.pickle\n"
     ]
    }
   ],
   "source": [
    "\n",
    "ha.NAVIGABLE_PARENT_IS_HEADER_DICT[child_str] = True\n",
    "s.store_objects(NAVIGABLE_PARENT_IS_HEADER_DICT=ha.NAVIGABLE_PARENT_IS_HEADER_DICT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "child_str in hc.TASK_SCOPE_HEADERS_LIST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pickling to C:\\Users\\dev\\Documents\\Repositories\\job-hunting\\saves\\pickle\\TASK_SCOPE_HEADERS_LIST.pickle\n"
     ]
    }
   ],
   "source": [
    "\n",
    "hc.TASK_SCOPE_HEADERS_LIST.append(child_str)\n",
    "s.store_objects(TASK_SCOPE_HEADERS_LIST=hc.TASK_SCOPE_HEADERS_LIST)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pickling to C:\\Users\\dev\\Documents\\Repositories\\job-hunting\\saves\\pickle\\HEADER_PATTERN_DICT.pickle\n",
      "Pickling to C:\\Users\\dev\\Documents\\Repositories\\job-hunting\\saves\\pickle\\HEADER_PATTERN_DICT.pickle\n",
      "Pickling to C:\\Users\\dev\\Documents\\Repositories\\job-hunting\\saves\\pickle\\HEADER_PATTERN_DICT.pickle\n",
      "Pickling to C:\\Users\\dev\\Documents\\Repositories\\job-hunting\\saves\\pickle\\HEADER_PATTERN_DICT.pickle\n",
      "Pickling to C:\\Users\\dev\\Documents\\Repositories\\job-hunting\\saves\\pickle\\HEADER_PATTERN_DICT.pickle\n",
      "Pickling to C:\\Users\\dev\\Documents\\Repositories\\job-hunting\\saves\\pickle\\HEADER_PATTERN_DICT.pickle\n",
      "Pickling to C:\\Users\\dev\\Documents\\Repositories\\job-hunting\\saves\\pickle\\HEADER_PATTERN_DICT.pickle\n",
      "Pickling to C:\\Users\\dev\\Documents\\Repositories\\job-hunting\\saves\\pickle\\HEADER_PATTERN_DICT.pickle\n",
      "Pickling to C:\\Users\\dev\\Documents\\Repositories\\job-hunting\\saves\\pickle\\HEADER_PATTERN_DICT.pickle\n",
      "Pickling to C:\\Users\\dev\\Documents\\Repositories\\job-hunting\\saves\\pickle\\HEADER_PATTERN_DICT.pickle\n",
      "Pickling to C:\\Users\\dev\\Documents\\Repositories\\job-hunting\\saves\\pickle\\HEADER_PATTERN_DICT.pickle\n",
      "Pickling to C:\\Users\\dev\\Documents\\Repositories\\job-hunting\\saves\\pickle\\HEADER_PATTERN_DICT.pickle\n",
      "Pickling to C:\\Users\\dev\\Documents\\Repositories\\job-hunting\\saves\\pickle\\HEADER_PATTERN_DICT.pickle\n",
      "Pickling to C:\\Users\\dev\\Documents\\Repositories\\job-hunting\\saves\\pickle\\HEADER_PATTERN_DICT.pickle\n",
      "Pickling to C:\\Users\\dev\\Documents\\Repositories\\job-hunting\\saves\\pickle\\HEADER_PATTERN_DICT.pickle\n",
      "Pickling to C:\\Users\\dev\\Documents\\Repositories\\job-hunting\\saves\\pickle\\HEADER_PATTERN_DICT.pickle\n",
      "Pickling to C:\\Users\\dev\\Documents\\Repositories\\job-hunting\\saves\\pickle\\HEADER_PATTERN_DICT.pickle\n",
      "Pickling to C:\\Users\\dev\\Documents\\Repositories\\job-hunting\\saves\\pickle\\HEADER_PATTERN_DICT.pickle\n",
      "Pickling to C:\\Users\\dev\\Documents\\Repositories\\job-hunting\\saves\\pickle\\HEADER_PATTERN_DICT.pickle\n",
      "Pickling to C:\\Users\\dev\\Documents\\Repositories\\job-hunting\\saves\\pickle\\HEADER_PATTERN_DICT.pickle\n"
     ]
    }
   ],
   "source": [
    "\n",
    "files_list = os.listdir(ha.SAVES_HTML_FOLDER)\n",
    "HEADER_PATTERN_DICT = {}\n",
    "for file_name in files_list:\n",
    "    if file_name in ea.CHILD_STRS_LIST_DICT:\n",
    "        child_strs_list = ea.CHILD_STRS_LIST_DICT[file_name]\n",
    "    else:\n",
    "        file_path = os.path.join(ha.SAVES_HTML_FOLDER, file_name)\n",
    "        with open(file_path, 'r', encoding='utf-8') as f:\n",
    "            html_str = f.read()\n",
    "            body_soup = ha.get_body_soup(html_str)\n",
    "            child_strs_list = ha.get_navigable_children(body_soup, [])\n",
    "    if not len(child_strs_list):\n",
    "            file_path = os.path.join(ha.SAVES_HTML_FOLDER, file_name)\n",
    "            os.remove(file_path)\n",
    "            continue\n",
    "    navigable_parent = child_strs_list[0]\n",
    "    if navigable_parent not in ha.NAVIGABLE_PARENT_IS_HEADER_DICT:\n",
    "        continue\n",
    "    child_tags_list = []\n",
    "    is_header_list = []\n",
    "    for navigable_parent in child_strs_list:\n",
    "        if navigable_parent not in ha.NAVIGABLE_PARENT_IS_HEADER_DICT:\n",
    "            break\n",
    "        tokenized_sent = ha.html_regex_tokenizer(navigable_parent)\n",
    "        try:\n",
    "            first_token = tokenized_sent[0]\n",
    "            if first_token[0] == '<':\n",
    "                child_tags_list.append(first_token[1:])\n",
    "            else:\n",
    "                child_tags_list.append('plaintext')\n",
    "        except:\n",
    "            child_tags_list.append('plaintext')\n",
    "        is_header = ha.NAVIGABLE_PARENT_IS_HEADER_DICT[navigable_parent]\n",
    "        is_header_list.append(is_header)\n",
    "    if len(child_tags_list) == len(child_strs_list):\n",
    "        if file_name not in ea.CHILD_STRS_LIST_DICT:\n",
    "            ea.CHILD_STRS_LIST_DICT[file_name] = child_strs_list\n",
    "            s.store_objects(CHILD_STRS_LIST_DICT=ea.CHILD_STRS_LIST_DICT)\n",
    "        if file_name not in HEADER_PATTERN_DICT:\n",
    "            item_sequence = hc.get_feature_dict_list(child_tags_list, is_header_list, child_strs_list)\n",
    "            HEADER_PATTERN_DICT[file_name] = item_sequence\n",
    "            s.store_objects(HEADER_PATTERN_DICT=HEADER_PATTERN_DICT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Job Hunting (Python 3.9.0)",
   "language": "python",
   "name": "jh"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
