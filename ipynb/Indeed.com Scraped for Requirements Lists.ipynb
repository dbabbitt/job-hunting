{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pretty printing has been turned OFF\n"
     ]
    }
   ],
   "source": [
    "\n",
    "%pprint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from bs4.element import NavigableString\n",
    "import random\n",
    "import re\n",
    "import numpy as np\n",
    "import os\n",
    "from IPython.display import clear_output\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from urllib.parse import urlparse, parse_qs\n",
    "\n",
    "%run ../load_magic/storage.py\n",
    "s = Storage()\n",
    "\n",
    "lt_regex = re.compile(r'\\s+<')\n",
    "gt_regex = re.compile(r'>\\s+')\n",
    "saves_html_folder = os.path.join(s.saves_folder, 'html')\n",
    "basic_tags_dict = s.load_object('basic_tags_dict')\n",
    "\n",
    "def clean_html_str(html_obj):\n",
    "    html_str = str(html_obj)\n",
    "    html_str = html_str.strip()\n",
    "    html_str = lt_regex.sub('<', html_str)\n",
    "    html_str = gt_regex.sub('>', html_str)\n",
    "    \n",
    "    return html_str\n",
    "\n",
    "def get_navigable_children(tag, result_list=[]):\n",
    "    if type(tag) is not NavigableString:\n",
    "        for child_tag in tag.children:\n",
    "            result_list = get_navigable_children(child_tag, result_list)\n",
    "    else:\n",
    "        base_str = clean_html_str(tag)\n",
    "        if base_str:\n",
    "            tag_str = clean_html_str(tag.parent)\n",
    "            if tag_str.count('<') > 2:\n",
    "                tag_str = base_str\n",
    "            result_list.append(tag_str)\n",
    "    \n",
    "    return result_list\n",
    "\n",
    "def update_child_strs_list_dict(child_strs_list_dict=None, basic_tags_dict=None):\n",
    "    if basic_tags_dict is None:\n",
    "        basic_tags_dict = s.load_object('basic_tags_dict')\n",
    "    if child_strs_list_dict is None:\n",
    "        files_list = os.listdir(saves_html_folder)\n",
    "        child_strs_list_dict = {}\n",
    "        for file_name in files_list:\n",
    "            file_path = os.path.join(saves_html_folder, file_name)\n",
    "            with open(file_path, 'r', encoding='utf-8') as f:\n",
    "                html_str = f.read()\n",
    "                job_soup = BeautifulSoup(html_str, 'lxml')\n",
    "                body_soup = job_soup.find_all(name='body')[0]\n",
    "                child_strs_list = get_navigable_children(body_soup, [])\n",
    "            if not len(child_strs_list):\n",
    "                os.remove(file_path)\n",
    "            child_strs_list = [child_str for child_str in child_strs_list if child_str not in basic_tags_dict]\n",
    "            child_strs_list_dict[file_name] = child_strs_list\n",
    "        \n",
    "        return child_strs_list_dict\n",
    "    \n",
    "    for file_name, child_strs_list in child_strs_list_dict.items():\n",
    "        child_strs_list = [child_str for child_str in child_strs_list if child_str not in basic_tags_dict]\n",
    "        if not len(child_strs_list):\n",
    "            child_strs_list = child_strs_list_dict.pop(file_name)\n",
    "            break\n",
    "        else:\n",
    "            child_strs_list_dict[file_name] = child_strs_list\n",
    "    \n",
    "    return child_strs_list_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "child_strs_list_dict = update_child_strs_list_dict()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "----"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "scanner_regex = re.compile(r'(</?|\\b)[1-9a-zA-Z][0-9a-zA-Z]*( *[#\\+]{1,2}|>|:\\b|\\.\\b|\\b)')\n",
    "try:\n",
    "    fit_estimators_dict = s.load_object('fit_estimators_dict')\n",
    "except:\n",
    "    fit_estimators_dict = {}\n",
    "    fit_estimators_dict['LogisticRegression'] = LogisticRegression(**{'C': 7.5, 'class_weight': 'balanced', 'dual': True, 'fit_intercept': False,\n",
    "                                                                      'max_iter': 64, 'penalty': 'l2', 'solver': 'liblinear', 'tol': 1e-09})\n",
    "    s.store_objects(fit_estimators_dict=fit_estimators_dict)\n",
    "def regex_tokenizer(corpus):\n",
    "    \n",
    "    return [match.group() for match in re.finditer(scanner_regex, corpus)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "basic_tags_dict['<p>Benefits:</p>'] = False\n",
      "s.store_objects(basic_tags_dict=basic_tags_dict)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "basic_tags_dict = s.load_object('basic_tags_dict')\n",
    "child_strs_list_dict = update_child_strs_list_dict(child_strs_list_dict, basic_tags_dict)\n",
    "for file_name, child_strs_list in child_strs_list_dict.items():\n",
    "    for tag_str in child_strs_list:\n",
    "        print()\n",
    "        print()\n",
    "        if \"'\" in tag_str:\n",
    "            print(f'''\n",
    "basic_tags_dict[\"{tag_str}\"] = False\n",
    "s.store_objects(basic_tags_dict=basic_tags_dict)''')\n",
    "        else:\n",
    "            print(f'''\n",
    "basic_tags_dict['{tag_str}'] = False\n",
    "s.store_objects(basic_tags_dict=basic_tags_dict)''')\n",
    "        break\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pickling to C:\\Users\\dev\\Documents\\Repositories\\job-hunting\\saves\\pickle\\basic_tags_dict.pickle\n"
     ]
    }
   ],
   "source": [
    "\n",
    "basic_tags_dict['<p>Benefits:</p>'] = True\n",
    "s.store_objects(basic_tags_dict=basic_tags_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "for child_str, pred_array in zip(child_strs_list, predict_percent_fit(child_strs_list)):\n",
    "    print()\n",
    "    print(pred_array)\n",
    "    print(child_str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pickling to C:\\Users\\dev\\Documents\\Repositories\\job-hunting\\saves\\pickle\\cs_cv_vocab.pickle\n",
      "Pickling to C:\\Users\\dev\\Documents\\Repositories\\job-hunting\\saves\\pickle\\cs_tt.pickle\n",
      "Pickling to C:\\Users\\dev\\Documents\\Repositories\\job-hunting\\saves\\pickle\\child_str_clf.pickle\n",
      "Pickling to C:\\Users\\dev\\Documents\\Repositories\\job-hunting\\saves\\pickle\\fit_estimators_dict.pickle\n",
      "Retraining complete\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Re-transform the bag-of-words and tf-idf from the new manual scores\n",
    "basic_tags_dict = s.load_object('basic_tags_dict')\n",
    "rows_list = [{'navigable_parent': navigable_parent, 'is_header': is_header} for navigable_parent, is_header in basic_tags_dict.items()]\n",
    "child_str_df = pd.DataFrame(rows_list)\n",
    "\n",
    "if child_str_df.shape[0]:\n",
    "    sents_list = child_str_df.navigable_parent.tolist()\n",
    "    \n",
    "    # Bag-of-words\n",
    "    cv = CountVectorizer(lowercase=False, tokenizer=regex_tokenizer, ngram_range=(1, 3))\n",
    "    bow_matrix = cv.fit_transform(sents_list)\n",
    "    s.store_objects(cs_cv_vocab=cv.vocabulary_)\n",
    "    \n",
    "    # Tf-idf must get from Bag-of-words first\n",
    "    tt = TfidfTransformer()\n",
    "    tfidf_matrix = tt.fit_transform(bow_matrix)\n",
    "    s.store_objects(cs_tt=tt)\n",
    "    \n",
    "    # Re-train the classifier\n",
    "    X = tfidf_matrix.toarray()\n",
    "    y = child_str_df.is_header.to_numpy()\n",
    "    fit_estimators_dict = s.load_object('fit_estimators_dict')\n",
    "    child_str_clf = fit_estimators_dict['LogisticRegression']\n",
    "    child_str_clf.fit(X, y)\n",
    "    fit_estimators_dict['LogisticRegression'] = child_str_clf\n",
    "    s.store_objects(child_str_clf=child_str_clf, fit_estimators_dict=fit_estimators_dict)\n",
    "    \n",
    "    # Re-calibrate the inference engine\n",
    "    cs_cv_vocab = s.load_object('cs_cv_vocab')\n",
    "    cs_cv = CountVectorizer(vocabulary=cs_cv_vocab)\n",
    "    cs_cv._validate_vocabulary()\n",
    "    cs_tt = s.load_object('cs_tt')\n",
    "    def predict_percent_fit(navigable_parents_list):\n",
    "        y_predict_proba = np.array([])\n",
    "        if len(navigable_parents_list):\n",
    "            X_test = cs_tt.transform(cs_cv.transform(navigable_parents_list)).toarray()\n",
    "            y_predict_proba = child_str_clf.predict_proba(X_test)\n",
    "\n",
    "        return y_predict_proba\n",
    "print('Retraining complete')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('way', 4.33), ('development of causal', 4.74), ('The Statistician is', 4.74), ('Full Time', 4.74), ('team This person', 4.74), ('Keep the', 4.74), ('contributed content millions', 4.74), ('<p> Job', 4.74), ('Plus </b>', 4.74), ('people', 4.74), ('closely tied', 4.74), ('as possible We', 4.74), ('their analytical statistical', 4.74), ('build predictive', 4.74), ('application', 4.74), ('ASAP', 4.74), ('Yelp we believe', 4.74), ('those', 4.74), ('or disability', 4.74), ('environment </div>', 4.74)]"
      ]
     },
     "execution_count": 201,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "sample_list = []\n",
    "try:\n",
    "    cs_cv_vocab = s.load_object('cs_cv_vocab')\n",
    "    sample_list = random.sample([(w, i) for w, i in cs_cv_vocab.items()], 20)\n",
    "except:\n",
    "    cs_cv_vocab = {}\n",
    "    s.store_objects(cs_cv_vocab=cs_cv_vocab)\n",
    "sorted([(w, round(cs_tt.idf_[i], 2)) for w, i in sample_list], key=lambda x: x[1], reverse=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['analyzer', 'binary', 'build_analyzer', 'build_preprocessor', 'build_tokenizer', 'decode', 'decode_error', 'dtype', 'encoding', 'fit', 'fit_transform', 'fixed_vocabulary_', 'get_feature_names', 'get_params', 'get_stop_words', 'input', 'inverse_transform', 'lowercase', 'max_df', 'max_features', 'min_df', 'ngram_range', 'preprocessor', 'set_params', 'stop_words', 'stop_words_', 'strip_accents', 'token_pattern', 'tokenizer', 'transform', 'vocabulary', 'vocabulary_']"
      ]
     },
     "execution_count": 168,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "[fn for fn in dir(cv) if not fn.startswith('_')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['quantitative', 'We re', 'diverse backgrounds and', '<b> Skills', 'degree', 'Media', 'hugely impactful', 'join', 'growing as', 'Applied scientists uncover', 'a new', 'Authenticity </i>', '401', 'problem solver with', 'expect you', 'believe that diversity', 'Clients for', 'quantitative discipline', 'we will consider', 'accept']"
      ]
     },
     "execution_count": 173,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "random.sample([w for w in cv.get_feature_names()], 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[1;31mSignature:\u001b[0m \u001b[0mcv\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtokenizer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcorpus\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
       "\u001b[1;31mDocstring:\u001b[0m <no docstring>\n",
       "\u001b[1;31mSource:\u001b[0m   \n",
       "\u001b[1;32mdef\u001b[0m \u001b[0mregex_tokenizer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcorpus\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\n",
       "\u001b[0m    \u001b[1;33m\n",
       "\u001b[0m    \u001b[1;32mreturn\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mmatch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgroup\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mmatch\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mre\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfinditer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mscanner_regex\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcorpus\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
       "\u001b[1;31mFile:\u001b[0m      c:\\users\\dev\\documents\\repositories\\job-hunting\\ipynb\\<ipython-input-23-644ac792dd2c>\n",
       "\u001b[1;31mType:\u001b[0m      function\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "cv.tokenizer??"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "----"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "if len(child_strs_list):\n",
    "    df = pd.DataFrame(child_strs_list, columns=['navigable_parent'])\n",
    "    df['is_header'] = False\n",
    "    id_list = []\n",
    "    df.loc[id_list, 'is_header'] = True\n",
    "    df.tail(60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "name 'df' is not defined\n"
     ]
    }
   ],
   "source": [
    "\n",
    "try:\n",
    "    for row_index, row_series in df.iterrows():\n",
    "        navigable_parent = row_series.navigable_parent\n",
    "        is_header = row_series.is_header\n",
    "        basic_tags_dict[navigable_parent] = is_header\n",
    "    s.store_objects(basic_tags_dict=basic_tags_dict)\n",
    "    rows_list = [{'navigable_parent': navigable_parent, 'is_header': is_header} for navigable_parent, is_header in basic_tags_dict.items()]\n",
    "    child_str_df = pd.DataFrame(rows_list)\n",
    "    s.store_objects(child_str_df=child_str_df)\n",
    "except Exception as e:\n",
    "    print(str(e).strip())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "----"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pickling to C:\\Users\\dev\\Documents\\Repositories\\job-hunting\\saves\\pickle\\child_str_df.pickle\n"
     ]
    }
   ],
   "source": [
    "\n",
    "try:\n",
    "    basic_tags_dict = s.load_object('basic_tags_dict')\n",
    "    rows_list = [{'navigable_parent': navigable_parent, 'is_header': is_header} for navigable_parent, is_header in basic_tags_dict.items()]\n",
    "    child_str_df = pd.DataFrame(rows_list)\n",
    "    s.store_objects(child_str_df=child_str_df)\n",
    "except:\n",
    "    child_str_df = s.load_object('child_str_df')\n",
    "    try:\n",
    "        child_str_df = pd.concat([child_str_df, df])\n",
    "    except:\n",
    "        pass\n",
    "    basic_tags_dict = child_str_df.set_index('navigable_parent').to_dict()['is_header']\n",
    "    s.store_objects(basic_tags_dict=basic_tags_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "----\n",
    "# Download Job HTML"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "%run ../load_magic/storage.py\n",
    "s = Storage()\n",
    "base_url = 'https://www.indeed.com'\n",
    "site_url = base_url + '/jobs'\n",
    "site_url = '?'.join([site_url, 'q=data+scientist'])\n",
    "site_url = '&'.join([site_url, 'jt=fulltime'])\n",
    "site_url = '&'.join([site_url, 'remotejob=032b3046-06a3-4876-8dfd-474eb5e7ed11'])\n",
    "site_url = '&'.join([site_url, 'vjk=ca16b63c03e40c57'])\n",
    "#site_url = '&'.join([site_url, 'pp=gQAPAAABdY7RMKwAAAABkQdgZAAkAQBEKPpaoZstIag3f-UtQXXG_HFSo1gfBp9OQ0B0TvZ4yMp4AAA'])\n",
    "start_num = 0\n",
    "try:\n",
    "    job_urls_list = s.load_object('job_urls_list')\n",
    "except:\n",
    "    job_urls_list = []\n",
    "    s.store_objects(job_urls_list=job_urls_list)\n",
    "space_regex = re.compile(r'[\\s<>:\"/\\\\\\|\\?\\*_]+')\n",
    "print_regex = re.compile(r'[\\x9c-\\x9d\\uf0b7\\u200b\\ufb02]+')\n",
    "s.encoding_type = ['latin1', 'iso8859-1', 'utf-8'][2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Complete.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "fccid_htmls_list = [fn.split('_')[-1] for fn in os.listdir(saves_html_folder)]\n",
    "row_count = len(job_urls_list)\n",
    "for i, job_url in enumerate(job_urls_list):\n",
    "    qs = urlparse(job_url).query\n",
    "    query_dict = parse_qs(qs)\n",
    "    fccid_str = query_dict['fccid'][0]\n",
    "    file_name = f'{fccid_str}.html'\n",
    "    if file_name not in fccid_htmls_list:\n",
    "        job_page = requests.get(url=job_url)\n",
    "        job_soup = BeautifulSoup(job_page.content, 'lxml')\n",
    "        if not len(job_soup.text):\n",
    "            break\n",
    "        title_str = job_soup.find_all(name='title')[0].text.strip()\n",
    "        clear_output(wait=True)\n",
    "        print(f'{title_str}')\n",
    "        print(f'{i}/{row_count}: {job_url}')\n",
    "        if 'CAPTCHA' in title_str:\n",
    "            break\n",
    "        file_name = space_regex.sub('_', title_str)\n",
    "        file_name = f'{file_name}_{fccid_str}.html'\n",
    "        file_path = os.path.join(saves_html_folder, file_name)\n",
    "        body_soup = job_soup.find_all(name='body')[0]\n",
    "        html_str = '<html><head><title>' + title_str + '</title></head><body>'\n",
    "        with open(file_path, 'w', encoding='utf-8') as f:\n",
    "            print(html_str, file=f)\n",
    "            for div_tag in body_soup.find_all(name='div', class_='jobsearch-JobComponent-description'):\n",
    "                for s in div_tag.select('template'):\n",
    "                    s.extract()\n",
    "                for s in div_tag.select('script'):\n",
    "                    s.extract()\n",
    "                div_str = div_tag.prettify(formatter='html')\n",
    "                div_str = print_regex.sub('', div_str)\n",
    "                print(div_str, file=f)\n",
    "            print('</body></html>', file=f)\n",
    "print('Complete.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "while start_num < 3000:\n",
    "    page_url = '&'.join([site_url, f'start={start_num}'])\n",
    "    start_num += 10\n",
    "    site_page = requests.get(url=page_url)\n",
    "    page_soup = BeautifulSoup(site_page.content, 'lxml')\n",
    "    row_div_list = page_soup.find_all(name='div', class_=['row', 'result'])\n",
    "    row_count = len(row_div_list)\n",
    "    if row_count == 0:\n",
    "        print('Nothing left')\n",
    "        break\n",
    "    for i, row_div in enumerate(row_div_list):\n",
    "        link = row_div.find_all(name='a')[0]\n",
    "        if 'title' in link.attrs:\n",
    "            if 'href' in link.attrs:\n",
    "                job_url = base_url + link['href']\n",
    "                qs = urlparse(job_url).query\n",
    "                query_dict = parse_qs(qs)\n",
    "                if 'fccid' in query_dict:\n",
    "                    job_urls_list.append(job_url)\n",
    "                    s.store_objects(verbose=False, job_urls_list=job_urls_list)\n",
    "                    clear_output(wait=True)\n",
    "                    print(f'{page_url}')\n",
    "                    print(f'{i}/{row_count}: {job_url}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "273"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "fccid_htmls_list = [fn.split('_')[-1] for fn in os.listdir(saves_html_folder)]\n",
    "len(fccid_htmls_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\Users\\dev\\Anaconda3\\envs\\jh\\python.exe -m pip install --upgrade lxml\n",
      "Collecting lxml\n",
      "  Downloading lxml-4.6.1-cp39-cp39-win_amd64.whl (3.5 MB)\n",
      "Installing collected packages: lxml\n",
      "Successfully installed lxml-4.6.1\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import sys\n",
    "\n",
    "command_str = f'{sys.executable} -m pip install --upgrade lxml'\n",
    "print(command_str)\n",
    "!{command_str}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "----"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import os\n",
    "\n",
    "from selenium import webdriver\n",
    "from base64 import b64encode\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "from selenium.common.exceptions import NoSuchElementException\n",
    "import selenium.webdriver.support.ui as ui"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Retrieve the page with tag results and set it up to be scraped\n",
    "sitePage = requests.get(url=site_url)\n",
    "sitePageSoup = BeautifulSoup(sitePage.content, 'lxml')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "saves_txt_folder = os.path.join(s.saves_folder, 'txt')\n",
    "os.makedirs(name=saves_txt_folder, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "row_div_list = sitePageSoup.find_all(name='div', class_=['row', 'result'])\n",
    "reqs_regex = re.compile('ducat|xperience|equire')\n",
    "html_regex = re.compile('<[^>]+>')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<div class=\"jobsearch-JobDescriptionSection-jobDescriptionTitle icl-u-xs-my--md\" id=\"jobDescriptionTitle\">Full Job Description</div>'"
      ]
     },
     "execution_count": 201,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "df.loc[0, 'navigable_parent']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import pandas as pd\n",
    "\n",
    "div_tag = div_tag_list[11]\n",
    "child_str_list = get_navigable_children(div_tag, [])\n",
    "df = pd.DataFrame(child_str_list, columns=['navigable_parent'])\n",
    "df['is_header'] = False\n",
    "#df.loc[[0, 2, 4, 17, 19], 'is_header'] = True\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "text_list = ['What we are looking for', 'Key skills and Experience', 'Minimum qualifications', 'The Essentials', 'Qualifications',\n",
    "             'Skills and experience', 'Required Qualifications', 'What your background looks like', \"We(?:&rsquo;|')re looking for \\w+ who have\",\n",
    "             'Experience/Minimum Qualifications', 'Requirements', 'We are looking for someone with', 'Qualifications']\n",
    "reqs_regex = re.compile(f'^\\\\s*({'|'.join(text_list)}):?\\\\s*$', re.IGNORECASE | re.MULTILINE)\n",
    "for match_obj in reqs_regex.finditer(text_str):\n",
    "    # match start: match_obj.start()\n",
    "    # match end (exclusive): match_obj.end()\n",
    "    # matched text: match_obj.group()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['div_tag._lastRecursiveChild', 'div_tag.childGenerator', 'div_tag.children', 'div_tag.findChild', 'div_tag.findChildren', 'div_tag.recursiveChildGenerator', 'div_tag.replaceWithChildren', 'div_tag.replace_with_children']"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "[f'div_tag.{fn}' for fn in dir(div_tag) if 'child' in fn.lower()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "selector_list = ['body', 'div', 'div.jobsearch-ViewJobLayout-fluidContainer',\n",
    "                 'div.jobsearch-ViewJobLayout-content.jobsearch-ViewJobLayout-mainContent', 'div', 'div',\n",
    "                 'div.jobsearch-ViewJobLayout-jobDisplay', 'div.jobsearch-JobComponent', 'div.jobsearch-JobComponent-description']\n",
    "content_selector = ' > '.join(selector_list)\n",
    "\n",
    "def has_role_attr(tag):\n",
    "    \n",
    "    return tag.has_attr('role')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def has_class_and_id(tag):\n",
    "    \n",
    "    return tag.has_attr('class') and tag.has_attr('id')\n",
    "\n",
    "# <span class=\"summary\">We need a <b>data</b> <b>scientist</b> and <b>data</b> wrangler. Maybe survey <b>data</b>. \n",
    "# Better still if you have some demonstrable experience with more advanced machine learning methods...</span>\n",
    "summary_list = sitePageSoup.find_all(name='span', class_='summary')\n",
    "for summary in summary_list:\n",
    "    print(summary.text.strip())\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# <a target=\"_blank\" id=\"sja5\" data-tn-element=\"jobTitle\" class=\"jobtitle turnstileLink\" \n",
    "# href=\"https://www.indeed.com/pagead/clk?mo=r&amp;ad=-...-...-...-...-...-...-...&\n",
    "# amp;p=5&amp;sk=&amp;fvj=1&amp;tk=1c06s8995av53coj&amp;jsa=6565\" \n",
    "# title=\"Perception Scientist for Marine Autonomy\" rel=\"noopener nofollow\" \n",
    "# onmousedown=\"sjomd('sja5'); clk('sja5');\" onclick=\"setRefineByCookie([]); sjoc('sja5',0); convCtr('SJ')\"\n",
    "# >Perception <b>Scientist</b> for Marine Autonomy</a>\n",
    "siteCss = '#sja5'\n",
    "siteLinks = sitePageSoup.select(siteCss)\n",
    "print(siteLinks)\n",
    "\n",
    "max_page = 0\n",
    "if len(siteLinks):\n",
    "    max_page = int(siteLinks[0][\"href\"].split('/')[-1].split('-')[1].split(',')[0])\n",
    "print(max_page)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lxml 1\n",
      "html5lib 0\n",
      "html.parser 1\n"
     ]
    }
   ],
   "source": [
    "#<h1 class=\"srp-header\">25,589 Used Vehicles for sale</h1>\n",
    "\n",
    "for parser in ['lxml', 'html5lib', 'html.parser']:\n",
    "    sitePageSoup = BeautifulSoup(page_html, parser)\n",
    "    print(parser, len(sitePageSoup.select('h1.srp-header')))\n",
    "\n",
    "#max_page = int(sitePageSoup.find_all(\"a\", class_=\"js-last-page\")[0].text)\n",
    "#print(max_page)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Job Hunting (Python 3.9.0)",
   "language": "python",
   "name": "jh"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
