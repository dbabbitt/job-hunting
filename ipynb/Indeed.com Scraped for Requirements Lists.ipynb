{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pretty printing has been turned OFF\n"
     ]
    }
   ],
   "source": [
    "\n",
    "%pprint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from IPython.display import clear_output\n",
    "from bs4 import BeautifulSoup\n",
    "from bs4.element import NavigableString\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from urllib.parse import urlparse, parse_qs\n",
    "import numpy as np\n",
    "import os\n",
    "import random\n",
    "import re\n",
    "import requests\n",
    "\n",
    "%run ../load_magic/storage.py\n",
    "s = Storage()\n",
    "\n",
    "LT_REGEX = re.compile(r'\\s+<')\n",
    "GT_REGEX = re.compile(r'>\\s+')\n",
    "SAVES_HTML_FOLDER = os.path.join(s.saves_folder, 'html')\n",
    "try:\n",
    "    BASIC_TAGS_DICT = s.load_object('BASIC_TAGS_DICT')\n",
    "except:\n",
    "    BASIC_TAGS_DICT = {}\n",
    "    s.store_objects(BASIC_TAGS_DICT=BASIC_TAGS_DICT)\n",
    "try:\n",
    "    FIT_ESTIMATORS_DICT = s.load_object('FIT_ESTIMATORS_DICT')\n",
    "except:\n",
    "    FIT_ESTIMATORS_DICT = {}\n",
    "    FIT_ESTIMATORS_DICT['LogisticRegression'] = LogisticRegression(**{'C': 85.0, 'class_weight': 'balanced', 'dual': False,\n",
    "                                                                      'fit_intercept': True, 'max_iter': 6, 'penalty': 'l2', 'solver': 'sag',\n",
    "                                                                      'tol': 1e-08})\n",
    "    s.store_objects(FIT_ESTIMATORS_DICT=FIT_ESTIMATORS_DICT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def clean_html_str(html_obj):\n",
    "    html_str = str(html_obj)\n",
    "    html_str = html_str.strip()\n",
    "    html_str = LT_REGEX.sub('<', html_str)\n",
    "    html_str = GT_REGEX.sub('>', html_str)\n",
    "    \n",
    "    return html_str"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def get_navigable_children(tag, result_list=[]):\n",
    "    if type(tag) is not NavigableString:\n",
    "        for child_tag in tag.children:\n",
    "            result_list = get_navigable_children(child_tag, result_list)\n",
    "    else:\n",
    "        base_str = clean_html_str(tag)\n",
    "        if base_str:\n",
    "            tag_str = clean_html_str(tag.parent)\n",
    "            if tag_str.count('<') > 2:\n",
    "                tag_str = base_str\n",
    "            result_list.append(tag_str)\n",
    "    \n",
    "    return result_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def update_child_strs_list_dictionary(child_strs_list_dict=None, basic_tags_dict=None):\n",
    "    if basic_tags_dict is None:\n",
    "        basic_tags_dict = s.load_object('basic_tags_dict')\n",
    "    if child_strs_list_dict is None:\n",
    "        files_list = os.listdir(SAVES_HTML_FOLDER)\n",
    "        child_strs_list_dict = {}\n",
    "        for file_name in files_list:\n",
    "            file_path = os.path.join(SAVES_HTML_FOLDER, file_name)\n",
    "            with open(file_path, 'r', encoding='utf-8') as f:\n",
    "                html_str = f.read()\n",
    "                job_soup = BeautifulSoup(html_str, 'lxml')\n",
    "                body_soup = job_soup.find_all(name='body')[0]\n",
    "                child_strs_list = get_navigable_children(body_soup, [])\n",
    "            if not len(child_strs_list):\n",
    "                os.remove(file_path)\n",
    "            child_strs_list = [child_str for child_str in child_strs_list if child_str not in basic_tags_dict]\n",
    "            child_strs_list_dict[file_name] = child_strs_list\n",
    "        \n",
    "        return child_strs_list_dict\n",
    "    \n",
    "    for file_name, child_strs_list in child_strs_list_dict.items():\n",
    "        child_strs_list = [child_str for child_str in child_strs_list if child_str not in basic_tags_dict]\n",
    "        if not len(child_strs_list):\n",
    "            child_strs_list = child_strs_list_dict.pop(file_name)\n",
    "            break\n",
    "        else:\n",
    "            child_strs_list_dict[file_name] = child_strs_list\n",
    "    \n",
    "    return child_strs_list_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "try:\n",
    "    CHILD_STRS_LIST_DICT = s.load_object('CHILD_STRS_LIST_DICT')\n",
    "except:\n",
    "    CHILD_STRS_LIST_DICT = update_child_strs_list_dictionary()\n",
    "    s.store_objects(CHILD_STRS_LIST_DICT=CHILD_STRS_LIST_DICT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#SCANNER_REGEX = re.compile(r'(</?|\\b|:)[1-9a-zA-Z][0-9a-zA-Z]*( *[#\\+]{1,2}|>|\\.\\b|\\b)')\n",
    "SCANNER_REGEX = re.compile(r'</?\\w+|\\w+[#\\+]*|:|\\.')\n",
    "def regex_tokenizer(corpus):\n",
    "    \n",
    "    return [match.group() for match in re.finditer(SCANNER_REGEX, corpus)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 300,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['<li', 'MS', 'or', 'PhD', 'in', 'Applied', 'Mathematics', 'Physics', 'Computer', 'Science', 'Statistics', 'or', 'related', 'technical', 'field', '.', '</li']"
      ]
     },
     "execution_count": 300,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "regex_tokenizer('<li>MS or PhD in Applied Mathematics, Physics, Computer Science, Statistics or related technical field.</li>')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "----"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def create_dictionary_code():\n",
    "    BASIC_TAGS_DICT = s.load_object('BASIC_TAGS_DICT')\n",
    "    child_strs_list_dict = update_child_strs_list_dictionary(CHILD_STRS_LIST_DICT, BASIC_TAGS_DICT)\n",
    "    s.store_objects(CHILD_STRS_LIST_DICT=child_strs_list_dict)\n",
    "    for file_name, child_strs_list in child_strs_list_dict.items():\n",
    "        for tag_str in child_strs_list:\n",
    "            print()\n",
    "            print()\n",
    "            if \"'\" in tag_str:\n",
    "                print(f'''BASIC_TAGS_DICT[\"{tag_str}\"] = False''')\n",
    "            else:\n",
    "                print(f'''BASIC_TAGS_DICT['{tag_str}'] = False''')\n",
    "            print(f'''print(len(BASIC_TAGS_DICT.keys()))\\ns.store_objects(BASIC_TAGS_DICT=BASIC_TAGS_DICT)''')\n",
    "            break\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 388,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pickling to C:\\Users\\dev\\Documents\\Repositories\\job-hunting\\saves\\pickle\\CHILD_STRS_LIST_DICT.pickle\n",
      "\n",
      "\n",
      "BASIC_TAGS_DICT['<p>Your first three months will be spent getting up-to-speed on our team’s tools and processes, learning the details of the business problems we’re working on, contributing new ideas for solutions to those problems based on your experience</p>'] = False\n",
      "print(len(BASIC_TAGS_DICT.keys()))\n",
      "s.store_objects(BASIC_TAGS_DICT=BASIC_TAGS_DICT)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "create_dictionary_code()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 389,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "216\n",
      "Pickling to C:\\Users\\dev\\Documents\\Repositories\\job-hunting\\saves\\pickle\\BASIC_TAGS_DICT.pickle\n"
     ]
    }
   ],
   "source": [
    "\n",
    "BASIC_TAGS_DICT['<b>What does your success look like in the first 90 days?</b>'] = True\n",
    "print(len(BASIC_TAGS_DICT.keys()))\n",
    "s.store_objects(BASIC_TAGS_DICT=BASIC_TAGS_DICT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 390,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pickling to C:\\Users\\dev\\Documents\\Repositories\\job-hunting\\saves\\pickle\\cs_cv_vocab.pickle\n",
      "Pickling to C:\\Users\\dev\\Documents\\Repositories\\job-hunting\\saves\\pickle\\CS_TT.pickle\n",
      "Pickling to C:\\Users\\dev\\Documents\\Repositories\\job-hunting\\saves\\pickle\\child_str_clf.pickle\n",
      "Pickling to C:\\Users\\dev\\Documents\\Repositories\\job-hunting\\saves\\pickle\\FIT_ESTIMATORS_DICT.pickle\n",
      "Retraining complete\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\dev\\Anaconda3\\envs\\jh\\lib\\site-packages\\sklearn\\svm\\_base.py:976: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\"Liblinear failed to converge, increase \"\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Re-transform the bag-of-words and tf-idf from the new manual scores\n",
    "BASIC_TAGS_DICT = s.load_object('BASIC_TAGS_DICT')\n",
    "rows_list = [{'navigable_parent': navigable_parent, 'is_header': is_header} for navigable_parent, is_header in BASIC_TAGS_DICT.items()]\n",
    "child_str_df = pd.DataFrame(rows_list)\n",
    "\n",
    "if child_str_df.shape[0]:\n",
    "    sents_list = child_str_df.navigable_parent.tolist()\n",
    "    \n",
    "    # Bag-of-words\n",
    "    cv = CountVectorizer(**{'analyzer': 'word', 'binary': False, 'decode_error': 'strict', 'lowercase': False, 'max_df': 1.0,\n",
    "                            'max_features': None, 'min_df': 0.0, 'ngram_range': (1, 5), 'stop_words': None, 'strip_accents': 'ascii',\n",
    "                            'tokenizer': regex_tokenizer})\n",
    "    bow_matrix = cv.fit_transform(sents_list)\n",
    "    s.store_objects(cs_cv_vocab=cv.vocabulary_)\n",
    "    \n",
    "    # Tf-idf must get from Bag-of-words first\n",
    "    tt = TfidfTransformer(**{'norm': 'l1', 'smooth_idf': True, 'sublinear_tf': False, 'use_idf': True})\n",
    "    tfidf_matrix = tt.fit_transform(bow_matrix)\n",
    "    s.store_objects(CS_TT=tt)\n",
    "    \n",
    "    # Re-train the classifier\n",
    "    X = tfidf_matrix.toarray()\n",
    "    y = child_str_df.is_header.to_numpy()\n",
    "    FIT_ESTIMATORS_DICT = s.load_object('FIT_ESTIMATORS_DICT')\n",
    "    #child_str_clf = FIT_ESTIMATORS_DICT['LogisticRegression']\n",
    "    child_str_clf = LogisticRegression(**{'C': 375.0, 'class_weight': 'balanced', 'dual': False, 'fit_intercept': True, 'max_iter': 4,\n",
    "                                          'penalty': 'l1', 'solver': 'liblinear', 'tol': 7e-07})\n",
    "    child_str_clf.fit(X, y)\n",
    "    FIT_ESTIMATORS_DICT['LogisticRegression'] = child_str_clf\n",
    "    s.store_objects(child_str_clf=child_str_clf, FIT_ESTIMATORS_DICT=FIT_ESTIMATORS_DICT)\n",
    "    \n",
    "    # Re-calibrate the inference engine\n",
    "    cs_cv_vocab = s.load_object('cs_cv_vocab')\n",
    "    CS_CV = CountVectorizer(vocabulary=cs_cv_vocab)\n",
    "    CS_CV._validate_vocabulary()\n",
    "    CS_TT = s.load_object('CS_TT')\n",
    "    def predict_percent_fit(navigable_parents_list):\n",
    "        y_predict_proba = np.array([])\n",
    "        if len(navigable_parents_list):\n",
    "            X_test = CS_TT.transform(CS_CV.transform(navigable_parents_list)).toarray()\n",
    "            y_predict_proba = child_str_clf.predict_proba(X_test)\n",
    "\n",
    "        return y_predict_proba\n",
    "print('Retraining complete')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 391,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Preferred </li', 4.43), ('<p We are', 5.28), ('<li Good understanding of', 5.28), ('<li Retirement plan', 5.69), ('<li Bonus pay', 5.69), ('Liberty </b', 5.69), ('<p COVID 19', 5.69), ('<b Position Requirements : </b', 5.69), ('Retirement plan </li', 5.69), ('<li Partner with', 5.69), ('circuit design . </p', 5.69), ('always </li', 5.69), ('969 8488 . </i', 5.69), ('<li Oil', 5.69), ('<b Primary Responsibilities : </b', 5.69), ('learn on the job </li', 5.69), ('<p We are looking for', 5.69), ('<li Bachelor s Preferred', 5.69), ('<b Experience directly managing a', 5.69), ('on the job </li', 5.69)]"
      ]
     },
     "execution_count": 391,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "sample_list = []\n",
    "try:\n",
    "    cs_cv_vocab = s.load_object('cs_cv_vocab')\n",
    "    sample_list = random.sample([(w, i) for w, i in cs_cv_vocab.items() if '<' in w], 20)\n",
    "except:\n",
    "    cs_cv_vocab = {}\n",
    "    s.store_objects(cs_cv_vocab=cs_cv_vocab)\n",
    "sorted([(w, round(CS_TT.idf_[i], 2)) for w, i in sample_list], key=lambda x: x[1], reverse=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 392,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "<p>Tiger Analytics is looking for experienced Data Scientists to join our fast-growing advanced analytics consulting firm. Our consultants bring deep expertise in Data Science, Machine Learning and AI. We are the trusted analytics partner for multiple Fortune 500 companies, enabling them to generate business value from data. Our business value and leadership has been recognized by various market research firms, including Forrester and Gartner. We are looking for top-notch talent as we continue to build the best global analytics consulting team in the world.</p>\n",
      "\n",
      "As a Data Scientist, you will apply strong expertise in AI through the use of machine learning, data mining, and information retrieval to design, prototype, and build next generation advanced analytics engines and services. You will collaborate with cross-functional teams and business partners to define the technical problem statement and hypotheses to test. You will develop efficient and accurate analytical models which mimic business decisions and incorporate those models into analytical data products and tools. You will have the opportunity to drive current and future strategy by leveraging your analytical skills as you ensure business value and communicate the results.\n",
      "\n",
      "Header\n",
      "<b>Key Responsibilities</b>\n",
      "\n",
      "<li>Collaborate with business partners to develop innovative solutions to meet objectives utilizing cutting edge techniques and tools.</li>\n",
      "\n",
      "<li>Effectively communicate the analytics approach and how it will meet and address objectives to business partners.</li>\n",
      "\n",
      "<li>Advocate and educate on the value of data-driven decision making; focus on the “how and why” of solutioning.</li>\n",
      "\n",
      "<li>Lead analytic approaches; integrate solutions collaboratively into applications and tools with data engineers, business leads, analysts and developers.</li>\n",
      "\n",
      "<li>Create repeatable, interpretable, dynamic and scalable models that are seamlessly incorporated into analytic data products.</li>\n",
      "\n",
      "<li>Engineer features by using your business acumen to find new ways to combine disparate internal and external data sources.</li>\n",
      "\n",
      "<li>Share your passion for Data Science with the broader enterprise community; identify and develop long-term processes, frameworks, tools, methods and standards.</li>\n",
      "\n",
      "<li>Collaborate, coach, and learn with a growing team of experienced Data Scientists.</li>\n",
      "\n",
      "<li>Stay connected with external sources of ideas through conferences and community engagements</li>\n",
      "\n",
      "Header\n",
      "<b>Requirements</b>\n",
      "\n",
      "<li>Bachelors Degree in Data Science, Computer Science, or related field</li>\n",
      "\n",
      "<li>2-5 years of Data Science and Machine Learning experience required</li>\n",
      "\n",
      "<li>Proficiency in Python or R. Ability to write complex SQL queries</li>\n",
      "\n",
      "<li>Proficiency with Machine Learning concepts and modeling techniques to solve problems such as clustering, classification, regression, anomaly detection, simulation and optimization problems on large scale data sets.</li>\n",
      "\n",
      "<li>Ability to implement ML best practices for the entire Data Science lifecycle</li>\n",
      "\n",
      "<li>Ability to apply various analytical models to business use cases (NLP, Supervised, Un-Supervised, Neural Nets, etc.)</li>\n",
      "\n",
      "<li>Exceptional communication and collaboration skills to understand business partner needs and deliver solutions</li>\n",
      "\n",
      "<li>Bias for action, with the ability to deliver outstanding results through task prioritization and time management</li>\n",
      "\n",
      "<li>Experience with data visualization tools — Tableau, R Shiny, etc. preferred</li>\n",
      "\n",
      "Header\n",
      "<b>Benefits</b>\n",
      "\n",
      "<p>This position offers an excellent opportunity for significant career development in a fast-growing and challenging entrepreneurial environment with a high degree of individual responsibility.</p>\n"
     ]
    }
   ],
   "source": [
    "\n",
    "files_list = os.listdir(SAVES_HTML_FOLDER)\n",
    "file_name = random.choice(files_list)\n",
    "file_path = os.path.join(SAVES_HTML_FOLDER, file_name)\n",
    "with open(file_path, 'r', encoding='utf-8') as f:\n",
    "    html_str = f.read()\n",
    "    job_soup = BeautifulSoup(html_str, 'lxml')\n",
    "    body_soup = job_soup.find_all(name='body')[0]\n",
    "    child_strs_list = get_navigable_children(body_soup, [])\n",
    "for y_predict_proba, child_str in zip(predict_percent_fit(child_strs_list), child_strs_list):\n",
    "    print()\n",
    "    if y_predict_proba[1] > 0.05:\n",
    "        print('Header')\n",
    "    print(child_str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['analyzer', 'binary', 'build_analyzer', 'build_preprocessor', 'build_tokenizer', 'decode', 'decode_error', 'dtype', 'encoding', 'fit', 'fit_transform', 'fixed_vocabulary_', 'get_feature_names', 'get_params', 'get_stop_words', 'input', 'inverse_transform', 'lowercase', 'max_df', 'max_features', 'min_df', 'ngram_range', 'preprocessor', 'set_params', 'stop_words', 'stop_words_', 'strip_accents', 'token_pattern', 'tokenizer', 'transform', 'vocabulary', 'vocabulary_']"
      ]
     },
     "execution_count": 168,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "[fn for fn in dir(cv) if not fn.startswith('_')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['capability for', 'Come In', 'You', 'example', 'Experience', '80', 'language', 'learning to', 'and the', 'safe and', 'graduate', 'Schedule :', 'Yelp', 'Bachelor s', 'experience </li', 'Supplemental Pay', 'automate', 'work', 'persistence', 'full time']"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "random.sample([w for w in cv.get_feature_names()], 20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "----"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "if len(child_strs_list):\n",
    "    df = pd.DataFrame(child_strs_list, columns=['navigable_parent'])\n",
    "    df['is_header'] = False\n",
    "    id_list = []\n",
    "    df.loc[id_list, 'is_header'] = True\n",
    "    df.tail(60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "name 'df' is not defined\n"
     ]
    }
   ],
   "source": [
    "\n",
    "try:\n",
    "    for row_index, row_series in df.iterrows():\n",
    "        navigable_parent = row_series.navigable_parent\n",
    "        is_header = row_series.is_header\n",
    "        BASIC_TAGS_DICT[navigable_parent] = is_header\n",
    "    s.store_objects(BASIC_TAGS_DICT=BASIC_TAGS_DICT)\n",
    "    rows_list = [{'navigable_parent': navigable_parent, 'is_header': is_header} for navigable_parent, is_header in BASIC_TAGS_DICT.items()]\n",
    "    child_str_df = pd.DataFrame(rows_list)\n",
    "    s.store_objects(child_str_df=child_str_df)\n",
    "except Exception as e:\n",
    "    print(str(e).strip())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "----"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pickling to C:\\Users\\dev\\Documents\\Repositories\\job-hunting\\saves\\pickle\\child_str_df.pickle\n"
     ]
    }
   ],
   "source": [
    "\n",
    "try:\n",
    "    BASIC_TAGS_DICT = s.load_object('BASIC_TAGS_DICT')\n",
    "    rows_list = [{'navigable_parent': navigable_parent, 'is_header': is_header} for navigable_parent, is_header in BASIC_TAGS_DICT.items()]\n",
    "    child_str_df = pd.DataFrame(rows_list)\n",
    "    s.store_objects(child_str_df=child_str_df)\n",
    "except:\n",
    "    child_str_df = s.load_object('child_str_df')\n",
    "    try:\n",
    "        child_str_df = pd.concat([child_str_df, df])\n",
    "    except:\n",
    "        pass\n",
    "    BASIC_TAGS_DICT = child_str_df.set_index('navigable_parent').to_dict()['is_header']\n",
    "    s.store_objects(BASIC_TAGS_DICT=BASIC_TAGS_DICT)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "----\n",
    "# Download Job HTML"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "%run ../load_magic/storage.py\n",
    "s = Storage()\n",
    "base_url = 'https://www.indeed.com'\n",
    "site_url = base_url + '/jobs'\n",
    "site_url = '?'.join([site_url, 'q=data+scientist'])\n",
    "site_url = '&'.join([site_url, 'jt=fulltime'])\n",
    "site_url = '&'.join([site_url, 'remotejob=032b3046-06a3-4876-8dfd-474eb5e7ed11'])\n",
    "site_url = '&'.join([site_url, 'vjk=ca16b63c03e40c57'])\n",
    "#site_url = '&'.join([site_url, 'pp=gQAPAAABdY7RMKwAAAABkQdgZAAkAQBEKPpaoZstIag3f-UtQXXG_HFSo1gfBp9OQ0B0TvZ4yMp4AAA'])\n",
    "start_num = 0\n",
    "try:\n",
    "    job_urls_list = s.load_object('job_urls_list')\n",
    "except:\n",
    "    job_urls_list = []\n",
    "    s.store_objects(job_urls_list=job_urls_list)\n",
    "space_regex = re.compile(r'[\\s<>:\"/\\\\\\|\\?\\*_]+')\n",
    "print_regex = re.compile(r'[\\x9c-\\x9d\\uf0b7\\u200b\\ufb02]+')\n",
    "s.encoding_type = ['latin1', 'iso8859-1', 'utf-8'][2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Complete.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "fccid_htmls_list = [fn.split('_')[-1] for fn in os.listdir(SAVES_HTML_FOLDER)]\n",
    "row_count = len(job_urls_list)\n",
    "for i, job_url in enumerate(job_urls_list):\n",
    "    qs = urlparse(job_url).query\n",
    "    query_dict = parse_qs(qs)\n",
    "    fccid_str = query_dict['fccid'][0]\n",
    "    file_name = f'{fccid_str}.html'\n",
    "    if file_name not in fccid_htmls_list:\n",
    "        job_page = requests.get(url=job_url)\n",
    "        job_soup = BeautifulSoup(job_page.content, 'lxml')\n",
    "        if not len(job_soup.text):\n",
    "            break\n",
    "        title_str = job_soup.find_all(name='title')[0].text.strip()\n",
    "        clear_output(wait=True)\n",
    "        print(f'{title_str}')\n",
    "        print(f'{i}/{row_count}: {job_url}')\n",
    "        if 'CAPTCHA' in title_str:\n",
    "            break\n",
    "        file_name = space_regex.sub('_', title_str)\n",
    "        file_name = f'{file_name}_{fccid_str}.html'\n",
    "        file_path = os.path.join(SAVES_HTML_FOLDER, file_name)\n",
    "        body_soup = job_soup.find_all(name='body')[0]\n",
    "        html_str = '<html><head><title>' + title_str + '</title></head><body>'\n",
    "        with open(file_path, 'w', encoding='utf-8') as f:\n",
    "            print(html_str, file=f)\n",
    "            for div_tag in body_soup.find_all(name='div', class_='jobsearch-JobComponent-description'):\n",
    "                for s in div_tag.select('template'):\n",
    "                    s.extract()\n",
    "                for s in div_tag.select('script'):\n",
    "                    s.extract()\n",
    "                div_str = div_tag.prettify(formatter='html')\n",
    "                div_str = print_regex.sub('', div_str)\n",
    "                print(div_str, file=f)\n",
    "            print('</body></html>', file=f)\n",
    "print('Complete.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "while start_num < 3000:\n",
    "    page_url = '&'.join([site_url, f'start={start_num}'])\n",
    "    start_num += 10\n",
    "    site_page = requests.get(url=page_url)\n",
    "    page_soup = BeautifulSoup(site_page.content, 'lxml')\n",
    "    row_div_list = page_soup.find_all(name='div', class_=['row', 'result'])\n",
    "    row_count = len(row_div_list)\n",
    "    if row_count == 0:\n",
    "        print('Nothing left')\n",
    "        break\n",
    "    for i, row_div in enumerate(row_div_list):\n",
    "        link = row_div.find_all(name='a')[0]\n",
    "        if 'title' in link.attrs:\n",
    "            if 'href' in link.attrs:\n",
    "                job_url = base_url + link['href']\n",
    "                qs = urlparse(job_url).query\n",
    "                query_dict = parse_qs(qs)\n",
    "                if 'fccid' in query_dict:\n",
    "                    job_urls_list.append(job_url)\n",
    "                    s.store_objects(verbose=False, job_urls_list=job_urls_list)\n",
    "                    clear_output(wait=True)\n",
    "                    print(f'{page_url}')\n",
    "                    print(f'{i}/{row_count}: {job_url}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "273"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "fccid_htmls_list = [fn.split('_')[-1] for fn in os.listdir(SAVES_HTML_FOLDER)]\n",
    "len(fccid_htmls_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\Users\\dev\\Anaconda3\\envs\\jh\\python.exe -m pip install --upgrade lxml\n",
      "Collecting lxml\n",
      "  Downloading lxml-4.6.1-cp39-cp39-win_amd64.whl (3.5 MB)\n",
      "Installing collected packages: lxml\n",
      "Successfully installed lxml-4.6.1\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import sys\n",
    "\n",
    "command_str = f'{sys.executable} -m pip install --upgrade lxml'\n",
    "print(command_str)\n",
    "!{command_str}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "----"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import os\n",
    "\n",
    "from selenium import webdriver\n",
    "from base64 import b64encode\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "from selenium.common.exceptions import NoSuchElementException\n",
    "import selenium.webdriver.support.ui as ui"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Retrieve the page with tag results and set it up to be scraped\n",
    "sitePage = requests.get(url=site_url)\n",
    "sitePageSoup = BeautifulSoup(sitePage.content, 'lxml')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "saves_txt_folder = os.path.join(s.saves_folder, 'txt')\n",
    "os.makedirs(name=saves_txt_folder, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "row_div_list = sitePageSoup.find_all(name='div', class_=['row', 'result'])\n",
    "reqs_regex = re.compile('ducat|xperience|equire')\n",
    "html_regex = re.compile('<[^>]+>')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<div class=\"jobsearch-JobDescriptionSection-jobDescriptionTitle icl-u-xs-my--md\" id=\"jobDescriptionTitle\">Full Job Description</div>'"
      ]
     },
     "execution_count": 201,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "df.loc[0, 'navigable_parent']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import pandas as pd\n",
    "\n",
    "div_tag = div_tag_list[11]\n",
    "child_str_list = get_navigable_children(div_tag, [])\n",
    "df = pd.DataFrame(child_str_list, columns=['navigable_parent'])\n",
    "df['is_header'] = False\n",
    "#df.loc[[0, 2, 4, 17, 19], 'is_header'] = True\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "text_list = ['What we are looking for', 'Key skills and Experience', 'Minimum qualifications', 'The Essentials', 'Qualifications',\n",
    "             'Skills and experience', 'Required Qualifications', 'What your background looks like', \"We(?:&rsquo;|')re looking for \\w+ who have\",\n",
    "             'Experience/Minimum Qualifications', 'Requirements', 'We are looking for someone with', 'Qualifications']\n",
    "reqs_regex = re.compile(f'^\\\\s*({'|'.join(text_list)}):?\\\\s*$', re.IGNORECASE | re.MULTILINE)\n",
    "for match_obj in reqs_regex.finditer(text_str):\n",
    "    # match start: match_obj.start()\n",
    "    # match end (exclusive): match_obj.end()\n",
    "    # matched text: match_obj.group()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['div_tag._lastRecursiveChild', 'div_tag.childGenerator', 'div_tag.children', 'div_tag.findChild', 'div_tag.findChildren', 'div_tag.recursiveChildGenerator', 'div_tag.replaceWithChildren', 'div_tag.replace_with_children']"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "[f'div_tag.{fn}' for fn in dir(div_tag) if 'child' in fn.lower()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "selector_list = ['body', 'div', 'div.jobsearch-ViewJobLayout-fluidContainer',\n",
    "                 'div.jobsearch-ViewJobLayout-content.jobsearch-ViewJobLayout-mainContent', 'div', 'div',\n",
    "                 'div.jobsearch-ViewJobLayout-jobDisplay', 'div.jobsearch-JobComponent', 'div.jobsearch-JobComponent-description']\n",
    "content_selector = ' > '.join(selector_list)\n",
    "\n",
    "def has_role_attr(tag):\n",
    "    \n",
    "    return tag.has_attr('role')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def has_class_and_id(tag):\n",
    "    \n",
    "    return tag.has_attr('class') and tag.has_attr('id')\n",
    "\n",
    "# <span class=\"summary\">We need a <b>data</b> <b>scientist</b> and <b>data</b> wrangler. Maybe survey <b>data</b>. \n",
    "# Better still if you have some demonstrable experience with more advanced machine learning methods...</span>\n",
    "summary_list = sitePageSoup.find_all(name='span', class_='summary')\n",
    "for summary in summary_list:\n",
    "    print(summary.text.strip())\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# <a target=\"_blank\" id=\"sja5\" data-tn-element=\"jobTitle\" class=\"jobtitle turnstileLink\" \n",
    "# href=\"https://www.indeed.com/pagead/clk?mo=r&amp;ad=-...-...-...-...-...-...-...&\n",
    "# amp;p=5&amp;sk=&amp;fvj=1&amp;tk=1c06s8995av53coj&amp;jsa=6565\" \n",
    "# title=\"Perception Scientist for Marine Autonomy\" rel=\"noopener nofollow\" \n",
    "# onmousedown=\"sjomd('sja5'); clk('sja5');\" onclick=\"setRefineByCookie([]); sjoc('sja5',0); convCtr('SJ')\"\n",
    "# >Perception <b>Scientist</b> for Marine Autonomy</a>\n",
    "siteCss = '#sja5'\n",
    "siteLinks = sitePageSoup.select(siteCss)\n",
    "print(siteLinks)\n",
    "\n",
    "max_page = 0\n",
    "if len(siteLinks):\n",
    "    max_page = int(siteLinks[0][\"href\"].split('/')[-1].split('-')[1].split(',')[0])\n",
    "print(max_page)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lxml 1\n",
      "html5lib 0\n",
      "html.parser 1\n"
     ]
    }
   ],
   "source": [
    "#<h1 class=\"srp-header\">25,589 Used Vehicles for sale</h1>\n",
    "\n",
    "for parser in ['lxml', 'html5lib', 'html.parser']:\n",
    "    sitePageSoup = BeautifulSoup(page_html, parser)\n",
    "    print(parser, len(sitePageSoup.select('h1.srp-header')))\n",
    "\n",
    "#max_page = int(sitePageSoup.find_all(\"a\", class_=\"js-last-page\")[0].text)\n",
    "#print(max_page)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Job Hunting (Python 3.9.0)",
   "language": "python",
   "name": "jh"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
