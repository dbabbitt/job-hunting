{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "anticipated-newsletter",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "broadband-posting",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pretty printing has been turned OFF\n"
     ]
    }
   ],
   "source": [
    "\n",
    "%pprint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "premium-bottom",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 1.52 s\n"
     ]
    }
   ],
   "source": [
    "\n",
    "%%time\n",
    "from neo4j import GraphDatabase\n",
    "%run ../py/cypher_utlis.py\n",
    "%run ../load_magic/storage.py\n",
    "%run ../load_magic/dataframes.py\n",
    "\n",
    "s = Storage()\n",
    "cu = CypherUtilities()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "greek-induction",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "uri = 'bolt://localhost:7687'\n",
    "user = 'neo4j'\n",
    "password = 'Genesis11'\n",
    "driver = GraphDatabase.driver(uri, auth=(user, password))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "handmade-vegetation",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "INFIX_STR = ''',\n",
    "    '''\n",
    "def get_tx_str(column_descriptions_df, idx_column, f_str):\n",
    "    mask_series = (column_descriptions_df.dtype == 'int64')\n",
    "    int64s_list = column_descriptions_df[mask_series].column_name.tolist()\n",
    "    with_str = INFIX_STR.join(['toInteger(row.{}) AS {}'.format(x, x) for x in int64s_list])\n",
    "    objects_list = column_descriptions_df[~mask_series].column_name.tolist()\n",
    "    if objects_list:\n",
    "        with_str += INFIX_STR + INFIX_STR.join(['row.{} AS {}'.format(x, x) for x in objects_list])\n",
    "    mask_series = (column_descriptions_df.column_name != idx_column)\n",
    "    nonidxs_list = column_descriptions_df[mask_series].column_name.tolist()\n",
    "    set_str = INFIX_STR.join(['x.{} = {}'.format(x, x) for x in nonidxs_list])\n",
    "    tx_str = f_str.format(with_str, set_str)\n",
    "    \n",
    "    return tx_str"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "reserved-filing",
   "metadata": {},
   "source": [
    "\n",
    "## Get Primary Keys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "quality-overview",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "sql_str = '''\n",
    "SELECT\n",
    "    tab.[name] AS table_name,\n",
    "    Substring(column_names, 1, Len(column_names) - 1) AS [columns]\n",
    "FROM\n",
    "    sys.tables tab LEFT OUTER JOIN\n",
    "    sys.indexes pk ON\n",
    "        tab.object_id = pk.object_id AND\n",
    "        pk.is_primary_key = 1\n",
    "    CROSS apply (\n",
    "        SELECT col.[name] + ', '\n",
    "        FROM\n",
    "            sys.index_columns ic INNER JOIN\n",
    "            sys.columns col ON\n",
    "                ic.object_id = col.object_id AND\n",
    "                ic.column_id = col.column_id\n",
    "        WHERE\n",
    "            ic.object_id = tab.object_id AND\n",
    "            ic.index_id = pk.index_id\n",
    "        ORDER  BY col.column_id\n",
    "        FOR xml path ('')\n",
    "        ) D (column_names)\n",
    "WHERE\n",
    "    Schema_name(tab.schema_id) = 'dbo' AND\n",
    "    tab.[name] != 'sysdiagrams'\n",
    "ORDER BY\n",
    "    Schema_name(tab.schema_id),\n",
    "    tab.[name];'''\n",
    "pks_df = pd.DataFrame(su.get_execution_results(CURSOR, sql_str, verbose=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "mental-interstate",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import os\n",
    "from shutil import copyfile\n",
    "\n",
    "def clear_database(tx):\n",
    "    delete_str = \"CALL apoc.periodic.iterate('MATCH (n) RETURN n', 'DETACH DELETE n', {batchSize:1000})\"\n",
    "    tx.run(delete_str)\n",
    "def create_constraint(tx, class_name):\n",
    "    constraint_str = 'CREATE CONSTRAINT Unique{} IF NOT EXISTS ON (x:{}) ASSERT x.id IS UNIQUE;'\n",
    "    constraint_str = constraint_str.format(class_name, class_name)\n",
    "    tx.run(constraint_str)\n",
    "def f(x):\n",
    "    if type(x) is str:\n",
    "        x = x.strip()\n",
    "    \n",
    "    return x\n",
    "dbmss_dir = r'C:\\Users\\dev\\.Neo4jDesktop\\relate-data\\dbmss'\n",
    "import_dir = os.path.join(dbmss_dir, 'dbms-c715bff3-5764-4d71-bf04-6eceb4dd13f4', 'import')\n",
    "csv_dir = '../saves/csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "middle-wiring",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import pandas as pd\n",
    "\n",
    "def do_cypher_tx(tx, cypher):\n",
    "    results_list = tx.run(cypher)\n",
    "    values_list = []\n",
    "    for record in results_list:\n",
    "        values_list.append(dict(record.items()))\n",
    "    \n",
    "    return values_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "removable-transaction",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving to C:\\Users\\dev\\Documents\\Repositories\\job-hunting\\saves\\csv\\FileNames.csv\n",
      "\n",
      "LOAD CSV WITH HEADERS FROM 'file:///FileNames.csv' AS row\n",
      "WITH\n",
      "    toInteger(row.file_name_id) AS file_name_id,\n",
      "    row.file_name AS file_name,\n",
      "    row.percent_fit AS percent_fit,\n",
      "    row.is_opportunity_application_emailed AS is_opportunity_application_emailed,\n",
      "    row.opportunity_application_email_date AS opportunity_application_email_date,\n",
      "    row.is_remote_delivery AS is_remote_delivery,\n",
      "    row.manager_notes AS manager_notes\n",
      "MERGE (x:FileNames {file_name_id: file_name_id}) SET\n",
      "    x.file_name = file_name,\n",
      "    x.percent_fit = percent_fit,\n",
      "    x.is_opportunity_application_emailed = is_opportunity_application_emailed,\n",
      "    x.opportunity_application_email_date = opportunity_application_email_date,\n",
      "    x.is_remote_delivery = is_remote_delivery,\n",
      "    x.manager_notes = manager_notes;\n",
      "Saving to C:\\Users\\dev\\Documents\\Repositories\\job-hunting\\saves\\csv\\HeaderTags.csv\n",
      "\n",
      "LOAD CSV WITH HEADERS FROM 'file:///HeaderTags.csv' AS row\n",
      "WITH\n",
      "    toInteger(row.header_tag_id) AS header_tag_id,\n",
      "    row.is_in_document_structure_elements_set AS is_in_document_structure_elements_set,\n",
      "    row.is_in_document_head_elements_set AS is_in_document_head_elements_set,\n",
      "    row.is_in_document_body_elements_set AS is_in_document_body_elements_set,\n",
      "    row.is_in_block_elements_set AS is_in_block_elements_set,\n",
      "    row.is_in_basic_text_set AS is_in_basic_text_set,\n",
      "    row.is_in_section_headings_set AS is_in_section_headings_set,\n",
      "    row.is_in_lists_set AS is_in_lists_set,\n",
      "    row.is_in_other_block_elements_set AS is_in_other_block_elements_set,\n",
      "    row.is_in_inline_elements_set AS is_in_inline_elements_set,\n",
      "    row.is_in_anchor_set AS is_in_anchor_set,\n",
      "    row.is_in_phrase_elements_set AS is_in_phrase_elements_set,\n",
      "    row.is_in_general_set AS is_in_general_set,\n",
      "    row.is_in_computer_phrase_elements_set AS is_in_computer_phrase_elements_set,\n",
      "    row.is_in_presentation_set AS is_in_presentation_set,\n",
      "    row.is_in_span_set AS is_in_span_set,\n",
      "    row.is_in_other_inline_elements_set AS is_in_other_inline_elements_set,\n",
      "    row.is_in_images_and_objects_set AS is_in_images_and_objects_set,\n",
      "    row.is_in_forms_set AS is_in_forms_set,\n",
      "    row.is_in_tables_set AS is_in_tables_set,\n",
      "    row.is_in_frames_set AS is_in_frames_set,\n",
      "    row.is_in_historic_elements_set AS is_in_historic_elements_set,\n",
      "    row.is_in_non_standard_elements_set AS is_in_non_standard_elements_set,\n",
      "    row.header_tag AS header_tag\n",
      "MERGE (x:HeaderTags {header_tag_id: header_tag_id}) SET\n",
      "    x.is_in_document_structure_elements_set = is_in_document_structure_elements_set,\n",
      "    x.is_in_document_head_elements_set = is_in_document_head_elements_set,\n",
      "    x.is_in_document_body_elements_set = is_in_document_body_elements_set,\n",
      "    x.is_in_block_elements_set = is_in_block_elements_set,\n",
      "    x.is_in_basic_text_set = is_in_basic_text_set,\n",
      "    x.is_in_section_headings_set = is_in_section_headings_set,\n",
      "    x.is_in_lists_set = is_in_lists_set,\n",
      "    x.is_in_other_block_elements_set = is_in_other_block_elements_set,\n",
      "    x.is_in_inline_elements_set = is_in_inline_elements_set,\n",
      "    x.is_in_anchor_set = is_in_anchor_set,\n",
      "    x.is_in_phrase_elements_set = is_in_phrase_elements_set,\n",
      "    x.is_in_general_set = is_in_general_set,\n",
      "    x.is_in_computer_phrase_elements_set = is_in_computer_phrase_elements_set,\n",
      "    x.is_in_presentation_set = is_in_presentation_set,\n",
      "    x.is_in_span_set = is_in_span_set,\n",
      "    x.is_in_other_inline_elements_set = is_in_other_inline_elements_set,\n",
      "    x.is_in_images_and_objects_set = is_in_images_and_objects_set,\n",
      "    x.is_in_forms_set = is_in_forms_set,\n",
      "    x.is_in_tables_set = is_in_tables_set,\n",
      "    x.is_in_frames_set = is_in_frames_set,\n",
      "    x.is_in_historic_elements_set = is_in_historic_elements_set,\n",
      "    x.is_in_non_standard_elements_set = is_in_non_standard_elements_set,\n",
      "    x.header_tag = header_tag;\n",
      "Saving to C:\\Users\\dev\\Documents\\Repositories\\job-hunting\\saves\\csv\\HeaderTagSequence.csv\n",
      "\n",
      "LOAD CSV WITH HEADERS FROM 'file:///HeaderTagSequence.csv' AS row\n",
      "WITH\n",
      "    toInteger(row.header_tag_sequence_id) AS header_tag_sequence_id,\n",
      "    toInteger(row.file_name_id) AS file_name_id,\n",
      "    toInteger(row.header_tag_id) AS header_tag_id,\n",
      "    toInteger(row.sequence_order) AS sequence_order\n",
      "MERGE (x:HeaderTagSequence {header_tag_sequence_id: header_tag_sequence_id}) SET\n",
      "    x.file_name_id = file_name_id,\n",
      "    x.header_tag_id = header_tag_id,\n",
      "    x.sequence_order = sequence_order;\n",
      "Saving to C:\\Users\\dev\\Documents\\Repositories\\job-hunting\\saves\\csv\\MinimumRequirementsSection.csv\n",
      "\n",
      "LOAD CSV WITH HEADERS FROM 'file:///MinimumRequirementsSection.csv' AS row\n",
      "WITH\n",
      "    toInteger(row.mrs_id) AS mrs_id,\n",
      "    row.mrs_symbol AS mrs_symbol,\n",
      "    row.mrs_explanation AS mrs_explanation\n",
      "MERGE (x:MinimumRequirementsSection {mrs_id: mrs_id}) SET\n",
      "    x.mrs_symbol = mrs_symbol,\n",
      "    x.mrs_explanation = mrs_explanation;\n",
      "Saving to C:\\Users\\dev\\Documents\\Repositories\\job-hunting\\saves\\csv\\NavigableParents.csv\n",
      "\n",
      "LOAD CSV WITH HEADERS FROM 'file:///NavigableParents.csv' AS row\n",
      "WITH\n",
      "    toInteger(row.navigable_parent_id) AS navigable_parent_id,\n",
      "    toInteger(row.header_tag_id) AS header_tag_id,\n",
      "    row.navigable_parent AS navigable_parent,\n",
      "    row.is_header AS is_header,\n",
      "    row.is_task_scope AS is_task_scope,\n",
      "    row.is_minimum_qualification AS is_minimum_qualification,\n",
      "    row.is_preferred_qualification AS is_preferred_qualification,\n",
      "    row.is_legal_notification AS is_legal_notification,\n",
      "    row.is_job_title AS is_job_title,\n",
      "    row.is_office_location AS is_office_location,\n",
      "    row.is_job_duration AS is_job_duration,\n",
      "    row.is_supplemental_pay AS is_supplemental_pay,\n",
      "    row.is_educational_requirement AS is_educational_requirement,\n",
      "    row.is_interview_procedure AS is_interview_procedure,\n",
      "    row.is_corporate_scope AS is_corporate_scope,\n",
      "    row.is_posting_date AS is_posting_date,\n",
      "    row.is_other AS is_other,\n",
      "    row.is_qualification AS is_qualification\n",
      "MERGE (x:NavigableParents {navigable_parent_id: navigable_parent_id}) SET\n",
      "    x.header_tag_id = header_tag_id,\n",
      "    x.navigable_parent = navigable_parent,\n",
      "    x.is_header = is_header,\n",
      "    x.is_task_scope = is_task_scope,\n",
      "    x.is_minimum_qualification = is_minimum_qualification,\n",
      "    x.is_preferred_qualification = is_preferred_qualification,\n",
      "    x.is_legal_notification = is_legal_notification,\n",
      "    x.is_job_title = is_job_title,\n",
      "    x.is_office_location = is_office_location,\n",
      "    x.is_job_duration = is_job_duration,\n",
      "    x.is_supplemental_pay = is_supplemental_pay,\n",
      "    x.is_educational_requirement = is_educational_requirement,\n",
      "    x.is_interview_procedure = is_interview_procedure,\n",
      "    x.is_corporate_scope = is_corporate_scope,\n",
      "    x.is_posting_date = is_posting_date,\n",
      "    x.is_other = is_other,\n",
      "    x.is_qualification = is_qualification;\n",
      "Saving to C:\\Users\\dev\\Documents\\Repositories\\job-hunting\\saves\\csv\\NavigableParentSequence.csv\n",
      "\n",
      "LOAD CSV WITH HEADERS FROM 'file:///NavigableParentSequence.csv' AS row\n",
      "WITH\n",
      "    toInteger(row.navigable_parent_sequence_id) AS navigable_parent_sequence_id,\n",
      "    toInteger(row.file_name_id) AS file_name_id,\n",
      "    toInteger(row.navigable_parent_id) AS navigable_parent_id,\n",
      "    toInteger(row.sequence_order) AS sequence_order,\n",
      "    toInteger(row.mrs_id) AS mrs_id\n",
      "MERGE (x:NavigableParentSequence {navigable_parent_sequence_id: navigable_parent_sequence_id}) SET\n",
      "    x.file_name_id = file_name_id,\n",
      "    x.navigable_parent_id = navigable_parent_id,\n",
      "    x.sequence_order = sequence_order,\n",
      "    x.mrs_id = mrs_id;\n",
      "Saving to C:\\Users\\dev\\Documents\\Repositories\\job-hunting\\saves\\csv\\PartsOfSpeech.csv\n",
      "\n",
      "LOAD CSV WITH HEADERS FROM 'file:///PartsOfSpeech.csv' AS row\n",
      "WITH\n",
      "    toInteger(row.pos_id) AS pos_id,\n",
      "    row.is_header AS is_header,\n",
      "    row.is_task_scope AS is_task_scope,\n",
      "    row.is_minimum_qualification AS is_minimum_qualification,\n",
      "    row.is_preferred_qualification AS is_preferred_qualification,\n",
      "    row.is_legal_notification AS is_legal_notification,\n",
      "    row.is_job_title AS is_job_title,\n",
      "    row.is_office_location AS is_office_location,\n",
      "    row.is_job_duration AS is_job_duration,\n",
      "    row.is_supplemental_pay AS is_supplemental_pay,\n",
      "    row.is_educational_requirement AS is_educational_requirement,\n",
      "    row.is_interview_procedure AS is_interview_procedure,\n",
      "    row.is_corporate_scope AS is_corporate_scope,\n",
      "    row.is_posting_date AS is_posting_date,\n",
      "    row.is_other AS is_other,\n",
      "    row.pos_symbol AS pos_symbol,\n",
      "    row.pos_explanation AS pos_explanation\n",
      "MERGE (x:PartsOfSpeech {pos_id: pos_id}) SET\n",
      "    x.is_header = is_header,\n",
      "    x.is_task_scope = is_task_scope,\n",
      "    x.is_minimum_qualification = is_minimum_qualification,\n",
      "    x.is_preferred_qualification = is_preferred_qualification,\n",
      "    x.is_legal_notification = is_legal_notification,\n",
      "    x.is_job_title = is_job_title,\n",
      "    x.is_office_location = is_office_location,\n",
      "    x.is_job_duration = is_job_duration,\n",
      "    x.is_supplemental_pay = is_supplemental_pay,\n",
      "    x.is_educational_requirement = is_educational_requirement,\n",
      "    x.is_interview_procedure = is_interview_procedure,\n",
      "    x.is_corporate_scope = is_corporate_scope,\n",
      "    x.is_posting_date = is_posting_date,\n",
      "    x.is_other = is_other,\n",
      "    x.pos_symbol = pos_symbol,\n",
      "    x.pos_explanation = pos_explanation;\n"
     ]
    }
   ],
   "source": [
    "\n",
    "with driver.session() as session:\n",
    "    session.write_transaction(clear_database)\n",
    "for row_index, row_series in pks_df.iterrows():\n",
    "    class_name = row_series.table_name\n",
    "    with driver.session() as session:\n",
    "        session.write_transaction(create_constraint, class_name)\n",
    "    idx_column = row_series.columns\n",
    "    sql_str = 'SELECT * FROM [Jobhunting].[dbo].[{}];'.format(class_name)\n",
    "    table_df = pd.DataFrame(su.get_execution_results(CURSOR, sql_str, verbose=False))\n",
    "    column_descriptions_df = get_column_descriptions(table_df)\n",
    "    mask_series = (column_descriptions_df.dtype == 'object')\n",
    "    for column_name in column_descriptions_df[mask_series].column_name.tolist():\n",
    "        table_df[column_name] = table_df[column_name].map(f)\n",
    "    s.save_dataframes(**{class_name: table_df})\n",
    "    csv_name = f'{class_name}.csv'\n",
    "    src = os.path.join(csv_dir, csv_name)\n",
    "    dst = os.path.join(import_dir, csv_name)\n",
    "    copyfile(src, dst)\n",
    "    formatted_tx_str = \"\\nLOAD CSV WITH HEADERS FROM 'file:///{}' AS row\\nWITH\\n    {{}}\".format(csv_name)\n",
    "    formatted_tx_str += '\\nMERGE (x:{} {{{{{}: {}}}}}) SET'.format(class_name, idx_column, idx_column)\n",
    "    formatted_tx_str += '\\n    {};'\n",
    "    tx_str = get_tx_str(column_descriptions_df, idx_column, formatted_tx_str)\n",
    "    print(tx_str)\n",
    "    with driver.session() as session:\n",
    "        session.write_transaction(do_cypher_tx, tx_str)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "demographic-screw",
   "metadata": {},
   "source": [
    "\n",
    "## Get Foreign Keys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "sunset-program",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "columns_list = ['FKTABLE_NAME', 'FKCOLUMN_NAME']\n",
    "formatted_sql_str = '''\n",
    "EXEC sp_fkeys @pktable_name = '{}', @pktable_owner = 'dbo';'''\n",
    "rows_list = []\n",
    "for pkrow_index, pkrow_series in pks_df.iterrows():\n",
    "    pktable_name = pkrow_series.table_name\n",
    "    fks_df = pd.DataFrame(su.get_execution_results(CURSOR, formatted_sql_str.format(pktable_name)))\n",
    "    for fkrow_index, fkrow_series in fks_df.iterrows():\n",
    "        row_dict = {}\n",
    "        row_dict['pktable_name'] = pktable_name\n",
    "        fktable_name = fkrow_series.FKTABLE_NAME\n",
    "        row_dict['fktable_name'] = fktable_name\n",
    "        fkcolumn_name = fkrow_series.FKCOLUMN_NAME\n",
    "        row_dict['fkcolumn_name'] = fkcolumn_name\n",
    "        rows_list.append(row_dict)\n",
    "fks_df = pd.DataFrame(rows_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "nutritional-house",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "tx_str = \"\"\"\n",
      "MATCH (a:FileNames), (b:HeaderTagSequence) \n",
      "   WHERE a.file_name_id = b.file_name_id\n",
      "CREATE (a)-[r: Relation]->(b);\"\"\"\n",
      "with driver.session() as session:\n",
      "    session.write_transaction(do_cypher_tx, tx_str)\n",
      "\n",
      "tx_str = \"\"\"\n",
      "MATCH (a:FileNames), (b:NavigableParentSequence) \n",
      "   WHERE a.file_name_id = b.file_name_id\n",
      "CREATE (a)-[r: Relation]->(b);\"\"\"\n",
      "with driver.session() as session:\n",
      "    session.write_transaction(do_cypher_tx, tx_str)\n",
      "\n",
      "tx_str = \"\"\"\n",
      "MATCH (a:HeaderTags), (b:HeaderTagSequence) \n",
      "   WHERE a.header_tag_id = b.header_tag_id\n",
      "CREATE (a)-[r: Relation]->(b);\"\"\"\n",
      "with driver.session() as session:\n",
      "    session.write_transaction(do_cypher_tx, tx_str)\n",
      "\n",
      "tx_str = \"\"\"\n",
      "MATCH (a:HeaderTags), (b:NavigableParents) \n",
      "   WHERE a.header_tag_id = b.header_tag_id\n",
      "CREATE (a)-[r: Relation]->(b);\"\"\"\n",
      "with driver.session() as session:\n",
      "    session.write_transaction(do_cypher_tx, tx_str)\n",
      "\n",
      "tx_str = \"\"\"\n",
      "MATCH (a:MinimumRequirementsSection), (b:NavigableParentSequence) \n",
      "   WHERE a.mrs_id = b.mrs_id\n",
      "CREATE (a)-[r: Relation]->(b);\"\"\"\n",
      "with driver.session() as session:\n",
      "    session.write_transaction(do_cypher_tx, tx_str)\n",
      "\n",
      "tx_str = \"\"\"\n",
      "MATCH (a:NavigableParents), (b:NavigableParentSequence) \n",
      "   WHERE a.navigable_parent_id = b.navigable_parent_id\n",
      "CREATE (a)-[r: Relation]->(b);\"\"\"\n",
      "with driver.session() as session:\n",
      "    session.write_transaction(do_cypher_tx, tx_str)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "formatted_tx_str = '''\n",
    "MATCH (a:{}), (b:{}) \n",
    "   WHERE a.{} = b.{}\n",
    "CREATE (a)-[r: Relation]->(b);'''\n",
    "write_str = '''\n",
    "with driver.session() as session:\n",
    "    session.write_transaction(do_cypher_tx, tx_str)'''\n",
    "for fkrow_index, fkrow_series in fks_df.iterrows():\n",
    "    pktable_name = fkrow_series.pktable_name\n",
    "    fktable_name = fkrow_series.fktable_name\n",
    "    fkcolumn_name = fkrow_series.fkcolumn_name\n",
    "    print('\\ntx_str = ', end='\"\"\"')\n",
    "    print(formatted_tx_str.format(pktable_name, fktable_name, fkcolumn_name, fkcolumn_name), end='\"\"\"')\n",
    "    print(write_str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "complete-madison",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "tx_str = \"\"\"\n",
    "MATCH (a:FileNames), (b:HeaderTagSequence) \n",
    "   WHERE a.file_name_id = b.file_name_id\n",
    "CREATE (b)-[r: IS_CONTAINED_IN]->(a);\"\"\"\n",
    "with driver.session() as session:\n",
    "    session.write_transaction(do_cypher_tx, tx_str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "trying-content",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "tx_str = \"\"\"\n",
    "MATCH (a:FileNames), (b:NavigableParentSequence) \n",
    "   WHERE a.file_name_id = b.file_name_id\n",
    "CREATE (b)-[r: IS_CONTAINED_IN]->(a);\"\"\"\n",
    "with driver.session() as session:\n",
    "    session.write_transaction(do_cypher_tx, tx_str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "intensive-strengthening",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "tx_str = \"\"\"\n",
    "MATCH (a:HeaderTags), (b:HeaderTagSequence) \n",
    "   WHERE a.header_tag_id = b.header_tag_id\n",
    "CREATE (a)-[r: IS_PART_OF]->(b);\"\"\"\n",
    "with driver.session() as session:\n",
    "    session.write_transaction(do_cypher_tx, tx_str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "surprised-appointment",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "tx_str = \"\"\"\n",
    "MATCH (a:HeaderTags), (b:NavigableParents) \n",
    "   WHERE a.header_tag_id = b.header_tag_id\n",
    "CREATE (a)-[r: SUMMARIZES]->(b);\"\"\"\n",
    "with driver.session() as session:\n",
    "    session.write_transaction(do_cypher_tx, tx_str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "broad-throw",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "tx_str = \"\"\"\n",
    "MATCH (a:MinimumRequirementsSection), (b:NavigableParentSequence) \n",
    "   WHERE a.mrs_id = b.mrs_id\n",
    "CREATE (a)-[r: SUMMARIZES]->(b);\"\"\"\n",
    "with driver.session() as session:\n",
    "    session.write_transaction(do_cypher_tx, tx_str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "center-albert",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "tx_str = \"\"\"\n",
    "MATCH (a:NavigableParents), (b:NavigableParentSequence) \n",
    "   WHERE a.navigable_parent_id = b.navigable_parent_id\n",
    "CREATE (a)-[r: IS_PART_OF]->(b);\"\"\"\n",
    "with driver.session() as session:\n",
    "    session.write_transaction(do_cypher_tx, tx_str)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "still-stuart",
   "metadata": {},
   "source": [
    "\n",
    "----\n",
    "## Display Nodes and Relationships"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "loose-major",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "TX_STR = '''\n",
    "CALL {\n",
    "  MATCH (a)-[r: IS_CONTAINED_IN]->(b)\n",
    "  RETURN a, b\n",
    "  ORDER BY rand()\n",
    "  LIMIT 15\n",
    "UNION\n",
    "  MATCH (a)-[r: IS_PART_OF]->(b)\n",
    "  RETURN a, b\n",
    "  ORDER BY rand()\n",
    "  LIMIT 15\n",
    "UNION\n",
    "  MATCH (a)-[r: SUMMARIZES]->(b)\n",
    "  RETURN a, b\n",
    "  ORDER BY rand()\n",
    "  LIMIT 15\n",
    "}\n",
    "RETURN a, b;'''\n",
    "def display_nodes_tx(tx):\n",
    "    result = tx.run(TX_STR)\n",
    "    print([f'result.{fn}' for fn in dir(result) if not fn.startswith('_')])\n",
    "    record_dict = result.single()\n",
    "    \n",
    "    return record_dict\n",
    "session = driver.session()\n",
    "session.read_transaction(display_nodes_tx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "french-garbage",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "TX_STR = '''\n",
    "CALL {\n",
    "  MATCH (x:FileNames)\n",
    "  RETURN x\n",
    "  LIMIT 10\n",
    "UNION\n",
    "  MATCH (x:HeaderTags)\n",
    "  RETURN x\n",
    "  LIMIT 10\n",
    "UNION\n",
    "  MATCH (x:HeaderTagSequence)\n",
    "  RETURN x\n",
    "  LIMIT 10\n",
    "UNION\n",
    "  MATCH (x:MinimumRequirementsSection)\n",
    "  RETURN x\n",
    "  LIMIT 10\n",
    "UNION\n",
    "  MATCH (x:NavigableParents)\n",
    "  RETURN x\n",
    "  LIMIT 10\n",
    "UNION\n",
    "  MATCH (x:NavigableParentSequence)\n",
    "  RETURN x\n",
    "  LIMIT 10\n",
    "UNION\n",
    "  MATCH (x:PartsOfSpeech)\n",
    "  RETURN x\n",
    "  LIMIT 10\n",
    "}\n",
    "RETURN x;'''\n",
    "def display_nodes_tx(tx):\n",
    "    result = tx.run(TX_STR)\n",
    "    print([f'result.{fn}' for fn in dir(result) if not fn.startswith('_')])\n",
    "    record_dict = result.single()\n",
    "    \n",
    "    return record_dict\n",
    "session = driver.session()\n",
    "session.read_transaction(display_nodes_tx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acute-mozambique",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
