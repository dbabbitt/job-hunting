{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "174c9bb2-7d08-4fa2-aa2b-e3be342e52ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pretty printing has been turned OFF\n"
     ]
    }
   ],
   "source": [
    "\n",
    "%pprint"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34d2c4fa-fe66-4b71-9899-4b533c04515b",
   "metadata": {
    "tags": []
   },
   "source": [
    "\n",
    "---\n",
    "# Load needed libraries and functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "06fd23c4-353b-4800-973d-785e63a9856e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "%matplotlib inline\n",
    "from IPython.display import HTML\n",
    "from datetime import datetime\n",
    "from neo4j.exceptions import ServiceUnavailable\n",
    "import enchant\n",
    "import humanize\n",
    "import os\n",
    "import sys\n",
    "import time\n",
    "import warnings\n",
    "import winsound\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "duration = 1000  # milliseconds\n",
    "freq = 880  # Hz\n",
    "\n",
    "# Insert at 1, 0 is the script path (or '' in REPL)\n",
    "sys.path.insert(1, '../py')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e42298f7-a9cc-4d21-871b-bf2abf92b800",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======== Neo4j/4.4.7 ========\n",
      "Utility libraries created in 5 seconds\n"
     ]
    }
   ],
   "source": [
    "\n",
    "t0 = time.time()\n",
    "\n",
    "# Get the Neo4j driver\n",
    "from storage import Storage\n",
    "s = Storage(\n",
    "    data_folder_path=os.path.abspath('../data'),\n",
    "    saves_folder_path=os.path.abspath('../saves')\n",
    ")\n",
    "\n",
    "from ha_utils import HeaderAnalysis\n",
    "ha = HeaderAnalysis(s=s, verbose=False)\n",
    "\n",
    "from scrape_utils import WebScrapingUtilities\n",
    "wsu = WebScrapingUtilities(\n",
    "    s=s,\n",
    "    secrets_json_path=os.path.abspath('../data/secrets/jh_secrets.json')\n",
    ")\n",
    "uri = wsu.secrets_json['neo4j']['connect_url']\n",
    "user =  wsu.secrets_json['neo4j']['username']\n",
    "password = wsu.secrets_json['neo4j']['password']\n",
    "\n",
    "# Get the neo4j object\n",
    "from cypher_utils import CypherUtilities\n",
    "cu = CypherUtilities(\n",
    "    uri=uri, user=user, password=password, driver=None, s=s, ha=ha\n",
    ")\n",
    "\n",
    "try:\n",
    "    version_str = cu.driver.get_server_info().agent\n",
    "    print(f'======== {version_str} ========')\n",
    "except ServiceUnavailable as e:\n",
    "    print('You need to start Neo4j as a console')\n",
    "    raise\n",
    "except Exception as e:\n",
    "    print(f'{e.__class__}: {str(e).strip()}')\n",
    "\n",
    "from hc_utils import HeaderCategories\n",
    "hc = HeaderCategories(cu=cu, verbose=False)\n",
    "\n",
    "from lr_utils import LrUtilities\n",
    "lru = LrUtilities(ha=ha, cu=cu, hc=hc, verbose=False)\n",
    "\n",
    "from section_classifier_utils import SectionLRClassifierUtilities, SectionSGDClassifierUtilities, SectionCRFClassifierUtilities\n",
    "slrcu = SectionLRClassifierUtilities(ha=ha, cu=cu, verbose=False)\n",
    "ssgdcu = SectionSGDClassifierUtilities(ha=ha, cu=cu, verbose=False)\n",
    "scrfcu = SectionCRFClassifierUtilities(cu=cu, ha=ha, verbose=False)\n",
    "\n",
    "from crf_utils import CrfUtilities\n",
    "crf = CrfUtilities(ha=ha, hc=hc, cu=cu, lru=lru, slrcu=slrcu, verbose=True)\n",
    "\n",
    "from section_utils import SectionUtilities\n",
    "su = SectionUtilities(wsu=wsu, crf=crf, verbose=False)\n",
    "\n",
    "duration_str = humanize.precisedelta(time.time() - t0, minimum_unit='seconds', format='%0.0f')\n",
    "print(f'Utility libraries created in {duration_str}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8dbe7bc3-7ab4-4950-a7a1-04ebe419187a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I have 49,087 hand-labeled header htmls prepared\n",
      "7 iterations seen during training fit for a total of 49,087 records trained\n",
      "Is-header classifier trained in 6 seconds\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Train the isheader classifier\n",
    "t1 = time.time()\n",
    "from is_header_sgd_classifier import IsHeaderSgdClassifier\n",
    "ihu = IsHeaderSgdClassifier(ha=ha, cu=cu, verbose=False)\n",
    "ihu.build_pos_stochastic_gradient_descent_elements(verbose=True)\n",
    "duration_str = humanize.precisedelta(time.time() - t1, minimum_unit='seconds', format='%0.0f')\n",
    "print(f'Is-header classifier trained in {duration_str}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d91632d9-a9f4-43a3-b76b-4bdc57736c0f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I have 49,102 labeled parts of speech in here\n",
      "Parts-of-speech SGD classifier trained in 8 seconds\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Train the POS SGD classifier\n",
    "t1 = time.time()\n",
    "if not hasattr(ssgdcu, 'pos_predict_percent_fit_dict'):\n",
    "    ssgdcu.build_pos_stochastic_gradient_descent_elements(verbose=True)\n",
    "duration_str = humanize.precisedelta(time.time() - t1, minimum_unit='seconds', format='%0.0f')\n",
    "print(f'Parts-of-speech SGD classifier trained in {duration_str}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "551caae7-a91c-44fd-a2a8-fcd00dd454e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I have 424,879 is-qualified vocabulary tokens in here\n",
      "Is-qualified LR elements built in 4 seconds\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Check if the lru has built its is-qualified classifier\n",
    "t0 = time.time()\n",
    "if not hasattr(lru, 'ISQUALIFIED_LR'):\n",
    "    lru.build_isqualified_logistic_regression_elements(sampling_strategy_limit=5_000, verbose=True)\n",
    "duration_str = humanize.precisedelta(time.time() - t0, minimum_unit='seconds', format='%0.0f')\n",
    "print(f'Is-qualified LR elements built in {duration_str}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2d8dcb74-e13b-4f85-826e-4b9053f53c64",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parts-of-speech CRF classifier trained in 1 second\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Train the POS CRF classifier\n",
    "t1 = time.time()\n",
    "if not hasattr(scrfcu, 'pos_symbol_crf'):\n",
    "    scrfcu.build_pos_conditional_random_field_elements(verbose=True)\n",
    "duration_str = humanize.precisedelta(time.time() - t1, minimum_unit='seconds', format='%0.0f')\n",
    "print(f'Parts-of-speech CRF classifier trained in {duration_str}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "52458533-60c4-4597-9f55-dcec3d328504",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Seek a SectionLRClassifierUtilities object\n",
    "if hasattr(slrcu, 'pos_predict_percent_fit_dict'):\n",
    "    pos_lr_predict_single = slrcu.predict_single\n",
    "elif crf.is_flask_running():\n",
    "    pos_lr_predict_single = crf.get_pos_lr_predict_single_from_api\n",
    "else:\n",
    "    pos_lr_predict_single = None\n",
    "\n",
    "# Seek a SectionCRFClassifierUtilities object\n",
    "if hasattr(scrfcu, 'pos_symbol_crf'):\n",
    "    pos_crf_predict_single = scrfcu.predict_single\n",
    "elif crf.is_flask_running():\n",
    "    pos_crf_predict_single = crf.get_pos_crf_predict_single_from_api\n",
    "else:\n",
    "    pos_crf_predict_single = None\n",
    "\n",
    "# Seek a SectionSGDClassifierUtilities object\n",
    "if hasattr(ssgdcu, 'pos_predict_percent_fit_dict'):\n",
    "    pos_sgd_predict_single = ssgdcu.predict_single\n",
    "elif crf.is_flask_running():\n",
    "    pos_sgd_predict_single = crf.get_pos_sgd_predict_single_from_api\n",
    "else:\n",
    "    pos_sgd_predict_single = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "01e7d031-531c-4024-b9f4-03485a91a943",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "predict_single is now available\n",
      "POS classifier trained in 0 seconds\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Check if the crf has built its parts-of-speech classifier\n",
    "# POS classifier trained in 12 hours, 15 minutes and 36 seconds\n",
    "t1 = time.time()\n",
    "if not hasattr(crf, 'CRF'):\n",
    "    crf.build_pos_conditional_random_field_elements(verbose=True)\n",
    "if hasattr(crf, 'CRF'):\n",
    "    print('predict_single is now available')\n",
    "else:\n",
    "    print('predict_single is not available')\n",
    "duration_str = humanize.precisedelta(time.time() - t1, minimum_unit='seconds', format='%0.0f')\n",
    "print(f'POS classifier trained in {duration_str}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d5d5f01c-6342-4c46-9d52-a3608cc29ba5",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "file_name = '20231130141017677230_100_Remote_Opportunity_for_Data_Scientist_Sr_Satyam_Kumar_Pandey.html'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f7248bd-9c02-48f1-8bd2-e1028b202405",
   "metadata": {},
   "source": [
    "\n",
    "---\n",
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7ed87db8-7c12-48c4-b2c0-932cf9333f59",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I have 13,281 hand-labeled qualification strings in here\n",
      "I have 440,750 is-qualified vocabulary tokens in here\n",
      "Is-qualified classifer retrained in 7 seconds\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# You need to run this again if you changed the qualification dictionary in another notebook\n",
    "t0 = time.time()\n",
    "lru.sync_basic_quals_dict(sampling_strategy_limit=None, verbose=False)\n",
    "lru.retrain_isqualified_classifier(verbose=True)\n",
    "duration_str = humanize.precisedelta(time.time() - t0, minimum_unit='seconds', format='%0.0f')\n",
    "print(f'Is-qualified classifer retrained in {duration_str}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae75c495-79a9-4475-ad97-46ae6d42a294",
   "metadata": {},
   "source": [
    "\n",
    "----\n",
    "# Prepare cover sheet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "265ddacf-68eb-49fd-8b0b-1b78afceaa61",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<p>I only meet 100.0% of the minimum requirements for the 20231130141017677230 100 Remote Opportunity for Data Scientist Sr position, but I can explain:</p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"font-family:Century Gothic,sans-serif\">1) No west coast candidates</span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"font-family:Century Gothic,sans-serif\">2) None</span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "3) • 5+ years of experience as a Data Scientist – Proficiency in data science modeling (AI, Machine Learning, Deep Learning, Decision Trees, Random Forest, Neural Networks, Supervised / Unsupervised Learning, Forecasting, Predictive Modeling and Clustering)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "4) • Experience with machine learning methods like k-nearest neighbors, random forests, and ensemble methods."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "5) • Experience using Python Machine Learning & Data Pre-processing Libraries. (Scikit Learn, Numpy, Pandas)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "6) • Python and PySpark scripting experience"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"font-family:Century Gothic,sans-serif\">7) Bachelor's degree, an additional 4 years of relevant work experience is required</span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Qualifications shown in 4 seconds\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Show what qualifications you have for this posting\n",
    "t0 = time.time()\n",
    "ask_str = ''\n",
    "child_strs_list = ha.get_child_strs_from_file(file_name=file_name)\n",
    "\n",
    "feature_tuple_list = []\n",
    "for feature_dict in cu.get_feature_dict_list(ha.get_child_tags_list(child_strs_list), child_strs_list):\n",
    "    feature_tuple_list.append(hc.get_feature_tuple(\n",
    "        feature_dict, pos_lr_predict_single=pos_lr_predict_single, pos_crf_predict_single=pos_crf_predict_single,\n",
    "        pos_sgd_predict_single=pos_sgd_predict_single\n",
    "    ))\n",
    "\n",
    "crf_list = crf.CRF.predict_single(crf.sent2features(feature_tuple_list))\n",
    "indices_list = su.find_basic_quals_section_indexes(child_strs_list=child_strs_list, crf_list=crf_list, file_name=file_name)\n",
    "quals_list = [child_str for i, child_str in enumerate(child_strs_list) if i in indices_list]\n",
    "prediction_list = list(lru.predict_job_hunt_percent_fit(quals_list))\n",
    "_, qual_count = lru.get_quals_str(prediction_list, quals_list)\n",
    "job_fitness = qual_count/len(prediction_list)\n",
    "d = enchant.Dict('en_US')\n",
    "job_title = ' '.join([w for w in file_name.replace('.html', '').replace('_Indeed_com', '').split('_') if d.check(w)])\n",
    "met_str = f'<p>I only meet {job_fitness:.1%} of the minimum requirements for the {job_title} position, but I can explain:</p>'\n",
    "ask_str += met_str\n",
    "display(HTML(met_str))\n",
    "for i, qual_str in enumerate(quals_list):\n",
    "    if qual_str in lru.basic_quals_dict:\n",
    "        if lru.basic_quals_dict[qual_str]:\n",
    "            met_str = f'{i+1}) {qual_str}'\n",
    "            ask_str += ' ' + met_str\n",
    "            idx = qual_str.find('>')\n",
    "            if idx == -1:\n",
    "                display(HTML(met_str))\n",
    "            else:\n",
    "                display(HTML(f'{qual_str[:idx+1]}{i+1}) {qual_str[idx+1:]}'))\n",
    "duration_str = humanize.precisedelta(time.time() - t0, minimum_unit='seconds', format='%0.0f')\n",
    "print(f'Qualifications shown in {duration_str}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "0ef7b2a4-d443-4cdd-86ad-a7f4938d4f34",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<p>The minimum requirements that I don't meet are:</p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "unmet_str = \"<p>The minimum requirements that I don't meet are:</p>\"\n",
    "display(HTML(unmet_str))\n",
    "for i, qual_str in enumerate(quals_list):\n",
    "    if (qual_str not in lru.basic_quals_dict) or not lru.basic_quals_dict[qual_str]:\n",
    "        met_str = f'{i+1}) {qual_str}'\n",
    "        unmet_str += ' ' + met_str\n",
    "        idx = qual_str.find('>')\n",
    "        if idx == -1:\n",
    "            display(HTML(met_str))\n",
    "        else:\n",
    "            display(HTML(f'{qual_str[:idx+1]}{i+1}) {qual_str[idx+1:]}'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d2d705f1-6522-416e-a223-682fdfd6e66a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<p>The preferred requirements that I meet are:</p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "1) • Prior healthcare payer experience"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "# This doesn't work unless you score all the O-PQs\n",
    "db_pos_list = []\n",
    "for navigable_parent in child_strs_list:\n",
    "    db_pos_list = cu.append_parts_of_speech_list(navigable_parent, pos_list=db_pos_list)\n",
    "pos_list = []\n",
    "for i, (crf_symbol, db_symbol) in enumerate(zip(crf_list, db_pos_list)):\n",
    "    if db_symbol in [None, 'O', 'H']:\n",
    "        pos_list.append(crf_symbol)\n",
    "    else:\n",
    "        pos_list.append(db_symbol)\n",
    "met_str = f\"<p>The preferred requirements that I meet are:</p>\"\n",
    "display(HTML(met_str))\n",
    "min_str = ''\n",
    "pqs_list = [child_str for pos_str, child_str in zip(pos_list, child_strs_list) if (pos_str in ['O-PQ'])]\n",
    "for i, qual_str in enumerate(pqs_list):\n",
    "    if qual_str in lru.basic_quals_dict:\n",
    "        if lru.basic_quals_dict[qual_str]:\n",
    "            pref_str = f'{i+1}) {qual_str}'\n",
    "            min_str += ' ' + pref_str\n",
    "            idx = qual_str.find('>')\n",
    "            if idx == -1:\n",
    "                display(HTML(pref_str))\n",
    "            else:\n",
    "                display(HTML(f'{qual_str[:idx+1]}{i+1}) {qual_str[idx+1:]}'))\n",
    "if min_str:\n",
    "    ask_str += met_str + min_str\n",
    "winsound.Beep(freq, duration)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a271b3e8-1d86-46a8-aa8e-3e9d519a5eeb",
   "metadata": {},
   "source": [
    "\n",
    "----\n",
    "# Write cover sheet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "087a9de1-5276-415b-8ee6-c6d249c26189",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Explain in first person singular why I would be interested in this role at <span style=\"font-family:Century Gothic,sans-serif\">Data Scientist Sr</span>, given\n",
      "\n",
      "1) this information about the task scope:\n",
      "\n",
      "CareFirst is seeking a Data Scientist to support the Machine Learning component of its AuditIQ Product which helps proactively identify claim overpayments prior to FEPOC’s annual OIG (Office of Inspector General) Audits. C <strong>areFirst is</strong> <span style=\"font-family:Century Gothic,sans-serif\">NOT</span> <span style=\"font-family:Century Gothic,sans-serif\">looking for a data science researcher. They are seeking a resource that can assist with the ingestion of data along with the deployment and production of machine learning scripts.</span> •Identifies and solves business problems by using various numerical techniques, algorithms, and models in statistical modeling, machine learning, operations research, and data mining. •Uses advanced analytical capabilities to support data science initiatives. •Communicates across product teams and with customers and educates on artificial intelligence, machine learning, and statistical models. •Acting as a liaison and interface between analytics, business units and other departments.\n",
      "\n",
      "and, 2) my resume:\n",
      "\n",
      "\n",
      "Dave Babbitt (413) 244-9949 dave.babbitt@gmail.com\n",
      "Residence: Clinton, MA\n",
      "\n",
      "COMPETENCIES\n",
      "* Large Dataset Analysis (4 years)\n",
      "* SME Collaboration (21 years)\n",
      "* Hidden Insight Discovery (6+ years)\n",
      "* Complex Problem Solving (21+ years)\n",
      "* Disparate Data Exploration (5+ years)\n",
      "* Data Analysis (11+ years)\n",
      "* Text Extraction and Processing (6+ years)\n",
      "* New programming language mastery (21 years)\n",
      "* Simulation Development (3 years)\n",
      "* Web App/AI Development (21 years)\n",
      "* Strong desire to learn new skills\n",
      "* Excellent work ethic\n",
      "* Advanced programming skills in R or Python\n",
      "* Good programming practices\n",
      "* Adherence to code style\n",
      "* Working knowledge of version-control tools\n",
      "* Statistical inferences (6 years)\n",
      "* Time-series analysis (3 years)\n",
      "* Statistical simulation (6 years)\n",
      "* Building machine learning models (6 years)\n",
      "\n",
      "\n",
      "CERTIFICATIONS\n",
      "\n",
      "Machine Learning (2017)\n",
      "Security+ (2020)\n",
      "\n",
      "\n",
      "SUMMARY OF SKILLS, TOOLS, AND TECHNOLOGIES:\n",
      "\n",
      "Operating Systems: Linux (8 years), Windows (20 years), Mac (8 years)\n",
      "Statistics/ML: Regression (8 years), Clustering (7 years), Decision Trees (8 years), Neural Networks (4 years)\n",
      "Programming Languages: Python/TensorFlow/Keras (6/2/1 years), Regex (21 years), Java/Groovy/JUnit (3 years), MATLAB/Octave (3 years), R (5 years), SQL (15 years), JavaScript (20 years), HTML/CSS/XML/XSLT (21 years), VBA (3 years), C# (1 year), CFML/CFScript (15 years), Flash (1 year), SAS (1 year)\n",
      "Web Services: AWS/S3 (3 years), Machine Shop (1 year)\n",
      "Data Collection: Server-side Web APIs (11 years), Web Crawling (21 years)\n",
      "Querying: PostgreSQL (2 years), MySQL (2 years), MSSQL (15 years), MongoDB (2 years), Neo4j (1 year)\n",
      "Web Traffic Analysis: Google Analytics (1 year)\n",
      "Visualization: Jupyter Notebooks (5 years), matplotlib (5 years), ggplot2 (4 years), Excel (20 years), PowerPoint (3 years)\n",
      "Other: Eclipse (2 years), RStudio (2 years), Visual Studio 2013 (1 year), Fireworks (5 years), RegexBuddy (10 years), Source Tree/Git (5 years), JIRA (3 years), Subversion/SVN (2 years)\n",
      "\n",
      "EXPERIENCE DETAILS\n",
      "\tBigBear.ai, Machine Learning Engineer \t03/2023 - Present\n",
      "* Conducted a literature search on research done so far on aligning AI models to human decision-making processes. (Google Scholar, NLP)\n",
      "* Wrote code that bleeps out identifiers from an audio stream using either a local LLM model or a NER model for PII masking. (LlamaIndex, Hugging Face, Cohere, GPT4All)\n",
      "* Created various conda environments to preserve the kernel versioning. (Python, PowerShell)\n",
      "* Created a python function that accepts a natural language response to an open-ended question prompt (the probe) and outputs a low dimension representation of that text using a BERT based model, and made it configurable, so later if needed we could switch to LLMs.\n",
      "* Created a Flask API that exposed this; created a pytest scaffolding for the python function and Flask API; created PowerShell scripts that ran the Flask server and the tests.\n",
      "* Extracted key frames from a video and wrote some code to identify various objects in the frames in order to describe the scene. (Ffmpeg, PySceneDetect, PowerShell)\n",
      "* Conducted technical diligence on the latest AI techniques emerging from academia, industry, and the open-source machine learning community.\n",
      "\n",
      "\tAccenture, Inc., Senior Data Scientist \t09/2022 - 03/2023\n",
      "Meta, Menlo Park, CA\n",
      "* Developed a pipeline to evaluate a lead KPI indicator, including testing model assumptions, model feature engineering, model regression training, and model prediction visualizations. (SQL, Bento, Python)\n",
      "\n",
      "\tIT Concepts, Inc., Senior Data Scientist \t05/2021 - 03/2022\n",
      "Kessel Run (Det 12), Boston, MA\n",
      "* Developed a system for retrieving GitLab and Pivotal Tracker issue metrics, aggregating them, and uploading them to Gdrive sheets via Excel workbooks and the Elasticsearch API. (GitLab, Excel, Pivotal Tracker, Python)\n",
      "\n",
      "\tPlateau Software, Inc., Data Scientist \t12/2020 - 05/2021\n",
      "USTRANSOM, Scott Air Force Base, IL\n",
      "* Determined the \"hits\" where a satellite had geodetic overlap with any vessel(s) at any point(s) in time. (For simplicity, used a standard Geodetic Overlap Accuracy model). (Python)\n",
      "* Programmatically instantiated resources, performed data munging, uploaded to database, sent JSON data to UI to display hits. (AWS SAM/S3/Lambda, Docker, Python)\n",
      "\n",
      "Major Accomplishments:\n",
      "See Space Track Scenario on GitHub (https://github.com/PlateauInc/space-track-scenario).\n",
      "\n",
      "\tBooz Allen Hamilton, Inc., Data Scientist \t7/2020 - 10/2020\n",
      "US Army SOCOM, Fort Bragg, NC\n",
      "* Processed structured, unstructured, and semi-structured data to derive patterns, trends, and correlations and enable entity extraction, disambiguation and other data-related support using appropriate data science tools and techniques. (Groovy, Python)\n",
      "* Employed techniques and theories drawn from many fields within the broad areas of mathematics, statistics, operations research, information science, and computer science, including signal processing, probability models, machine learning, statistical learning, pattern recognition and learning, predictive analytics, uncertainty modeling, and artificial intelligence. (R, Python)\n",
      "\n",
      "Major Accomplishments:\n",
      "Jira Importer/Listener in 2020.\n",
      "\n",
      "\tBooz Allen Hamilton, Inc., Cloud Data Scientist \t1/2018 - 6/2020\n",
      "Hanscom AFB, MA\n",
      "* Analyzed large data sets and develop automated analytics to assist decision makers with making sense of data affecting DoD operations (R, Python, Keras, TensorFlow)\n",
      "* Developed machine learning, data mining, statistical, and graph-based algorithms to analyze massive data sets. Implement analytical approaches that require knowledge of statistical programming languages, including SAS, R, Java, or Python (R, Python)\n",
      "* Developed scripts that extract information from a variety of data formats, including SQL tables, structured metadata, and network logs (Python)\n",
      "* Employed process improvements and reengineering methodologies to support the modernization of IT systems (PowerShell, Bash, Docker)\n",
      "* Produced data visualizations that provide insight into data set structure and meaning (Jupyter)\n",
      "* Developed and implemented statistical, machine-learning, and heuristic techniques to create descriptive, predictive, and prescriptive analytics (R, Python)\n",
      "\n",
      "Major Accomplishments:\n",
      "Battle Damage Assessment Model (STARFADE) in 2018, Special Telemetry Analysis Resource Toolkit (STARK) in 2018, DIUx xView Challenge (BRANNDON) in 2018, and DRACARUS in 2018.\n",
      "\n",
      "\tBooz Allen Hamilton, Inc., Data Scientist \t1/2015 - 12/2017\n",
      "25th Air Force, Tactical ISR Training, Offutt AFB, NE\n",
      "* Developed data science competencies for intelligence analysts supporting US Air Force airborne intelligence, surveillance, and reconnaissance systems and analytical processes (R, Python, MongoDB)\n",
      "* Reviewed data processing issues and devise customized, repeatable, and scalable methods for data manipulation about follow-on analysis (R, Python, MongoDB)\n",
      "* Conducted research and nominate unclassified/open information sources for data mining to assist current analytical initiatives\n",
      "* Programmed logical interfaces and apply techniques for effective program logic and data manipulation\n",
      "* Oversaw systems programming and support functions, including new/revised segments of language code and processing\n",
      "* Facilitated documentation of developed processes for data extraction, manipulation, analysis, and storage (Jupyter, Word)\n",
      "* Provided training for intelligence analysts on programming and mathematical aspects of data science and implementation of developed processes (PowerPoint)\n",
      "\n",
      "Major Accomplishments:\n",
      "Aircrew Training Data Analysis in 2017, Crewmember Cluster Model in 2015/16, and Reports to KML Tool in 2015.\n",
      "\tNATO ACT C-IED IPT, Web Developer \t2012 - 2015\n",
      "Norfolk, VA.\n",
      "* Interfaced with the 8-nation C-IED COE in Madrid to migrate the hosting of C-IED.ORG out of the NCIA in Norfolk\n",
      "* Devised a NATO UNCLASS system to upload sanitized documents that cannot be accessed by unregistered users\n",
      "* Established the registration and reporting systems for different global conferences\n",
      "\n",
      "\tCIDNE DB, Software Engineer \t2010 - 2012\n",
      "* Created a CF templating system that also looks for and operates localized cleanup activities\n",
      "* Managed automatic testing\n",
      "* Devised a system to record duplicate CSS IDs in reports\n",
      "* Improved the GUI of different reports to tactfully display information updated on the server (ColdFusion)\n",
      "\tKnights of Columbus, ColdFusion Technical Lead \t2006 - 2009\n",
      "* Developed a system that created and emailed multiple PDFs and Word Documents in different Latin encodings\n",
      "* Transformed different ColdFusion apps into Java Servlets while still adding new features to the kofc.org website\n",
      "\tClipboard Solutions, Senior Systems Analyst \t2000 - 2006\n",
      "* Designed an efficient health care payroll system\n",
      "* Constructed a large-scale time entry and scheduling system that includes mobile and biometric devices\n",
      "\n",
      "Major Accomplishments:\n",
      "\"Save As Template\" Tool in 2012, Multi-language Email Generator in 2008/09, and Payroll System in 2003 (maintained until 2013).\n",
      "Education\n",
      "MS Predictive Analytics, Northwestern University, 2015. \"Agent-Based Modeling of Peer-to-Peer\n",
      "Economic Systems\" (Master's Thesis)\n",
      "BS Liberal Arts, Excelsior College, 1991\n"
     ]
    }
   ],
   "source": [
    "\n",
    "topic = 'interested'\n",
    "if topic == 'recruiter':\n",
    "    recruiter_name = 'Joseph St.Denis'\n",
    "    youchat_str = f\"Reply to the recruiter email that you don't meet {1-job_fitness:.1%} of the requirements\"\n",
    "    youchat_str += f\" ({unmet_str}), though you do meet these criterion: {ask_str} and am interested in applying for the job.\"\n",
    "    #and have applied for the job.\n",
    "    youchat_str += f\" (Replace [Your Name] with Dave Babbitt, Replace [Recruiter] with {recruiter_name})\"\n",
    "elif topic == 'cover':\n",
    "    import pandas as pd\n",
    "    cypher_str = f\"\"\"\n",
    "        MATCH (fn:FileNames {{file_name: \"{file_name}\"}})\n",
    "        RETURN\n",
    "            fn.role_primary_contact AS role_primary_contact,\n",
    "            fn.role_primary_contact_email_id AS role_primary_contact_email_id,\n",
    "            fn.role_title AS role_title\n",
    "        ORDER BY fn.percent_fit DESC;\"\"\"\n",
    "    cover_df = pd.DataFrame(cu.get_execution_results(cypher_str, verbose=False))\n",
    "    recruiter_name = cover_df.role_primary_contact.squeeze()\n",
    "    email_address = cover_df.role_primary_contact_email_id.squeeze()\n",
    "    role_title = cover_df.role_title.squeeze()\n",
    "    if (recruiter_name is None) or (email_address is None):\n",
    "        suffix_str = ''\n",
    "    else:\n",
    "        suffix_str = f' to \"{recruiter_name}\" <{email_address}>'\n",
    "    youchat_str = f\"Write a cover letter email{suffix_str}, complete with subject, using this information:\"\n",
    "    youchat_str += f\" {ask_str} Replace [Your Name] with Dave Babbitt\"\n",
    "elif topic == 'zoom':\n",
    "    interviewer_name = 'Dan, David, Alex, and Melinda'\n",
    "    company_name = '3GIMBALS'\n",
    "    youchat_str = f\"Write a follow up thank you note for an interview using this information: a) Interviewer Name: {interviewer_name}, b)\"\n",
    "    youchat_str += f\" Position: {job_title}, c) Company Name {company_name}, d) relevant skills: {ask_str}, e) Your Name: Dave Babbitt.\"\n",
    "    youchat_str += f\" Ask about going over the programming exercise.\"\n",
    "elif topic == 'phone':\n",
    "    interviewer_name = 'Dan, David, Alex, and Melinda'\n",
    "    company_name = '3GIMBALS'\n",
    "    interviewer_title = 'interview team'\n",
    "    youchat_str = f\"Write an email, complete with subject, to {interviewer_name} about what a pleasure it was to talk to them, the\"\n",
    "    youchat_str += f\" {interviewer_title}, on the phone about the {job_title} position with {company_name}. Replace [Your Name]\"\n",
    "    youchat_str += \" with Dave Babbitt\"\n",
    "elif topic == 'interested':\n",
    "    file_path = '../data/txt/resume.txt'\n",
    "    with open(file_path, 'r') as file:\n",
    "        resume_str = file.read().rstrip()\n",
    "    task_strs_list = []\n",
    "    for task_str in [child_str for pos_str, child_str in zip(pos_list, child_strs_list) if (pos_str in ['O-TS'])]:\n",
    "        task_strs_list.append(task_str)\n",
    "    company_name = child_strs_list[1]\n",
    "    youchat_str = f\"Explain in first person singular why I would be interested in this role at {company_name}, given\\n\\n1)\"\n",
    "    youchat_str += f\" this information about the task scope:\\n\\n{' '.join(task_strs_list)}\\n\\nand, 2) my resume:\\n\\n{resume_str}\"\n",
    "elif topic == 'question':\n",
    "    file_path = '../data/txt/resume.txt'\n",
    "    with open(file_path, 'r') as file:\n",
    "        resume_str = file.read().rstrip()\n",
    "    task_strs_list = []\n",
    "    for task_str in [child_str for pos_str, child_str in zip(pos_list, child_strs_list) if (pos_str in ['O-TS'])]:\n",
    "        task_strs_list.append(task_str)\n",
    "    company_name = child_strs_list[1]\n",
    "    youchat_str = f\"Pretend you have the competencies and experience listed on the resume. Explain in first person singular\"\n",
    "    youchat_str += \" how you manage projects, communicate with clients, discover their needs, make recommendations, stay in budget,\"\n",
    "    youchat_str += f\" manage changing requirements, and produce results, given this resume:\\n\\n{resume_str[75:]}\"\n",
    "elif topic == 'rejected':\n",
    "    job_title = 'Senior Data Engineering Analyst, Platform Engineering (Remote, Anywhere in US)'\n",
    "    recruiting_team_name = 'Humana Recruiting Team'\n",
    "    company_name = 'Humana'\n",
    "    youchat_str = f\"Write a reply to the {recruiting_team_name} rejection letter for the {job_title} position at {company_name},\"\n",
    "    youchat_str += \" persuading the recruiting team to explain in more detail why I was rejected for the role. Replace [Name] with\"\n",
    "    youchat_str += \" Dave Babbitt\"\n",
    "print(youchat_str)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b88e1ad3-c36f-4a2a-9d49-3b4d59404d98",
   "metadata": {},
   "source": [
    "\n",
    "----"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "662a0814-82bf-48b9-99b6-a87d1b4780f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Write a cover letter email, complete with subject, using this information: <p>I only meet 80.0% of the minimum requirements for the Data Scientist London England United Kingdom Remote position, but I can explain:</p> 1) <li>No formal academic, postgraduate or professional qualifications are</li> 2) <li>Strong previous experience as a Data Scientist.</li> 3) <li>Proven knowledge of programming skills and database knowledge.</li> 5) <li>Database experience, ideally AWS tech stack, MongoDB, and PostgreSQL</li> 6) <li>Experience with integrating various other public/proprietary datasets is highly desirable.</li> 7) <li>Strong understanding and implementation experience of predictive modelling algorithms such as regressions, time series, clustering and decision trees.</li> 9) <li>Ability to analyse data to drive actions.</li> 10) <li>Familiarity dealing with trade-offs between model performance and business needs.</li> Replace [Your Name] with Dave Babbitt\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "No active exception to reraise",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[60], line 7\u001b[0m\n\u001b[0;32m      5\u001b[0m wsu\u001b[38;5;241m.\u001b[39mdriver_get_url(driver, youchat_url, verbose\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[0;32m      6\u001b[0m \u001b[38;5;28mprint\u001b[39m(youchat_str)\n\u001b[1;32m----> 7\u001b[0m winsound\u001b[38;5;241m.\u001b[39mBeep(freq, duration); \u001b[38;5;28;01mraise\u001b[39;00m\n",
      "\u001b[1;31mRuntimeError\u001b[0m: No active exception to reraise"
     ]
    }
   ],
   "source": [
    "\n",
    "import urllib.parse\n",
    "\n",
    "driver = wsu.get_driver(verbose=False)\n",
    "youchat_url = f'https://you.com/search?q={urllib.parse.quote_plus(youchat_str)}&fromSearchBar=true&tbm=youchat'\n",
    "wsu.driver_get_url(driver, youchat_url, verbose=False)\n",
    "print(youchat_str)\n",
    "winsound.Beep(freq, duration); raise"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a26bd4fd-ce1f-42aa-9c76-c5db6ff80e00",
   "metadata": {},
   "source": [
    "\n",
    "### Check the back FireFox window to make sure the chat writing has stopped before running this next cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "d5e80757-79be-4d5f-806e-d42608a19381",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dear Hiring Manager,\n",
      "I am writing to apply for the Data Scientist London England United Kingdom Remote position. Although I only meet 80.0% of the minimum requirements listed, there are several aspects of my experience that I believe make me a strong candidate for this position.\n",
      "First, while I do not have any formal academic, postgraduate, or professional qualifications, I have extensive experience as a Data Scientist. In my previous roles, I have demonstrated the ability to apply predictive modelling algorithms, such as regressions, time series, and decision trees, to drive actions and improve business outcomes. I am also skilled in programming and database knowledge, with experience in AWS tech stack, MongoDB, and PostgreSQL.\n",
      "In addition, I have experience integrating multiple public and proprietary datasets, which I believe would be beneficial in this position. I am familiar with the trade-offs between model performance and business needs, and I excel at analyzing data to drive actions.\n",
      "I am excited about the opportunity to work with your team and would appreciate the chance to further discuss my qualifications with you. Thank you for your time and consideration.\n",
      "Sincerely,\n",
      "Dave Babbitt\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from selenium.common.exceptions import NoSuchElementException\n",
    "from selenium.webdriver.common.by import By\n",
    "\n",
    "css_selector = 'div[data-testid=\"youchat-answer-turn-0\"]'\n",
    "try:\n",
    "    web_element = driver.find_element(By.CSS_SELECTOR, css_selector)\n",
    "    print(web_element.text)\n",
    "except NoSuchElementException as e:\n",
    "    pass\n",
    "except Exception as e:\n",
    "    print(f'{e.__class__.__name__} error: {str(e).strip()}')\n",
    "finally:\n",
    "    driver.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "886daf59-7a95-4c27-ab2e-f9f8150be523",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Job Hunting (Python 3.10.9)",
   "language": "python",
   "name": "jh_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
