{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "174c9bb2-7d08-4fa2-aa2b-e3be342e52ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pretty printing has been turned OFF\n"
     ]
    }
   ],
   "source": [
    "\n",
    "%pprint"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34d2c4fa-fe66-4b71-9899-4b533c04515b",
   "metadata": {
    "tags": []
   },
   "source": [
    "\n",
    "---\n",
    "# Load needed libraries and functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "06fd23c4-353b-4800-973d-785e63a9856e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "%matplotlib inline\n",
    "from IPython.display import HTML\n",
    "from datetime import datetime\n",
    "from neo4j.exceptions import ServiceUnavailable\n",
    "import enchant\n",
    "import humanize\n",
    "import os\n",
    "import sys\n",
    "import time\n",
    "import warnings\n",
    "import winsound\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "duration = 1000  # milliseconds\n",
    "freq = 880  # Hz\n",
    "\n",
    "# Insert at 1, 0 is the script path (or '' in REPL)\n",
    "if ('../py' not in sys.path): sys.path.insert(1, '../py')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e42298f7-a9cc-4d21-871b-bf2abf92b800",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======== Neo4j/4.4.7 ========\n",
      "Utility libraries created in 9 seconds\n"
     ]
    }
   ],
   "source": [
    "\n",
    "t0 = time.time()\n",
    "\n",
    "# Get the Neo4j driver\n",
    "from storage import Storage\n",
    "s = Storage(\n",
    "    data_folder_path=os.path.abspath('../data'),\n",
    "    saves_folder_path=os.path.abspath('../saves')\n",
    ")\n",
    "\n",
    "from ha_utils import HeaderAnalysis\n",
    "ha = HeaderAnalysis(s=s, verbose=False)\n",
    "\n",
    "from scrape_utils import WebScrapingUtilities\n",
    "wsu = WebScrapingUtilities(\n",
    "    s=s,\n",
    "    secrets_json_path=os.path.abspath('../data/secrets/jh_secrets.json')\n",
    ")\n",
    "uri = wsu.secrets_json['neo4j']['connect_url']\n",
    "user =  wsu.secrets_json['neo4j']['username']\n",
    "password = wsu.secrets_json['neo4j']['password']\n",
    "\n",
    "# Get the neo4j object\n",
    "from cypher_utils import CypherUtilities\n",
    "cu = CypherUtilities(\n",
    "    uri=uri, user=user, password=password, driver=None, s=s, ha=ha\n",
    ")\n",
    "\n",
    "try:\n",
    "    version_str = cu.driver.get_server_info().agent\n",
    "    print(f'======== {version_str} ========')\n",
    "except ServiceUnavailable as e:\n",
    "    print('You need to start Neo4j as a console')\n",
    "    raise\n",
    "except Exception as e:\n",
    "    print(f'{e.__class__}: {str(e).strip()}')\n",
    "\n",
    "from hc_utils import HeaderCategories\n",
    "hc = HeaderCategories(cu=cu, verbose=False)\n",
    "\n",
    "from lr_utils import LrUtilities\n",
    "lru = LrUtilities(ha=ha, cu=cu, hc=hc, verbose=False)\n",
    "\n",
    "from section_classifier_utils import SectionLRClassifierUtilities, SectionSGDClassifierUtilities, SectionCRFClassifierUtilities\n",
    "slrcu = SectionLRClassifierUtilities(ha=ha, cu=cu, verbose=False)\n",
    "ssgdcu = SectionSGDClassifierUtilities(ha=ha, cu=cu, verbose=False)\n",
    "scrfcu = SectionCRFClassifierUtilities(cu=cu, ha=ha, verbose=False)\n",
    "\n",
    "from crf_utils import CrfUtilities\n",
    "crf = CrfUtilities(ha=ha, hc=hc, cu=cu, lru=lru, slrcu=slrcu, verbose=True)\n",
    "\n",
    "from section_utils import SectionUtilities\n",
    "su = SectionUtilities(wsu=wsu, crf=crf, verbose=False)\n",
    "\n",
    "duration_str = humanize.precisedelta(time.time() - t0, minimum_unit='seconds', format='%0.0f')\n",
    "print(f'Utility libraries created in {duration_str}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8dbe7bc3-7ab4-4950-a7a1-04ebe419187a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I have 49,140 hand-labeled header htmls prepared\n",
      "7 iterations seen during training fit for a total of 49,140 records trained\n",
      "Is-header classifier trained in 6 seconds\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Train the isheader classifier\n",
    "t1 = time.time()\n",
    "from is_header_sgd_classifier import IsHeaderSgdClassifier\n",
    "ihu = IsHeaderSgdClassifier(ha=ha, cu=cu, verbose=False)\n",
    "ihu.build_pos_stochastic_gradient_descent_elements(verbose=True)\n",
    "duration_str = humanize.precisedelta(time.time() - t1, minimum_unit='seconds', format='%0.0f')\n",
    "print(f'Is-header classifier trained in {duration_str}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d91632d9-a9f4-43a3-b76b-4bdc57736c0f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I have 49,102 labeled parts of speech in here\n",
      "Parts-of-speech SGD classifier trained in 10 seconds\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Train the POS SGD classifier\n",
    "t1 = time.time()\n",
    "if not hasattr(ssgdcu, 'pos_predict_percent_fit_dict'):\n",
    "    ssgdcu.build_pos_stochastic_gradient_descent_elements(verbose=True)\n",
    "duration_str = humanize.precisedelta(time.time() - t1, minimum_unit='seconds', format='%0.0f')\n",
    "print(f'Parts-of-speech SGD classifier trained in {duration_str}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "551caae7-a91c-44fd-a2a8-fcd00dd454e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I have 424,879 is-qualified vocabulary tokens in here\n",
      "Is-qualified LR elements built in 5 seconds\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Check if the lru has built its is-qualified classifier\n",
    "t0 = time.time()\n",
    "if not hasattr(lru, 'ISQUALIFIED_LR'):\n",
    "    lru.build_isqualified_logistic_regression_elements(sampling_strategy_limit=5_000, verbose=True)\n",
    "duration_str = humanize.precisedelta(time.time() - t0, minimum_unit='seconds', format='%0.0f')\n",
    "print(f'Is-qualified LR elements built in {duration_str}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2d8dcb74-e13b-4f85-826e-4b9053f53c64",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parts-of-speech CRF classifier trained in 1 second\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Train the POS CRF classifier\n",
    "t1 = time.time()\n",
    "if not hasattr(scrfcu, 'pos_symbol_crf'):\n",
    "    scrfcu.build_pos_conditional_random_field_elements(verbose=True)\n",
    "duration_str = humanize.precisedelta(time.time() - t1, minimum_unit='seconds', format='%0.0f')\n",
    "print(f'Parts-of-speech CRF classifier trained in {duration_str}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "52458533-60c4-4597-9f55-dcec3d328504",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Seek a SectionLRClassifierUtilities object\n",
    "if hasattr(slrcu, 'pos_predict_percent_fit_dict'):\n",
    "    pos_lr_predict_single = slrcu.predict_single\n",
    "elif crf.is_flask_running():\n",
    "    pos_lr_predict_single = crf.get_pos_lr_predict_single_from_api\n",
    "else:\n",
    "    pos_lr_predict_single = None\n",
    "\n",
    "# Seek a SectionCRFClassifierUtilities object\n",
    "if hasattr(scrfcu, 'pos_symbol_crf'):\n",
    "    pos_crf_predict_single = scrfcu.predict_single\n",
    "elif crf.is_flask_running():\n",
    "    pos_crf_predict_single = crf.get_pos_crf_predict_single_from_api\n",
    "else:\n",
    "    pos_crf_predict_single = None\n",
    "\n",
    "# Seek a SectionSGDClassifierUtilities object\n",
    "if hasattr(ssgdcu, 'pos_predict_percent_fit_dict'):\n",
    "    pos_sgd_predict_single = ssgdcu.predict_single\n",
    "elif crf.is_flask_running():\n",
    "    pos_sgd_predict_single = crf.get_pos_sgd_predict_single_from_api\n",
    "else:\n",
    "    pos_sgd_predict_single = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "01e7d031-531c-4024-b9f4-03485a91a943",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "predict_single is now available\n",
      "POS classifier trained in 0 seconds\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Check if the crf has built its parts-of-speech classifier\n",
    "# POS classifier trained in 12 hours, 15 minutes and 36 seconds\n",
    "t1 = time.time()\n",
    "if not hasattr(crf, 'CRF'):\n",
    "    crf.build_pos_conditional_random_field_elements(verbose=True)\n",
    "if hasattr(crf, 'CRF'):\n",
    "    print('predict_single is now available')\n",
    "else:\n",
    "    print('predict_single is not available')\n",
    "duration_str = humanize.precisedelta(time.time() - t1, minimum_unit='seconds', format='%0.0f')\n",
    "print(f'POS classifier trained in {duration_str}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d5d5f01c-6342-4c46-9d52-a3608cc29ba5",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "file_name = '5f3d7c7dc4177679_Data_Integration_and_Ingestion_Engineering_Lead_Remote_Indeed_com.html'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f7248bd-9c02-48f1-8bd2-e1028b202405",
   "metadata": {},
   "source": [
    "\n",
    "---\n",
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7ed87db8-7c12-48c4-b2c0-932cf9333f59",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I have 13,364 hand-labeled qualification strings in here\n",
      "I have 443,334 is-qualified vocabulary tokens in here\n",
      "Is-qualified classifer retrained in 9 seconds\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# You need to run this again if you changed the qualification dictionary in another notebook\n",
    "t0 = time.time()\n",
    "lru.sync_basic_quals_dict(sampling_strategy_limit=None, verbose=False)\n",
    "lru.retrain_isqualified_classifier(verbose=True)\n",
    "duration_str = humanize.precisedelta(time.time() - t0, minimum_unit='seconds', format='%0.0f')\n",
    "print(f'Is-qualified classifer retrained in {duration_str}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae75c495-79a9-4475-ad97-46ae6d42a294",
   "metadata": {},
   "source": [
    "\n",
    "----\n",
    "# Prepare cover sheet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "265ddacf-68eb-49fd-8b0b-1b78afceaa61",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<p>I only meet 100.0% of the minimum requirements for the Data Integration and Ingestion Engineering Lead Remote position, but I can explain:</p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<b>1) Python</b>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<li>2) This role sometimes requires working with our customers data teams, so we need someone who is comfortable with and ideally somewhat enjoys customer interaction (through email and zoom/google meets)</li>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<b>3) Python</b>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<li>4) This role sometimes requires working with our customers data teams, so we need someone who is comfortable with and ideally somewhat enjoys customer interaction (through email and zoom/google meets)</li>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Qualifications shown in 13 seconds\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Show what qualifications you have for this posting\n",
    "t0 = time.time()\n",
    "ask_str = ''\n",
    "child_strs_list = ha.get_child_strs_from_file(file_name=file_name)\n",
    "\n",
    "feature_tuple_list = []\n",
    "for feature_dict in cu.get_feature_dict_list(ha.get_child_tags_list(child_strs_list), child_strs_list):\n",
    "    feature_tuple_list.append(hc.get_feature_tuple(\n",
    "        feature_dict, pos_lr_predict_single=pos_lr_predict_single, pos_crf_predict_single=pos_crf_predict_single,\n",
    "        pos_sgd_predict_single=pos_sgd_predict_single\n",
    "    ))\n",
    "\n",
    "crf_list = crf.CRF.predict_single(crf.sent2features(feature_tuple_list))\n",
    "indices_list = su.find_basic_quals_section_indexes(child_strs_list=child_strs_list, crf_list=crf_list, file_name=file_name)\n",
    "quals_list = [child_str for i, child_str in enumerate(child_strs_list) if i in indices_list]\n",
    "prediction_list = list(lru.predict_job_hunt_percent_fit(quals_list))\n",
    "_, qual_count = lru.get_quals_str(prediction_list, quals_list)\n",
    "job_fitness = qual_count/len(prediction_list)\n",
    "d = enchant.Dict('en_US')\n",
    "job_title = ' '.join([w for w in file_name.replace('.html', '').replace('_Indeed_com', '').split('_') if d.check(w)])\n",
    "met_str = f'<p>I only meet {job_fitness:.1%} of the minimum requirements for the {job_title} position, but I can explain:</p>'\n",
    "ask_str += met_str\n",
    "display(HTML(met_str))\n",
    "for i, qual_str in enumerate(quals_list):\n",
    "    if qual_str in lru.basic_quals_dict:\n",
    "        if lru.basic_quals_dict[qual_str]:\n",
    "            met_str = f'{i+1}) {qual_str}'\n",
    "            ask_str += ' ' + met_str\n",
    "            idx = qual_str.find('>')\n",
    "            if idx == -1:\n",
    "                display(HTML(met_str))\n",
    "            else:\n",
    "                display(HTML(f'{qual_str[:idx+1]}{i+1}) {qual_str[idx+1:]}'))\n",
    "duration_str = humanize.precisedelta(time.time() - t0, minimum_unit='seconds', format='%0.0f')\n",
    "print(f'Qualifications shown in {duration_str}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "0ef7b2a4-d443-4cdd-86ad-a7f4938d4f34",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<p>The minimum requirements that I don't meet are:</p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "unmet_str = \"<p>The minimum requirements that I don't meet are:</p>\"\n",
    "display(HTML(unmet_str))\n",
    "for i, qual_str in enumerate(quals_list):\n",
    "    if (qual_str not in lru.basic_quals_dict) or not lru.basic_quals_dict[qual_str]:\n",
    "        met_str = f'{i+1}) {qual_str}'\n",
    "        unmet_str += ' ' + met_str\n",
    "        idx = qual_str.find('>')\n",
    "        if idx == -1:\n",
    "            display(HTML(met_str))\n",
    "        else:\n",
    "            display(HTML(f'{qual_str[:idx+1]}{i+1}) {qual_str[idx+1:]}'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d2d705f1-6522-416e-a223-682fdfd6e66a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<p>The preferred requirements that I meet are:</p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "# This doesn't work unless you score all the O-PQs\n",
    "db_pos_list = []\n",
    "for navigable_parent in child_strs_list:\n",
    "    db_pos_list = cu.append_parts_of_speech_list(navigable_parent, pos_list=db_pos_list)\n",
    "pos_list = []\n",
    "for i, (crf_symbol, db_symbol) in enumerate(zip(crf_list, db_pos_list)):\n",
    "    if db_symbol in [None, 'O', 'H']:\n",
    "        pos_list.append(crf_symbol)\n",
    "    else:\n",
    "        pos_list.append(db_symbol)\n",
    "met_str = f\"<p>The preferred requirements that I meet are:</p>\"\n",
    "display(HTML(met_str))\n",
    "min_str = ''\n",
    "pqs_list = [child_str for pos_str, child_str in zip(pos_list, child_strs_list) if (pos_str in ['O-PQ'])]\n",
    "for i, qual_str in enumerate(pqs_list):\n",
    "    if qual_str in lru.basic_quals_dict:\n",
    "        if lru.basic_quals_dict[qual_str]:\n",
    "            pref_str = f'{i+1}) {qual_str}'\n",
    "            min_str += ' ' + pref_str\n",
    "            idx = qual_str.find('>')\n",
    "            if idx == -1:\n",
    "                display(HTML(pref_str))\n",
    "            else:\n",
    "                display(HTML(f'{qual_str[:idx+1]}{i+1}) {qual_str[idx+1:]}'))\n",
    "if min_str:\n",
    "    ask_str += met_str + min_str\n",
    "wsu.beep(freq, duration)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a271b3e8-1d86-46a8-aa8e-3e9d519a5eeb",
   "metadata": {},
   "source": [
    "\n",
    "----\n",
    "# Write cover sheet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "087a9de1-5276-415b-8ee6-c6d249c26189",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Explain in first person singular why I would be interested in this role at <span>Here’s how the job details align with your job preferences.</span>, given\n",
      "\n",
      "1) this information about the task scope:\n",
      "\n",
      "who <b>likes getting their hands dirty and jumping in on IC</b> work. We first want to talk to folks who have managed people and done project management and feel comfortable in both. <li>Currently, the CTO and VP of Eng are doing filling this role and we're looking to have someone come in and lead this pod of 4 engineers. You would report to VP of Engineering.</li> <b>Remote is fine or onsite in Austin.</b> and more who <b>likes getting their hands dirty and jumping in on IC</b> work. We first want to talk to folks who have managed people and done project management and feel comfortable in both. <li>Currently, the CTO and VP of Eng are doing filling this role and we're looking to have someone come in and lead this pod of 4 engineers. You would report to VP of Engineering.</li> <b>Remote is fine or onsite in Austin.</b> and more\n",
      "\n",
      "and, 2) my resume:\n",
      "\n",
      "\n",
      "Dave Babbitt\n",
      "Data Scientist\n",
      "+1 413 244 9949\n",
      "dave.babbitt@gmail.com\n",
      "Clinton, MA 01510\n",
      "\n",
      "Willing to relocate to Tucson, AZ\n",
      "Authorized to work in the US for any employer\n",
      "\n",
      "Summary\n",
      "The last seven years have been spent analyzing large datasets and visualizing them in a way that statistically validated every hunch about that data. Not only that, I was able to work with domain experts within the IC to create predictive analytics and prescriptive simulations of such power and flexibility that I was able to answer every \"what if\" and \"how should I do this\" scenario that anybody cared to ask. Previously, I spent seventeen years of my career developing complicated web applications.\n",
      "\n",
      "Work experience\n",
      "\n",
      "Machine Learning Engineer\n",
      "BigBear.ai - Remote\n",
      "March 2023 to Present\n",
      "Added a Responder Type Column to the OSU dataset of FRVRS Logs, analyzed Gaze and Intent, Issue with Logging Multiple TOOL_APPLIEDs, Preliminary Research Questions, TOOL_HOVERing as Indicative of Next Patient Choice, found Negative Metrics in DCEMS Data, did Orientation-Normal Sequence Analysis, performed Tool Applied Exploration, fixed Elapsed Time Simultaneity, identified any Anomalous Files, renamed Files, replaced the tool applied sender missing patient ID, reserialized DataFrame pickles, developed the Correct Count Triage Accuracy Metric, the Number of Patients Treated Metric, the Number of Pulses Taken Metric, the Number of Voice Captures per Session Metric, the Patient Accuracy Rate Metric, the R-squared Triage Accuracy Metric, the Total Number of Teleports Metric, the Treatment Placement Error Metric, and the Triage Accuracy Metric, Plotted Pie Charts for Tag to SALT Dataset, Visualized every Player Gaze, Visualized Location Points, and Visualized Non-cumulative Teleportation Events: (words, get_modal_state, get_synchrony, get_subsequences, and get_routine_scores)\n",
      "Analyzed Deidentified Simulation Voice Captures, analyzed Time Spent on Task, calculated Results for Abstract Submission, developed the Number of Patients Engaged Metric, the Time to First Engagement Metric, the Time to First Treatment Metric, the Time To Hemorrhage Control Metric, the Total User Actions Taken Metric, the Triage Efficiency Metric, Plotted Grouped Box and Whiskers for Time to First Engagement, Visualized Elapsed Time Spent on Patient, and Visualized every Patient Interaction: (timedelta, words, get_first_positions, get_subsequences, and get_routine_scores)\n",
      "Analyzed Elapsed Time Estimation for Operations: (association_rules, apriori, timedelta, get_modal_state, and get_subsequences)\n",
      "Analyzed Patient Engaged Events: (apriori, association_rules, get_modal_state, get_synchrony, and get_subsequences)\n",
      "Analyzed START Triage vs SALT Triage: (path, timedelta, words, get_modal_state, and get_subsequences)\n",
      "Built a Patient Engagement timeline CSV: (apriori, get_modal_state, get_synchrony, get_subsequences, and get_routine_scores)\n",
      "Built a Model to Predict Tag Applied Type: (train_test_split, permutation_importance, words, get_modal_state, and get_subsequences)\n",
      "Built the OSU dataset of FRVRS Logs and wrote a How to Use the Data: (en_core_web_sm, words, get_modal_state, get_synchrony, and get_subsequences)\n",
      "Catsim Library Exploration: (catsim, icc, plot, generate_item_bank, and words)\n",
      "Developed the Correct Count by Tag Triage Accuracy Metric and Plot Stacked Horizontal Bar Charts for Over-Under Triage Errors: (seaborn, words, get_first_positions, get_subsequences, and get_routine_scores)\n",
      "Explored Bleeping Proper Nouns out of Audio Files: (soundfile, speech_recognition, words, get_first_positions, and get_subsequences)\n",
      "Explored Voice Capture Ngrams: (word_tokenize, words, subjectivity, stopwords, and wordnet)\n",
      "Completed Near-engagement Transactions for Affinity Analysis: (association_rules, apriori, get_modal_state, get_synchrony, and get_subsequences)\n",
      "Attempted Rasch Analysis: (generate_item_bank, words, get_modal_state, get_synchrony, and get_subsequences)\n",
      "Recomputed Visualizations for the SDMPH 2023 Poster: (arange, seaborn, timedelta, words, and get_modal_state)\n",
      "Renamed elapsed time columns in DataFrame pickles and Rename time group columns in DataFrame pickles: (path, words, get_modal_state, get_subsequences, and get_routine_scores)\n",
      "Replaced UUIDs with Cleaned and Revised File Info: (en_core_web_sm, path, words, get_modal_state, and get_subsequences)\n",
      "Did Walk-Wave Sequence Analysis: (get_synchrony, get_transition_matrix, get_all_ngrams, get_transitions, and plot_element_counts)\n",
      "\n",
      "Decision Scientist\n",
      "Meta - Menlo Park, CA\n",
      "September 2022 to March 2023\n",
      "Developed a pipeline for an Accenture client to evaluate a lead KPI indicator, including testing model assumptions, model feature engineering, model regression training, and model prediction visualizations. (SQL, Bento notebooks, Python, Pandas, NumPy)\n",
      "\n",
      "Job Hunter\n",
      "Machine Learning Pipeline - Remote\n",
      "March 2022 to September 2022\n",
      "Developed a machine learning pipeline to aid in job hunting by scraping job postings off the web using Selenium and BeautifulSoup (Selenium, BeautifulSoup, Python)\n",
      "Identified the minimum requirements section among the parent-tagged navigable text elements of the web or email (Bag-of-Words, TF-IDF, Logistic Regression, CFR, Python)\n",
      "Trained a model to determine which qualifications I met. (Bag-of-Words, TF-IDF, Logistic Regression, Python)\n",
      "Used a GPT-3 model to generate cover letters if I met 80% or more of the minimum requirements. (GPT-3, Python)\n",
      "Stored the Header Tags, File Names, Navigable Parents, Qualification Strings, and Parts of Speech as nodes in a graph database management system (Neo4j, Cypher, Flask, Pandas)\n",
      "\n",
      "Senior Data Scientist\n",
      "Kessel Run - Boston, MA\n",
      "May 2021 to March 2022\n",
      "Created a pipeline for a IT Concepts Inc. client that automatically retrieves JSON data via API from various sources, date-pivots and merges them, and commits them to a repository, Excel, and Opensearch for further analysis. (Excel, Opensearch, Pivotal Tracker, GitLab, Python, Pandas, NumPy)\n",
      "Conducted code complexity analysis on the commit histories of various git repositories. (Pivotal Tracker, GitLab, Python, Jupyter, Pandas)\n",
      "Created KPIs on datasets retrieved from various Pivotal Tracker and GitLab projects. (Pivotal Tracker, GitLab, Python, Jupyter, Pandas)\n",
      "Created various visualizations including line and bar plots. (Opensearch, Python, Jupyter, Pandas)\n",
      "Trained a Logistic Regression predictive model using Bag-of-Words and TF-IDF (Term Frequency â€“ Inverse Document Frequency, Pandas)\n",
      "\n",
      "Data Scientist\n",
      "Plateau Software Inc - Scott AFB, IL\n",
      "December 2020 to May 2021\n",
      "Determined the \"hits\" where a satellite had geodetic overlap of any vessel(s) at any point(s) in time. (For simplicity, used a standard Geodetic Overlap Accuracy model). (Python, Pandas, NumPy)\n",
      "Programmatically instantiated resources, performed data munging, uploaded to database, sent JSON data to UI to display hits. (AWS SAM/S3 data bucket manipulation and querying/Lambda, Docker, Python, Pandas)\n",
      "\n",
      "Job Hunter\n",
      "Machine Learning Pipeline - Remote\n",
      "October 2020 to December 2020\n",
      "Developed a machine learning pipeline to aid in job hunting by scraping job postings off the web using Selenium and BeautifulSoup (Selenium, BeautifulSoup, Python, Pandas)\n",
      "Identified the minimum requirements section among the parent-tagged navigable text elements of the web or email (Bag-of-Words, TF-IDF, Logistic Regression, CFR, Python, Pandas)\n",
      "Trained a model to determine which qualifications I met. (Bag-of-Words, TF-IDF, Logistic Regression, Python, Pandas)\n",
      "Used a GPT-3 model to generate cover letters if I met 80% or more of the minimum requirements. (GPT-3, Python, Pandas)\n",
      "Stored the Header Tags, File Names, Navigable Parents, and Parts of Speech as tables in a relational database management system (Microsoft SQL Server, SQL)\n",
      "\n",
      "Data Scientist\n",
      "Booz Allen Hamilton\n",
      "January 2015 to October 2020\n",
      "Developed data science capability for intelligence analysts supporting US Air Force airborne intelligence, surveillance, and reconnaissance systems and analytical processes.\n",
      "Analyzed large data processing problems and created tailored, repeatable, and scalable approaches to data manipulation and preparation for follow-on analysis.\n",
      "Researched and nominated unclassified and open information sources for data mining to support ongoing analytical efforts.\n",
      "Programmed logical interfaces and applied techniques for efficient program logic and data manipulation.\n",
      "Conducted systems programming and support activities, including new/revised/segments of language code and processing.\n",
      "Documented developed processes for data extraction, manipulation, analysis, and storage.\n",
      "Trained intelligence analysts on programming and mathematical facets of data science and execution of developed processes.\n",
      "\n",
      "Senior Systems Analyst\n",
      "Clipboard Solutions\n",
      "May 2000 to February 2006\n",
      "Designed and developed a health care payroll system; in use 10 years; exported to check writing software.\n",
      "Built an extensive time entry and scheduling system that incorporates mobile and biometric devices.\n",
      "\n",
      "Education\n",
      "\n",
      "MS Predictive Analytics, Northwestern University - Chicago, IL\n",
      "December 2012 to March 2015\n",
      "\n",
      "Bachelor of Science, Excelsior College - Albany, NY\n",
      "March 1991\n",
      "\n",
      "Skills\n",
      "AWS - 1 year\n",
      "XML - 2 years\n",
      "Machine Learning - 7 years\n",
      "Java - 2 years\n",
      "SQL - 10 years\n",
      "R - 2 years\n",
      "SVN - 1 year\n",
      "HTML5 - 2 years\n",
      "Git - 10 years\n",
      "Python - 7 years\n",
      "GitHub - 5 years\n",
      "APIs - 5 years\n",
      "Data Science - 7 years\n",
      "Analytics - 7 years\n",
      "REST - 2 years\n",
      "Linux - 10 years\n",
      "GitLab - 1 year\n",
      "JavaScript - 2 years\n",
      "MySQL - 3 years\n",
      "Agile - 1 year\n",
      "\n",
      "Languages\n",
      "English - Fluent\n",
      "\n",
      "Links\n",
      "https://github.com/dbabbitt/\n",
      "https://stackexchange.com/users/31594/dave-babbitt?tab=accounts\n",
      "\n",
      "Military service\n",
      "Branch: United States Navy\n",
      "Rank: ET2 (E-5)\n",
      "\n",
      "Certifications and licenses\n",
      "Machine Learning\n",
      "2017 to Present\n",
      "Security+ CE\n",
      "May 2019 to May 2022\n",
      "\n",
      "Assessments\n",
      "Analyzing data - PROFICIENT\n",
      "May 2021\n",
      "\n",
      "Publications\n",
      "\n",
      "The COVID-19 pandemic as experienced by the individual\n",
      "https://arxiv.org/abs/2005.01167\n",
      "September 2020\n",
      "Patrick Garland, Dave Babbitt, Maksym Bondarenko, Alessandro Sorichetta, Andrew J. Tatem, Oliver Johnson\n",
      "\n",
      "Multi-INT Correlation and Fusion in Watchman for Defense\n",
      "2019\n",
      "Pottenger, W.M., Nagy, J, Scott, S, VanHorn, S., Campbell, M., Stone, C., Shatzman, G., Gantt, N., Janneck, C.D., Kelly, C.A., Shin, S., Tong, T., Mann, R., Blasch, E., Babbitt, D. (2019) Multi-INT Correlation and Fusion in Watchman for Defense, 2019 National Symposium on Sensor and Data Fusion (NSSDF), San Diego, CA\n"
     ]
    }
   ],
   "source": [
    "\n",
    "topic = 'interested'\n",
    "if topic == 'recruiter':\n",
    "    recruiter_name = 'Joseph St.Denis'\n",
    "    youchat_str = f\"Reply to the recruiter email that you don't meet {1-job_fitness:.1%} of the requirements\"\n",
    "    youchat_str += f\" ({unmet_str}), though you do meet these criterion: {ask_str} and am interested in applying for the job.\"\n",
    "    #and have applied for the job.\n",
    "    youchat_str += f\" (Replace [Your Name] with Dave Babbitt, Replace [Recruiter] with {recruiter_name})\"\n",
    "elif topic == 'cover':\n",
    "    import pandas as pd\n",
    "    cypher_str = f\"\"\"\n",
    "        MATCH (fn:FileNames {{file_name: \"{file_name}\"}})\n",
    "        RETURN\n",
    "            fn.role_primary_contact AS role_primary_contact,\n",
    "            fn.role_primary_contact_email_id AS role_primary_contact_email_id,\n",
    "            fn.role_title AS role_title\n",
    "        ORDER BY fn.percent_fit DESC;\"\"\"\n",
    "    cover_df = pd.DataFrame(cu.get_execution_results(cypher_str, verbose=False))\n",
    "    recruiter_name = cover_df.role_primary_contact.squeeze()\n",
    "    email_address = cover_df.role_primary_contact_email_id.squeeze()\n",
    "    role_title = cover_df.role_title.squeeze()\n",
    "    if (recruiter_name is None) or (email_address is None):\n",
    "        suffix_str = ''\n",
    "    else:\n",
    "        suffix_str = f' to \"{recruiter_name}\" <{email_address}>'\n",
    "    youchat_str = f\"Write a cover letter email{suffix_str}, complete with subject, using this information:\"\n",
    "    youchat_str += f\" {ask_str} Replace [Your Name] with Dave Babbitt\"\n",
    "elif topic == 'zoom':\n",
    "    interviewer_name = 'Dan, David, Alex, and Melinda'\n",
    "    company_name = '3GIMBALS'\n",
    "    youchat_str = f\"Write a follow up thank you note for an interview using this information: a) Interviewer Name: {interviewer_name}, b)\"\n",
    "    youchat_str += f\" Position: {job_title}, c) Company Name {company_name}, d) relevant skills: {ask_str}, e) Your Name: Dave Babbitt.\"\n",
    "    youchat_str += f\" Ask about going over the programming exercise.\"\n",
    "elif topic == 'phone':\n",
    "    interviewer_name = 'Dan, David, Alex, and Melinda'\n",
    "    company_name = '3GIMBALS'\n",
    "    interviewer_title = 'interview team'\n",
    "    youchat_str = f\"Write an email, complete with subject, to {interviewer_name} about what a pleasure it was to talk to them, the\"\n",
    "    youchat_str += f\" {interviewer_title}, on the phone about the {job_title} position with {company_name}. Replace [Your Name]\"\n",
    "    youchat_str += \" with Dave Babbitt\"\n",
    "elif topic == 'interested':\n",
    "    file_path = '../data/txt/resume.txt'\n",
    "    with open(file_path, 'r') as file:\n",
    "        resume_str = file.read().rstrip()\n",
    "    task_strs_list = []\n",
    "    for task_str in [child_str for pos_str, child_str in zip(pos_list, child_strs_list) if (pos_str in ['O-TS'])]:\n",
    "        task_strs_list.append(task_str)\n",
    "    company_name = child_strs_list[1]\n",
    "    youchat_str = f\"Explain in first person singular why I would be interested in this role at {company_name}, given\\n\\n1)\"\n",
    "    youchat_str += f\" this information about the task scope:\\n\\n{' '.join(task_strs_list)}\\n\\nand, 2) my resume:\\n\\n{resume_str}\"\n",
    "elif topic == 'question':\n",
    "    file_path = '../data/txt/resume.txt'\n",
    "    with open(file_path, 'r') as file:\n",
    "        resume_str = file.read().rstrip()\n",
    "    task_strs_list = []\n",
    "    for task_str in [child_str for pos_str, child_str in zip(pos_list, child_strs_list) if (pos_str in ['O-TS'])]:\n",
    "        task_strs_list.append(task_str)\n",
    "    company_name = child_strs_list[1]\n",
    "    youchat_str = f\"Pretend you have the competencies and experience listed on the resume. Explain in first person singular\"\n",
    "    youchat_str += \" how you manage projects, communicate with clients, discover their needs, make recommendations, stay in budget,\"\n",
    "    youchat_str += f\" manage changing requirements, and produce results, given this resume:\\n\\n{resume_str[75:]}\"\n",
    "elif topic == 'rejected':\n",
    "    job_title = 'Senior Data Engineering Analyst, Platform Engineering (Remote, Anywhere in US)'\n",
    "    recruiting_team_name = 'Humana Recruiting Team'\n",
    "    company_name = 'Humana'\n",
    "    youchat_str = f\"Write a reply to the {recruiting_team_name} rejection letter for the {job_title} position at {company_name},\"\n",
    "    youchat_str += \" persuading the recruiting team to explain in more detail why I was rejected for the role. Replace [Name] with\"\n",
    "    youchat_str += \" Dave Babbitt\"\n",
    "print(youchat_str)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b88e1ad3-c36f-4a2a-9d49-3b4d59404d98",
   "metadata": {},
   "source": [
    "\n",
    "----"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "662a0814-82bf-48b9-99b6-a87d1b4780f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Explain in first person singular why I would be interested in this role at <span>Here’s how the job details align with your job preferences.</span>, given\n",
      "\n",
      "1) this information about the task scope:\n",
      "\n",
      "who <b>likes getting their hands dirty and jumping in on IC</b> work. We first want to talk to folks who have managed people and done project management and feel comfortable in both. <li>Currently, the CTO and VP of Eng are doing filling this role and we're looking to have someone come in and lead this pod of 4 engineers. You would report to VP of Engineering.</li> <b>Remote is fine or onsite in Austin.</b> and more who <b>likes getting their hands dirty and jumping in on IC</b> work. We first want to talk to folks who have managed people and done project management and feel comfortable in both. <li>Currently, the CTO and VP of Eng are doing filling this role and we're looking to have someone come in and lead this pod of 4 engineers. You would report to VP of Engineering.</li> <b>Remote is fine or onsite in Austin.</b> and more\n",
      "\n",
      "and, 2) my resume:\n",
      "\n",
      "\n",
      "Dave Babbitt\n",
      "Data Scientist\n",
      "+1 413 244 9949\n",
      "dave.babbitt@gmail.com\n",
      "Clinton, MA 01510\n",
      "\n",
      "Willing to relocate to Tucson, AZ\n",
      "Authorized to work in the US for any employer\n",
      "\n",
      "Summary\n",
      "The last seven years have been spent analyzing large datasets and visualizing them in a way that statistically validated every hunch about that data. Not only that, I was able to work with domain experts within the IC to create predictive analytics and prescriptive simulations of such power and flexibility that I was able to answer every \"what if\" and \"how should I do this\" scenario that anybody cared to ask. Previously, I spent seventeen years of my career developing complicated web applications.\n",
      "\n",
      "Work experience\n",
      "\n",
      "Machine Learning Engineer\n",
      "BigBear.ai - Remote\n",
      "March 2023 to Present\n",
      "Added a Responder Type Column to the OSU dataset of FRVRS Logs, analyzed Gaze and Intent, Issue with Logging Multiple TOOL_APPLIEDs, Preliminary Research Questions, TOOL_HOVERing as Indicative of Next Patient Choice, found Negative Metrics in DCEMS Data, did Orientation-Normal Sequence Analysis, performed Tool Applied Exploration, fixed Elapsed Time Simultaneity, identified any Anomalous Files, renamed Files, replaced the tool applied sender missing patient ID, reserialized DataFrame pickles, developed the Correct Count Triage Accuracy Metric, the Number of Patients Treated Metric, the Number of Pulses Taken Metric, the Number of Voice Captures per Session Metric, the Patient Accuracy Rate Metric, the R-squared Triage Accuracy Metric, the Total Number of Teleports Metric, the Treatment Placement Error Metric, and the Triage Accuracy Metric, Plotted Pie Charts for Tag to SALT Dataset, Visualized every Player Gaze, Visualized Location Points, and Visualized Non-cumulative Teleportation Events: (words, get_modal_state, get_synchrony, get_subsequences, and get_routine_scores)\n",
      "Analyzed Deidentified Simulation Voice Captures, analyzed Time Spent on Task, calculated Results for Abstract Submission, developed the Number of Patients Engaged Metric, the Time to First Engagement Metric, the Time to First Treatment Metric, the Time To Hemorrhage Control Metric, the Total User Actions Taken Metric, the Triage Efficiency Metric, Plotted Grouped Box and Whiskers for Time to First Engagement, Visualized Elapsed Time Spent on Patient, and Visualized every Patient Interaction: (timedelta, words, get_first_positions, get_subsequences, and get_routine_scores)\n",
      "Analyzed Elapsed Time Estimation for Operations: (association_rules, apriori, timedelta, get_modal_state, and get_subsequences)\n",
      "Analyzed Patient Engaged Events: (apriori, association_rules, get_modal_state, get_synchrony, and get_subsequences)\n",
      "Analyzed START Triage vs SALT Triage: (path, timedelta, words, get_modal_state, and get_subsequences)\n",
      "Built a Patient Engagement timeline CSV: (apriori, get_modal_state, get_synchrony, get_subsequences, and get_routine_scores)\n",
      "Built a Model to Predict Tag Applied Type: (train_test_split, permutation_importance, words, get_modal_state, and get_subsequences)\n",
      "Built the OSU dataset of FRVRS Logs and wrote a How to Use the Data: (en_core_web_sm, words, get_modal_state, get_synchrony, and get_subsequences)\n",
      "Catsim Library Exploration: (catsim, icc, plot, generate_item_bank, and words)\n",
      "Developed the Correct Count by Tag Triage Accuracy Metric and Plot Stacked Horizontal Bar Charts for Over-Under Triage Errors: (seaborn, words, get_first_positions, get_subsequences, and get_routine_scores)\n",
      "Explored Bleeping Proper Nouns out of Audio Files: (soundfile, speech_recognition, words, get_first_positions, and get_subsequences)\n",
      "Explored Voice Capture Ngrams: (word_tokenize, words, subjectivity, stopwords, and wordnet)\n",
      "Completed Near-engagement Transactions for Affinity Analysis: (association_rules, apriori, get_modal_state, get_synchrony, and get_subsequences)\n",
      "Attempted Rasch Analysis: (generate_item_bank, words, get_modal_state, get_synchrony, and get_subsequences)\n",
      "Recomputed Visualizations for the SDMPH 2023 Poster: (arange, seaborn, timedelta, words, and get_modal_state)\n",
      "Renamed elapsed time columns in DataFrame pickles and Rename time group columns in DataFrame pickles: (path, words, get_modal_state, get_subsequences, and get_routine_scores)\n",
      "Replaced UUIDs with Cleaned and Revised File Info: (en_core_web_sm, path, words, get_modal_state, and get_subsequences)\n",
      "Did Walk-Wave Sequence Analysis: (get_synchrony, get_transition_matrix, get_all_ngrams, get_transitions, and plot_element_counts)\n",
      "\n",
      "Decision Scientist\n",
      "Meta - Menlo Park, CA\n",
      "September 2022 to March 2023\n",
      "Developed a pipeline for an Accenture client to evaluate a lead KPI indicator, including testing model assumptions, model feature engineering, model regression training, and model prediction visualizations. (SQL, Bento notebooks, Python, Pandas, NumPy)\n",
      "\n",
      "Job Hunter\n",
      "Machine Learning Pipeline - Remote\n",
      "March 2022 to September 2022\n",
      "Developed a machine learning pipeline to aid in job hunting by scraping job postings off the web using Selenium and BeautifulSoup (Selenium, BeautifulSoup, Python)\n",
      "Identified the minimum requirements section among the parent-tagged navigable text elements of the web or email (Bag-of-Words, TF-IDF, Logistic Regression, CFR, Python)\n",
      "Trained a model to determine which qualifications I met. (Bag-of-Words, TF-IDF, Logistic Regression, Python)\n",
      "Used a GPT-3 model to generate cover letters if I met 80% or more of the minimum requirements. (GPT-3, Python)\n",
      "Stored the Header Tags, File Names, Navigable Parents, Qualification Strings, and Parts of Speech as nodes in a graph database management system (Neo4j, Cypher, Flask, Pandas)\n",
      "\n",
      "Senior Data Scientist\n",
      "Kessel Run - Boston, MA\n",
      "May 2021 to March 2022\n",
      "Created a pipeline for a IT Concepts Inc. client that automatically retrieves JSON data via API from various sources, date-pivots and merges them, and commits them to a repository, Excel, and Opensearch for further analysis. (Excel, Opensearch, Pivotal Tracker, GitLab, Python, Pandas, NumPy)\n",
      "Conducted code complexity analysis on the commit histories of various git repositories. (Pivotal Tracker, GitLab, Python, Jupyter, Pandas)\n",
      "Created KPIs on datasets retrieved from various Pivotal Tracker and GitLab projects. (Pivotal Tracker, GitLab, Python, Jupyter, Pandas)\n",
      "Created various visualizations including line and bar plots. (Opensearch, Python, Jupyter, Pandas)\n",
      "Trained a Logistic Regression predictive model using Bag-of-Words and TF-IDF (Term Frequency â€“ Inverse Document Frequency, Pandas)\n",
      "\n",
      "Data Scientist\n",
      "Plateau Software Inc - Scott AFB, IL\n",
      "December 2020 to May 2021\n",
      "Determined the \"hits\" where a satellite had geodetic overlap of any vessel(s) at any point(s) in time. (For simplicity, used a standard Geodetic Overlap Accuracy model). (Python, Pandas, NumPy)\n",
      "Programmatically instantiated resources, performed data munging, uploaded to database, sent JSON data to UI to display hits. (AWS SAM/S3 data bucket manipulation and querying/Lambda, Docker, Python, Pandas)\n",
      "\n",
      "Job Hunter\n",
      "Machine Learning Pipeline - Remote\n",
      "October 2020 to December 2020\n",
      "Developed a machine learning pipeline to aid in job hunting by scraping job postings off the web using Selenium and BeautifulSoup (Selenium, BeautifulSoup, Python, Pandas)\n",
      "Identified the minimum requirements section among the parent-tagged navigable text elements of the web or email (Bag-of-Words, TF-IDF, Logistic Regression, CFR, Python, Pandas)\n",
      "Trained a model to determine which qualifications I met. (Bag-of-Words, TF-IDF, Logistic Regression, Python, Pandas)\n",
      "Used a GPT-3 model to generate cover letters if I met 80% or more of the minimum requirements. (GPT-3, Python, Pandas)\n",
      "Stored the Header Tags, File Names, Navigable Parents, and Parts of Speech as tables in a relational database management system (Microsoft SQL Server, SQL)\n",
      "\n",
      "Data Scientist\n",
      "Booz Allen Hamilton\n",
      "January 2015 to October 2020\n",
      "Developed data science capability for intelligence analysts supporting US Air Force airborne intelligence, surveillance, and reconnaissance systems and analytical processes.\n",
      "Analyzed large data processing problems and created tailored, repeatable, and scalable approaches to data manipulation and preparation for follow-on analysis.\n",
      "Researched and nominated unclassified and open information sources for data mining to support ongoing analytical efforts.\n",
      "Programmed logical interfaces and applied techniques for efficient program logic and data manipulation.\n",
      "Conducted systems programming and support activities, including new/revised/segments of language code and processing.\n",
      "Documented developed processes for data extraction, manipulation, analysis, and storage.\n",
      "Trained intelligence analysts on programming and mathematical facets of data science and execution of developed processes.\n",
      "\n",
      "Senior Systems Analyst\n",
      "Clipboard Solutions\n",
      "May 2000 to February 2006\n",
      "Designed and developed a health care payroll system; in use 10 years; exported to check writing software.\n",
      "Built an extensive time entry and scheduling system that incorporates mobile and biometric devices.\n",
      "\n",
      "Education\n",
      "\n",
      "MS Predictive Analytics, Northwestern University - Chicago, IL\n",
      "December 2012 to March 2015\n",
      "\n",
      "Bachelor of Science, Excelsior College - Albany, NY\n",
      "March 1991\n",
      "\n",
      "Skills\n",
      "AWS - 1 year\n",
      "XML - 2 years\n",
      "Machine Learning - 7 years\n",
      "Java - 2 years\n",
      "SQL - 10 years\n",
      "R - 2 years\n",
      "SVN - 1 year\n",
      "HTML5 - 2 years\n",
      "Git - 10 years\n",
      "Python - 7 years\n",
      "GitHub - 5 years\n",
      "APIs - 5 years\n",
      "Data Science - 7 years\n",
      "Analytics - 7 years\n",
      "REST - 2 years\n",
      "Linux - 10 years\n",
      "GitLab - 1 year\n",
      "JavaScript - 2 years\n",
      "MySQL - 3 years\n",
      "Agile - 1 year\n",
      "\n",
      "Languages\n",
      "English - Fluent\n",
      "\n",
      "Links\n",
      "https://github.com/dbabbitt/\n",
      "https://stackexchange.com/users/31594/dave-babbitt?tab=accounts\n",
      "\n",
      "Military service\n",
      "Branch: United States Navy\n",
      "Rank: ET2 (E-5)\n",
      "\n",
      "Certifications and licenses\n",
      "Machine Learning\n",
      "2017 to Present\n",
      "Security+ CE\n",
      "May 2019 to May 2022\n",
      "\n",
      "Assessments\n",
      "Analyzing data - PROFICIENT\n",
      "May 2021\n",
      "\n",
      "Publications\n",
      "\n",
      "The COVID-19 pandemic as experienced by the individual\n",
      "https://arxiv.org/abs/2005.01167\n",
      "September 2020\n",
      "Patrick Garland, Dave Babbitt, Maksym Bondarenko, Alessandro Sorichetta, Andrew J. Tatem, Oliver Johnson\n",
      "\n",
      "Multi-INT Correlation and Fusion in Watchman for Defense\n",
      "2019\n",
      "Pottenger, W.M., Nagy, J, Scott, S, VanHorn, S., Campbell, M., Stone, C., Shatzman, G., Gantt, N., Janneck, C.D., Kelly, C.A., Shin, S., Tong, T., Mann, R., Blasch, E., Babbitt, D. (2019) Multi-INT Correlation and Fusion in Watchman for Defense, 2019 National Symposium on Sensor and Data Fusion (NSSDF), San Diego, CA\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "No active exception to reraise",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[23], line 7\u001b[0m\n\u001b[0;32m      5\u001b[0m wsu\u001b[38;5;241m.\u001b[39mdriver_get_url(driver, youchat_url, verbose\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[0;32m      6\u001b[0m \u001b[38;5;28mprint\u001b[39m(youchat_str)\n\u001b[1;32m----> 7\u001b[0m winsound\u001b[38;5;241m.\u001b[39mBeep(freq, duration); \u001b[38;5;28;01mraise\u001b[39;00m\n",
      "\u001b[1;31mRuntimeError\u001b[0m: No active exception to reraise"
     ]
    }
   ],
   "source": [
    "\n",
    "import urllib.parse\n",
    "\n",
    "driver = wsu.get_driver(verbose=False)\n",
    "youchat_url = f'https://you.com/search?q={urllib.parse.quote_plus(youchat_str)}&fromSearchBar=true&tbm=youchat'\n",
    "wsu.driver_get_url(driver, youchat_url, verbose=False)\n",
    "print(youchat_str)\n",
    "wsu.beep(freq, duration); raise"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a26bd4fd-ce1f-42aa-9c76-c5db6ff80e00",
   "metadata": {},
   "source": [
    "\n",
    "### Check the back FireFox window to make sure the chat writing has stopped before running this next cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "d5e80757-79be-4d5f-806e-d42608a19381",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dear Hiring Manager,\n",
      "I am writing to apply for the Data Scientist London England United Kingdom Remote position. Although I only meet 80.0% of the minimum requirements listed, there are several aspects of my experience that I believe make me a strong candidate for this position.\n",
      "First, while I do not have any formal academic, postgraduate, or professional qualifications, I have extensive experience as a Data Scientist. In my previous roles, I have demonstrated the ability to apply predictive modelling algorithms, such as regressions, time series, and decision trees, to drive actions and improve business outcomes. I am also skilled in programming and database knowledge, with experience in AWS tech stack, MongoDB, and PostgreSQL.\n",
      "In addition, I have experience integrating multiple public and proprietary datasets, which I believe would be beneficial in this position. I am familiar with the trade-offs between model performance and business needs, and I excel at analyzing data to drive actions.\n",
      "I am excited about the opportunity to work with your team and would appreciate the chance to further discuss my qualifications with you. Thank you for your time and consideration.\n",
      "Sincerely,\n",
      "Dave Babbitt\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from selenium.common.exceptions import NoSuchElementException\n",
    "from selenium.webdriver.common.by import By\n",
    "\n",
    "css_selector = 'div[data-testid=\"youchat-answer-turn-0\"]'\n",
    "try:\n",
    "    web_element = driver.find_element(By.CSS_SELECTOR, css_selector)\n",
    "    print(web_element.text)\n",
    "except NoSuchElementException as e:\n",
    "    pass\n",
    "except Exception as e:\n",
    "    print(f'{e.__class__.__name__} error: {str(e).strip()}')\n",
    "finally:\n",
    "    driver.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "886daf59-7a95-4c27-ab2e-f9f8150be523",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Job Hunting (Python 3.10.9)",
   "language": "python",
   "name": "jh_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
