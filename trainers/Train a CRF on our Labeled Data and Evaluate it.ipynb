{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pretty printing has been turned OFF\n"
     ]
    }
   ],
   "source": [
    "\n",
    "%pprint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 8min 25s\n"
     ]
    }
   ],
   "source": [
    "\n",
    "%%time\n",
    "%run ../py/sql_utlis.py\n",
    "\n",
    "su = SqlUtilities()\n",
    "conn, cursor = su.get_jh_conn_cursor()\n",
    "su.build_child_strs_list_dictionary(cursor, verbose=False)\n",
    "su.create_header_pattern_dictionary(cursor, verbose=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Let's use our labeled data to build a NER system"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "assert su.s.pickle_exists('HEADER_PATTERN_DICT')\n",
    "HEADER_PATTERN_DICT = su.s.load_object('HEADER_PATTERN_DICT')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 130 ms\n"
     ]
    }
   ],
   "source": [
    "\n",
    "%%time\n",
    "%run ../py/html_analysis.py\n",
    "hc = HeaderCategories()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X = []\n",
    "y = []\n",
    "for file_name, feature_dict_list in HEADER_PATTERN_DICT.items():\n",
    "    X.append(feature_dict_list)\n",
    "    pos_list = [hc.get_feature_tuple(feature_dict)[2] for feature_dict in feature_dict_list]\n",
    "    y.append(pos_list)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, train_size=0.75, random_state=101)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training\n",
    "\n",
    "To see all possible CRF parameters check its docstring. Here we are useing L-BFGS training algorithm (it is default) with Elastic Net (L1 + L2) regularization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 4.34 s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\dev\\Documents\\Repositories\\job-hunting\\jh\\lib\\site-packages\\sklearn\\base.py:213: FutureWarning: From version 0.24, get_params will raise an AttributeError if a parameter cannot be retrieved as an instance attribute. Previously it would return None.\n",
      "  FutureWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "CRF(algorithm='lbfgs', all_possible_transitions=True, c1=0.1, c2=0.1,\n",
       "    keep_tempfiles=None, max_iterations=100)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "%%time\n",
    "import sklearn_crfsuite\n",
    "\n",
    "crf = sklearn_crfsuite.CRF(\n",
    "    algorithm='lbfgs', \n",
    "    c1=0.1, \n",
    "    c2=0.1, \n",
    "    max_iterations=100, \n",
    "    all_possible_transitions=True\n",
    ")\n",
    "crf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation\n",
    "\n",
    "There is much more O entities in data set, but we're more interested in other entities. To account for this we'll use averaged F1 score computed for all labels except for O. ``sklearn-crfsuite.metrics`` package provides some useful metrics for sequence classification task, including this one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['H', 'H-RQ', 'O', 'H-OL', 'H-TS', 'O-RQ', 'O-LN', 'H-SP', 'O-SP', 'H-JD', 'O-OL', 'H-ER', 'O-ER', 'H-CS', 'O-CS', 'H-PQ', 'H-IP', 'O-TS', 'H-LN', 'H-JT', 'O-IP', 'H-PD', 'H-O', 'O-PQ', 'O-O']"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "labels = list(crf.classes_)\n",
    "labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\dev\\Documents\\Repositories\\job-hunting\\jh\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1465: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no true nor predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  average, \"true nor predicted\", 'F-score is', len(true_sum)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.991907836016953"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "from sklearn_crfsuite import metrics\n",
    "\n",
    "y_pred = crf.predict(X_test)\n",
    "metrics.flat_f1_score(y_test, y_pred, \n",
    "                      average='weighted', labels=labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Inspect per-class results in more detail:"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "              precision    recall  f1-score   support\n",
    "\n",
    "           H      0.957     1.000     0.978        67\n",
    "           O      0.997     1.000     0.999       766\n",
    "        H-CS      1.000     1.000     1.000        11\n",
    "        H-ER      1.000     1.000     1.000        11\n",
    "        H-IP      1.000     1.000     1.000         4\n",
    "        H-JD      1.000     1.000     1.000        17\n",
    "        H-JT      0.000     0.000     0.000         1\n",
    "        H-LN      0.000     0.000     0.000         2\n",
    "         H-O      0.000     0.000     0.000         0\n",
    "        H-OL      1.000     1.000     1.000        30\n",
    "        H-PD      0.000     0.000     0.000         0\n",
    "        H-PQ      1.000     1.000     1.000         4\n",
    "        H-RQ      1.000     1.000     1.000        23\n",
    "        O-RQ      0.000     0.000     0.000         2\n",
    "        H-SP      1.000     1.000     1.000        31\n",
    "        O-SP      1.000     1.000     1.000        28\n",
    "        H-TS      1.000     1.000     1.000        20\n",
    "\n",
    "   micro avg      0.995     0.995     0.995      1017\n",
    "   macro avg      0.703     0.706     0.705      1017\n",
    "weighted avg      0.990     0.995     0.993      1017"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           H      0.956     1.000     0.977        65\n",
      "           O      0.996     1.000     0.998       724\n",
      "        H-CS      1.000     1.000     1.000        11\n",
      "        O-CS      1.000     0.500     0.667         2\n",
      "        H-ER      1.000     1.000     1.000        11\n",
      "        O-ER      0.000     0.000     0.000         0\n",
      "        H-IP      1.000     1.000     1.000         4\n",
      "        O-IP      0.000     0.000     0.000         0\n",
      "        H-JD      1.000     1.000     1.000        17\n",
      "        H-JT      1.000     0.667     0.800         3\n",
      "        H-LN      0.000     0.000     0.000         2\n",
      "        O-LN      0.000     0.000     0.000         0\n",
      "         H-O      0.000     0.000     0.000         0\n",
      "         O-O      0.000     0.000     0.000         0\n",
      "        H-OL      1.000     1.000     1.000        30\n",
      "        O-OL      1.000     1.000     1.000        15\n",
      "        H-PD      0.000     0.000     0.000         0\n",
      "        H-PQ      1.000     1.000     1.000         4\n",
      "        O-PQ      0.000     0.000     0.000         2\n",
      "        H-RQ      1.000     1.000     1.000        23\n",
      "        O-RQ      1.000     1.000     1.000         9\n",
      "        H-SP      1.000     1.000     1.000        31\n",
      "        O-SP      1.000     1.000     1.000        41\n",
      "        H-TS      1.000     1.000     1.000        20\n",
      "        O-TS      1.000     1.000     1.000         3\n",
      "\n",
      "   micro avg      0.994     0.994     0.994      1017\n",
      "   macro avg      0.678     0.647     0.658      1017\n",
      "weighted avg      0.990     0.994     0.992      1017\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\dev\\Documents\\Repositories\\job-hunting\\jh\\lib\\site-packages\\sklearn\\utils\\validation.py:70: FutureWarning: Pass labels=['H', 'O', 'H-CS', 'O-CS', 'H-ER', 'O-ER', 'H-IP', 'O-IP', 'H-JD', 'H-JT', 'H-LN', 'O-LN', 'H-O', 'O-O', 'H-OL', 'O-OL', 'H-PD', 'H-PQ', 'O-PQ', 'H-RQ', 'O-RQ', 'H-SP', 'O-SP', 'H-TS', 'O-TS'] as keyword args. From version 0.25 passing these as positional arguments will result in an error\n",
      "  FutureWarning)\n",
      "C:\\Users\\dev\\Documents\\Repositories\\job-hunting\\jh\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1221: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\dev\\Documents\\Repositories\\job-hunting\\jh\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1221: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Group results by type\n",
    "sorted_labels = sorted(\n",
    "    labels, \n",
    "    key=lambda name: (name[1:], name[0])\n",
    ")\n",
    "print(metrics.flat_classification_report(\n",
    "    y_test, y_pred, labels=sorted_labels, digits=3\n",
    "))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Job Hunting (Python 3.7.9)",
   "language": "python",
   "name": "jh"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
