{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pretty printing has been turned OFF\n"
     ]
    }
   ],
   "source": [
    "\n",
    "%pprint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import sys\n",
    "\n",
    "# Insert at 1, 0 is the script path (or '' in REPL)\n",
    "sys.path.insert(1, '../py')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Get the Neo4j driver\n",
    "from storage import Storage\n",
    "s = Storage()\n",
    "\n",
    "from ha_utils import HeaderAnalysis\n",
    "ha = HeaderAnalysis(s=s, verbose=False)\n",
    "\n",
    "from scrape_utils import WebScrapingUtilities\n",
    "wsu = WebScrapingUtilities(s=s)\n",
    "uri = wsu.secrets_json['neo4j']['connect_url']\n",
    "user =  wsu.secrets_json['neo4j']['username']\n",
    "password = wsu.secrets_json['neo4j']['password']\n",
    "\n",
    "from cypher_utils import CypherUtilities\n",
    "cu = CypherUtilities(uri=uri, user=user, password=password, driver=None, s=s, ha=ha)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from neo4j.exceptions import ServiceUnavailable\n",
    "\n",
    "try:\n",
    "    version_str = cu.driver.verify_connectivity()\n",
    "    \n",
    "    from hc_utils import HeaderCategories\n",
    "    hc = HeaderCategories(cu=cu, verbose=False)\n",
    "    \n",
    "    import warnings\n",
    "    warnings.filterwarnings('ignore')\n",
    "except ServiceUnavailable as e:\n",
    "    # print(str(e).strip())\n",
    "    raise ServiceUnavailable('You need to start Neo4j as a console')\n",
    "except Exception as e:\n",
    "    print(e.__class__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Let's use our labeled data to build a NER system"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: total: 10.3 s\n",
      "Wall time: 3min 48s\n"
     ]
    }
   ],
   "source": [
    "\n",
    "%%time\n",
    "cu.build_child_strs_list_dictionary(verbose=False)\n",
    "HEADER_PATTERN_DICT = cu.create_header_pattern_dictionary(verbose=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X = []\n",
    "y = []\n",
    "for file_name, feature_dict_list in HEADER_PATTERN_DICT.items():\n",
    "    X.append(feature_dict_list)\n",
    "    pos_list = [hc.get_feature_tuple(feature_dict, pos_lr_predict_single=None, pos_crf_predict_single=None, pos_sgd_predict_single=None)[2] for feature_dict in feature_dict_list]\n",
    "    y.append(pos_list)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, train_size=0.75, random_state=101)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training\n",
    "\n",
    "To see all possible CRF parameters check its docstring. Here we are useing L-BFGS training algorithm (it is default) with Elastic Net (L1 + L2) regularization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: total: 656 ms\n",
      "Wall time: 719 ms\n"
     ]
    }
   ],
   "source": [
    "\n",
    "%%time\n",
    "import sklearn_crfsuite\n",
    "\n",
    "crf = sklearn_crfsuite.CRF(\n",
    "    algorithm='lbfgs', \n",
    "    c1=0.1, \n",
    "    c2=0.1, \n",
    "    max_iterations=100, \n",
    "    all_possible_transitions=True\n",
    ")\n",
    "try:\n",
    "    crf.fit(X_train, y_train)\n",
    "except AttributeError as e:\n",
    "    print(str(e).strip())\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation\n",
    "\n",
    "There is much more O entities in data set, but we're more interested in other entities. To account for this we'll use averaged F1 score computed for all labels except for O. ``sklearn-crfsuite.metrics`` package provides some useful metrics for sequence classification task, including this one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['O-RQ', 'H', 'O', 'O-TS', 'H-RQ', 'O-JD', 'O-SP', 'H-SP', 'H-JD', 'H-ER', 'O-PQ', 'H-OL', 'O-OL', 'H-LN', 'H-TS', 'O-O', 'H-PQ', 'H-JT', 'O-CS', 'O-LN', 'H-CS', 'H-IP', 'O-ER', 'O-JT', 'O-IP', 'H-O', 'O-PD']"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "labels = list(crf.classes_)\n",
    "labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9950714267339033"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "from sklearn_crfsuite.metrics import flat_f1_score\n",
    "\n",
    "y_pred = crf.predict(X_test)\n",
    "flat_f1_score(y_test, y_pred, average='weighted', labels=labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Inspect per-class results in more detail:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           H       0.97      1.00      0.99        34\n",
      "           O       1.00      1.00      1.00       303\n",
      "        H-CS       1.00      1.00      1.00         1\n",
      "        O-CS       1.00      1.00      1.00         3\n",
      "        H-ER       0.90      0.90      0.90        10\n",
      "        O-ER       1.00      0.75      0.86         4\n",
      "        H-IP       1.00      1.00      1.00         3\n",
      "        O-IP       0.00      0.00      0.00         0\n",
      "        H-JD       1.00      1.00      1.00        11\n",
      "        O-JD       1.00      1.00      1.00        28\n",
      "        H-JT       0.00      0.00      0.00         0\n",
      "        O-JT       0.00      0.00      0.00         1\n",
      "        H-LN       1.00      1.00      1.00         4\n",
      "        O-LN       1.00      1.00      1.00         2\n",
      "         H-O       1.00      1.00      1.00         2\n",
      "         O-O       1.00      1.00      1.00         1\n",
      "        H-OL       1.00      1.00      1.00        20\n",
      "        O-OL       1.00      1.00      1.00        22\n",
      "        O-PD       0.00      0.00      0.00         0\n",
      "        H-PQ       1.00      1.00      1.00         6\n",
      "        O-PQ       1.00      1.00      1.00        29\n",
      "        H-RQ       1.00      1.00      1.00        31\n",
      "        O-RQ       1.00      1.00      1.00        76\n",
      "        H-SP       1.00      1.00      1.00        17\n",
      "        O-SP       1.00      1.00      1.00        93\n",
      "        H-TS       1.00      1.00      1.00        15\n",
      "        O-TS       1.00      1.00      1.00         7\n",
      "\n",
      "   micro avg       1.00      1.00      1.00       723\n",
      "   macro avg       0.85      0.84      0.84       723\n",
      "weighted avg       0.99      1.00      1.00       723\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Group results by type\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn_crfsuite.utils import flatten\n",
    "\n",
    "sorted_labels = sorted(\n",
    "    labels, \n",
    "    key=lambda name: (name[1:], name[0])\n",
    ")\n",
    "print(classification_report(y_true=flatten(y_test), y_pred=flatten(y_pred), labels=sorted_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
