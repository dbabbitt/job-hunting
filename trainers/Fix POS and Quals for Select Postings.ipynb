{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7675b025-b593-4d53-9759-6c1e01772c22",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pretty printing has been turned OFF\n"
     ]
    }
   ],
   "source": [
    "\n",
    "%pprint"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ed3ad81-7ac1-4a85-ba90-62ccbde24f49",
   "metadata": {
    "tags": []
   },
   "source": [
    "\n",
    "---\n",
    "# Load needed libraries and functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e1febade-58e7-47f1-8c6d-5447b5737d57",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from IPython.display import clear_output\n",
    "from datetime import datetime\n",
    "from neo4j.exceptions import ServiceUnavailable\n",
    "from pandas import DataFrame\n",
    "import humanize\n",
    "import numpy as np\n",
    "import os\n",
    "import sys\n",
    "import time\n",
    "import warnings\n",
    "import winsound\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "duration = 1000  # milliseconds\n",
    "freq = 880  # Hz\n",
    "\n",
    "# Insert at 1, 0 is the script path (or '' in REPL)\n",
    "sys.path.insert(1, '../py')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ff7aa81f-314d-4d0d-a238-355d5210f157",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======== Neo4j/4.4.7 ========\n",
      "Pickling to C:\\Users\\daveb\\OneDrive\\Documents\\GitHub\\job-hunting\\saves\\pkl\\basic_quals_dict.pkl\n",
      "I have 10,635 labeled parts of speech in here\n",
      "Got this <class 'numpy.core._exceptions._ArrayMemoryError'> error in build_pos_logistic_regression_elements trying  to turn the pos_symbol TF-IDF matrix into a normal array: Unable to allocate 31.7 GiB for an array with shape (10635, 400406) and data type float64\n",
      "Utility libraries created in 1 hour, 3 minutes and 8 seconds\n",
      "Last run on 2023-03-08 21:45:52.699876\n"
     ]
    }
   ],
   "source": [
    "\n",
    "t0 = time.time()\n",
    "\n",
    "# Get the Neo4j driver\n",
    "from storage import Storage\n",
    "s = Storage(\n",
    "    data_folder_path=os.path.abspath('../data'),\n",
    "    saves_folder_path=os.path.abspath('../saves')\n",
    ")\n",
    "\n",
    "from ha_utils import HeaderAnalysis\n",
    "ha = HeaderAnalysis(s=s, verbose=False)\n",
    "\n",
    "from scrape_utils import WebScrapingUtilities\n",
    "wsu = WebScrapingUtilities(\n",
    "    s=s,\n",
    "    secrets_json_path=os.path.abspath('../data/secrets/jh_secrets.json')\n",
    ")\n",
    "uri = wsu.secrets_json['neo4j']['connect_url']\n",
    "user =  wsu.secrets_json['neo4j']['username']\n",
    "password = wsu.secrets_json['neo4j']['password']\n",
    "\n",
    "# Get the neo4j object\n",
    "from cypher_utils import CypherUtilities\n",
    "cu = CypherUtilities(\n",
    "    uri=uri, user=user, password=password, driver=None, s=s, ha=ha\n",
    ")\n",
    "\n",
    "try:\n",
    "        \n",
    "    version_str = cu.driver.get_server_info().agent\n",
    "    print(f'======== {version_str} ========')\n",
    "    \n",
    "    from hc_utils import HeaderCategories\n",
    "    hc = HeaderCategories(cu=cu, verbose=False)\n",
    "    \n",
    "    # Keep the total creation time to less than one hour by adjusting the sampling strategy limit\n",
    "    from lr_utils import LrUtilities\n",
    "    lru = LrUtilities(ha=ha, cu=cu, hc=hc, verbose=False)\n",
    "    lru.build_isheader_logistic_regression_elements(verbose=False)\n",
    "    lru.build_isqualified_logistic_regression_elements(sampling_strategy_limit=5_000, verbose=False)\n",
    "    \n",
    "    # sampling_strategy_limit=6_400 gets 10,635 labeled parts of speech and takes 49 minutes and 30 seconds\n",
    "    # sampling_strategy_limit=7_000 gets 10,635 labeled parts of speech and takes 2 hours, 3 minutes and 35 seconds\n",
    "    lru.build_pos_logistic_regression_elements(sampling_strategy_limit=6_400, verbose=True)\n",
    "    \n",
    "    from crf_utils import CrfUtilities\n",
    "    crf = CrfUtilities(ha=ha, hc=hc, cu=cu, lru=lru, verbose=True)\n",
    "    \n",
    "    from section_utils import SectionUtilities\n",
    "    su = SectionUtilities(s=s, ha=ha, wsu=wsu, cu=cu, crf=crf, verbose=False)\n",
    "except ServiceUnavailable as e:\n",
    "    print('You need to start Neo4j as a console')\n",
    "    raise\n",
    "except Exception as e:\n",
    "    print(f'{e.__class__}: {str(e).strip()}')\n",
    "duration_str = humanize.precisedelta(time.time() - t0, minimum_unit='seconds', format='%0.0f')\n",
    "winsound.Beep(freq, duration)\n",
    "print(f'Utility libraries created in {duration_str}')\n",
    "print(f'Last run on {datetime.now()}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "268c479f-c162-454d-b4a0-cccb123c567f",
   "metadata": {},
   "source": [
    "\n",
    "---\n",
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e57ef681-27d5-4610-8496-ff1d30b09c89",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# You need to run this again if you changed the qualification dictionary in another notebook\n",
    "t0 = time.time()\n",
    "\n",
    "# Keep the total retraining time to less than two minutes by adjusting the sampling strategy limit\n",
    "# sampling_strategy_limit=9_000 gets 11,365 hand-labeled qualification strings and takes 2 minutes and 25 seconds\n",
    "basic_quals_dict = lru.sync_basic_quals_dict(sampling_strategy_limit=8_000, verbose=False)\n",
    "\n",
    "lru.retrain_isqualified_classifier(verbose=True)\n",
    "duration_str = humanize.precisedelta(time.time() - t0, minimum_unit='seconds', format='%0.0f')\n",
    "print(f'Is-qualified classifer retrained in {duration_str}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82dab229-b28c-4ddd-812d-6abf153aae3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from pandas import DataFrame\n",
    "\n",
    "cypher_str = f'''\n",
    "    MATCH (fn:FileNames)\n",
    "    WHERE\n",
    "        fn.percent_fit = 0.0 AND\n",
    "        ((fn.is_closed IS NULL) OR (fn.is_closed = false)) AND\n",
    "        ((fn.is_verified IS NULL) OR (fn.is_verified = false)) AND\n",
    "        ((fn.is_opportunity_application_emailed IS NULL) OR\n",
    "        (fn.is_opportunity_application_emailed = false))\n",
    "    RETURN\n",
    "        fn.percent_fit AS percent_fit,\n",
    "        fn.file_name AS file_name,\n",
    "        fn.posting_url AS url\n",
    "    ORDER BY fn.percent_fit ASC;'''\n",
    "row_objs_list = []\n",
    "with cu.driver.session() as session:\n",
    "    row_objs_list = session.write_transaction(cu.do_cypher_tx, cypher_str)\n",
    "files_list = []\n",
    "if row_objs_list:\n",
    "    files_list = DataFrame(row_objs_list).file_name.tolist()\n",
    "print(f'Only {len(files_list)} more mis-estimated minimum-requirements-met percentages to go!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7dfb9e50-c827-4656-9a8a-59258c69edd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "cypher_str = f'''\n",
    "    MATCH (fn:FileNames)\n",
    "    WHERE\n",
    "        (toLower(fn.file_name) CONTAINS \"data_scien\")\n",
    "        AND (fn.role_title IS NOT NULL)\n",
    "        AND ((fn.is_closed IS NULL) OR (fn.is_closed = false))\n",
    "        AND ((fn.is_verified IS NULL) OR (fn.is_verified = false))\n",
    "        AND ((fn.is_opportunity_application_emailed IS NULL) OR\n",
    "            (fn.is_opportunity_application_emailed = false))\n",
    "    RETURN\n",
    "        fn.percent_fit AS percent_fit,\n",
    "        fn.file_name AS file_name\n",
    "    ORDER BY fn.percent_fit ASC;'''\n",
    "row_objs_list = []\n",
    "with cu.driver.session() as session:\n",
    "    row_objs_list = session.write_transaction(cu.do_cypher_tx, cypher_str)\n",
    "files_list = []\n",
    "if row_objs_list:\n",
    "    files_list = DataFrame(row_objs_list).file_name.tolist()\n",
    "print(f'Only {len(files_list)} more mis-estimated minimum-requirements-met percentages to go!')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbed91ef-2d2e-46c9-be8b-f6458185fad8",
   "metadata": {},
   "source": [
    "\n",
    "----\n",
    "## Fix POS and Quals for this posting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 288,
   "id": "151cdf71-a078-43b6-9a1b-649bb51461e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8eda6f13ecba3a8c_Senior_Associate_Manager_Digital_Tax_Services_Washington_DC_20005_Indeed_com.html\n"
     ]
    }
   ],
   "source": [
    "\n",
    "file_name = '8eda6f13ecba3a8c_Senior_Associate_Manager_Digital_Tax_Services_Washington_DC_20005_Indeed_com.html'\n",
    "# file_name = files_list.pop()\n",
    "file_path = os.path.join(cu.SAVES_HTML_FOLDER, file_name)\n",
    "if os.path.isfile(file_path):\n",
    "    child_strs_list = ha.get_child_strs_from_file(file_name=file_name)\n",
    "    cu.ensure_filename(file_name, verbose=False)\n",
    "    cu.populate_from_child_strings(child_strs_list, file_name, verbose=False)\n",
    "    child_tags_list = ha.get_child_tags_list(child_strs_list)\n",
    "    is_header_list = []\n",
    "    for is_header, child_str in zip(ha.get_is_header_list(child_strs_list), child_strs_list):\n",
    "        if is_header is None:\n",
    "            probs_list = lru.ISHEADER_PREDICT_PERCENT_FIT(child_str)\n",
    "            idx = probs_list.index(max(probs_list))\n",
    "            is_header = [True, False][idx]\n",
    "        is_header_list.append(is_header)\n",
    "    feature_dict_list = hc.get_feature_dict_list(child_tags_list, is_header_list, child_strs_list)\n",
    "    feature_tuple_list = []\n",
    "    for feature_dict in feature_dict_list:\n",
    "        feature_tuple_list.append(hc.get_feature_tuple(feature_dict, pos_lr_predict_single=lru.pos_lr_predict_single, pos_crf_predict_single=None))\n",
    "    crf_list = crf.CRF.predict_single(crf.sent2features(feature_tuple_list))\n",
    "    print(file_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 289,
   "id": "3e8afe8b-e9cc-4dca-866e-88cf37461f2f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['H-JT', 'O-IP', 'O-IP', 'O-IP', 'O-IP', 'O-IP', 'H-JD', 'O-JD', 'H-SP', 'H-SP', 'O-SP', 'O-SP', 'H-TS', 'H-TS', 'H-TS', 'O-TS', 'O-RQ', 'O-TS', 'O-TS', 'O-RQ', 'O-TS', 'O-TS', 'H-TS', 'O-TS', 'H-RQ', 'O-RQ', 'O-RQ', 'O-RQ', 'O-RQ', 'O-PQ', 'O-RQ', 'O-RQ', 'H-O', 'H-O', 'O-PD']\n",
      "[16, 19, 25, 26, 27, 28, 30, 31]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "0 H-JT) <span style=\"color:#d62728ff;\"><h2 class=\"css-tmzs7i e1tiznh50\" id=\"jobDetails\" tabindex=\"-1\">Job details (H-JT Job Title Header)</h2></span><br />1 O-IP) <span style=\"color:#ffbb7880;\">Matches (O-IP Interview Procedures Non-header)</span><br />2 O-IP) <span style=\"color:#ffbb7880;\">job preferences (O-IP Interview Procedures Non-header)</span><br />3 O-IP) <span style=\"color:#ffbb7880;\">you are (O-IP Interview Procedures Non-header)</span><br />4 O-IP) <span style=\"color:#ffbb7880;\"><span class=\"css-1j9n3ml eu4oa1w0\">Interested (O-IP Interview Procedures Non-header)</span></span><br />5 O-IP) <span style=\"color:#ffbb7880;\">in (O-IP Interview Procedures Non-header)</span><br />6 H-JD) <span style=\"color:#98df8aff;\">Job Type (H-JD Job Duration Header)</span><br />7 O-JD) <span style=\"color:#98df8a80;\"><div class=\"css-tvvxwd ecydgvn1\">Full-time (O-JD Job Duration Non-header)</div></span><br />8 H-SP) <span style=\"color:#17becfff;\"><h2 class=\"jobsearch-JobDescriptionSection-jobDescriptionTitle\">Indeed's salary guide (H-SP Supplemental Pay Header)</h2></span><br />9 H-SP) <span style=\"color:#17becfff;\"><li class=\"css-vktqis eu4oa1w0\">Not provided by employer (H-SP Supplemental Pay Header)</li></span><br />10 O-SP) <span style=\"color:#17becf80;\">$123K - $155K a year is Indeed's estimated salary for this role in Washington, DC. (O-SP Supplemental Pay Non-header)</span><br />11 O-SP) <span style=\"color:#17becf80;\"><span aria-hidden=\"false\" class=\"css-h95u2o e1wnkr790\">Report inaccurate salary (O-SP Supplemental Pay Non-header)</span></span><br />12 H-TS) <span style=\"color:#9edae5ff;\"><h2 class=\"jobsearch-JobDescriptionSection-jobDescriptionTitle icl-u-xs-my--md\" id=\"jobDescriptionTitle\">Full Job Description (H-TS Task Scope Header)</h2></span><br />13 H-TS) <span style=\"color:#9edae5ff;\"><b>Description (H-TS Task Scope Header)</b></span><br />14 H-TS) <span style=\"color:#9edae5ff;\"><b>Responsibilities: (H-TS Task Scope Header)</b></span><br />15 O-TS) <span style=\"color:#9edae580;\"><li>Collecting and organizing large datasets from various sources. (O-TS Task Scope Non-header)</li></span><br /><hr />16 O-RQ) <span style=\"color:#bcbd2280;\"><li>Cleaning and validating data to ensure accuracy and consistency. (O-RQ Required Qualifications Non-header)</li></span><br />17 O-TS) <span style=\"color:#9edae580;\"><li>Setting up scheduled data flows between data sources and repositories. (O-TS Task Scope Non-header)</li></span><br />18 O-TS) <span style=\"color:#9edae580;\"><li>Designing, developing, and administrating databases. (O-TS Task Scope Non-header)</li></span><br />19 O-RQ) <span style=\"color:#bcbd2280;\"><li>Analyzing data to identify trends, patterns, and insights. (O-RQ Required Qualifications Non-header)</li></span><br />20 O-TS) <span style=\"color:#9edae580;\"><li>Creating dashboards, reports, and visualizations to communicate findings to stakeholders. (O-TS Task Scope Non-header)</li></span><br />21 O-TS) <span style=\"color:#9edae580;\"><li>Communicating findings to stakeholders in a clear and concise manner through visualization, reports, and patterns. (O-TS Task Scope Non-header)</li></span><br />22 H-TS) <span style=\"color:#9edae5ff;\"><li>Collaborating with cross-functional teams to provide data-driven insights and recommendations. (H-TS Task Scope Header)</li></span><br />23 O-TS) <span style=\"color:#9edae580;\"><li>Continuously monitoring data quality and making improvements as needed. (O-TS Task Scope Non-header)</li></span><br />24 H-RQ) <span style=\"color:#bcbd22ff;\"><b>Qualifications: (H-RQ Required Qualifications Header)</b></span><br />25 O-RQ) <span style=\"color:#bcbd2280;\"><li>Bachelor's or Master's degree in a quantitative field such as statistics, mathematics, engineering, or computer science. (O-RQ Required Qualifications Non-header)</li></span><br />26 O-RQ) <span style=\"color:#bcbd2280;\"><li>Proficiency in programming languages such as SQL, Python, Pandas, or NumPy. (O-RQ Required Qualifications Non-header)</li></span><br />27 O-RQ) <span style=\"color:#bcbd2280;\"><li>Proficiency in data visualization tools such PowerBI and Tableau. (O-RQ Required Qualifications Non-header)</li></span><br />28 O-RQ) <span style=\"color:#bcbd2280;\"><li>Familiarity with machine learning algorithms and techniques such as clustering and classification. (O-RQ Required Qualifications Non-header)</li></span><br />29 O-PQ) <span style=\"color:#c7c7c780;\"><li>Experience with data analysis tools and techniques such as Alteryx is a plus, but not required. (O-PQ Preferred Qualifications Non-header)</li></span><br />30 O-RQ) <span style=\"color:#bcbd2280;\"><li>Strong problem-solving skills and attention to detail. (O-RQ Required Qualifications Non-header)</li></span><br />31 O-RQ) <span style=\"color:#bcbd2280;\"><li>Excellent communication skills and ability to work collaboratively with others. (O-RQ Required Qualifications Non-header)</li></span><br /><hr />32 H-O) <span style=\"color:#8c564bff;\"><h2 class=\"css-14vqcyj e1tiznh50\">Hiring Insights (H-O Other Header)</h2></span><br />33 H-O) <span style=\"color:#8c564bff;\"><h3 class=\"css-1s8hy3a e1tiznh50\">Job activity (H-O Other Header)</h3></span><br />34 O-PD) <span style=\"color:#f7b6d280;\"><span class=\"css-kyg8or eu4oa1w0\">Posted Today (O-PD Post Date Non-header)</span></span><br />"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[16, 19, 25, 26, 27, 28, 30, 31]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "db_pos_list = []\n",
    "for navigable_parent in child_strs_list:\n",
    "    db_pos_list = cu.append_parts_of_speech_list(navigable_parent, pos_list=db_pos_list)\n",
    "pos_list, indices_list = su.visualize_basic_quals_section(crf_list, child_strs_list, db_pos_list=db_pos_list, verbose=True)\n",
    "winsound.Beep(freq, duration)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7070cc54-d512-456d-92fc-bd7bdb3f3e43",
   "metadata": {},
   "outputs": [],
   "source": [
    "raise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 294,
   "id": "1d97acb2-2b60-4a56-9b3b-ecfc2b6582e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[16, 19, 25, 26, 27, 28, 30, 31]\n",
      "0\n",
      "27 O-RQ) <li>Proficiency in data visualization tools such PowerBI and Tableau.</li>\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Display the context of an individual child string\n",
    "idx = 27\n",
    "print(indices_list); child_str = child_strs_list[idx]; pos_symbol = pos_list[idx]; basic_quals_dict = s.load_object('basic_quals_dict')\n",
    "print(str(basic_quals_dict[child_str]) + '\\n' if(child_str in basic_quals_dict) else '', end=''); print(f'{idx} {pos_symbol}) {child_str}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 275,
   "id": "3f1b210b-8651-4291-8c80-046a9c896392",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pickling to C:\\Users\\daveb\\OneDrive\\Documents\\GitHub\\job-hunting\\saves\\pkl\\basic_quals_dict.pkl\n",
      "\"<li>Experience with Operations Research or mathematical optimization.</li>\" in basic_quals_dict: 0\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Hand-label this particular child string in the quals dictionary\n",
    "basic_quals_dict = s.load_object('basic_quals_dict')\n",
    "basic_quals_dict[child_str] = 0\n",
    "s.store_objects(basic_quals_dict=basic_quals_dict); print(f'\"{child_str}\" in basic_quals_dict: {basic_quals_dict[child_str]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 293,
   "id": "5bb3f749-0a02-4266-876d-08983725feb2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'navigable_parent': '<li>Analyzing data to identify trends, patterns, and insights.</li>', 'is_header': 'False', 'is_task_scope': 'True', 'is_qualification': None, 'is_minimum_qualification': 'False', 'is_preferred_qualification': 'False', 'is_legal_notification': 'False', 'is_job_title': 'False', 'is_office_location': 'False', 'is_job_duration': 'False', 'is_supplemental_pay': 'False', 'is_educational_requirement': 'False', 'is_interview_procedure': 'False', 'is_corporate_scope': 'False', 'is_posting_date': 'False', 'is_other': 'False'}]"
      ]
     },
     "execution_count": 293,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "def do_cypher_tx(tx, navigable_parent, verbose=False):\n",
    "    cypher_str = \"\"\"MATCH (np:NavigableParents {navigable_parent: $navigable_parent})\n",
    "        SET\n",
    "            np.is_header = 'False',\n",
    "            np.is_task_scope = 'True',\n",
    "            np.is_minimum_qualification = 'False',\n",
    "            np.is_preferred_qualification = 'False',\n",
    "            np.is_educational_requirement = 'False',\n",
    "            np.is_legal_notification = 'False',\n",
    "            np.is_other = 'False',\n",
    "            np.is_corporate_scope = 'False',\n",
    "            np.is_job_title = 'False',\n",
    "            np.is_office_location = 'False',\n",
    "            np.is_job_duration = 'False',\n",
    "            np.is_supplemental_pay = 'False',\n",
    "            np.is_interview_procedure = 'False',\n",
    "            np.is_posting_date = 'False'\n",
    "        \"\"\" + cu.return_everything_str + ';'\n",
    "    results_list = tx.run(query=cypher_str, parameters={'navigable_parent': navigable_parent})\n",
    "\n",
    "    return [dict(record.items()) for record in results_list]\n",
    "with cu.driver.session() as session:\n",
    "    row_objs_list = session.write_transaction(do_cypher_tx, navigable_parent=child_str)\n",
    "row_objs_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 295,
   "id": "28836b66-b8d4-48cb-92a0-f28efbb39c8e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'navigable_parent': '<li>Proficiency in data visualization tools such PowerBI and Tableau.</li>', 'is_header': 'False', 'is_task_scope': 'False', 'is_qualification': None, 'is_minimum_qualification': 'True', 'is_preferred_qualification': 'False', 'is_legal_notification': 'False', 'is_job_title': 'False', 'is_office_location': 'False', 'is_job_duration': 'False', 'is_supplemental_pay': 'False', 'is_educational_requirement': 'False', 'is_interview_procedure': 'False', 'is_corporate_scope': 'False', 'is_posting_date': 'False', 'is_other': 'False'}]"
      ]
     },
     "execution_count": 295,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "# Show what's in the database already for this html string\n",
    "def do_cypher_tx(tx, navigable_parent, verbose=False):\n",
    "    cypher_str = '''MATCH (np:NavigableParents {navigable_parent: $navigable_parent})\n",
    "        ''' + cu.return_everything_str + ';'\n",
    "    results_list = tx.run(query=cypher_str, parameters={'navigable_parent': navigable_parent})\n",
    "\n",
    "    return [dict(record.items()) for record in results_list]\n",
    "with cu.driver.session() as session:\n",
    "    row_objs_list = session.write_transaction(do_cypher_tx, navigable_parent=child_str)\n",
    "row_objs_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "id": "c767d2cb-2c13-421a-b29e-546d2b1b401f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pickling to C:\\Users\\daveb\\OneDrive\\Documents\\GitHub\\job-hunting\\saves\\pkl\\basic_quals_dict.pkl\n",
      "\"<b>REQUIRED QUALIFICATIONS</b>\" in basic_quals_dict: False\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Remove this particular child string from the quals dictionary and database\n",
    "basic_quals_dict = s.load_object('basic_quals_dict')\n",
    "child_str = child_strs_list[idx]\n",
    "basic_quals_dict.pop(child_str, None)\n",
    "# basic_quals_dict[child_str] = 0\n",
    "s.store_objects(basic_quals_dict=basic_quals_dict)\n",
    "print(f'\"{child_str}\" in basic_quals_dict: {child_str in basic_quals_dict}')\n",
    "def do_cypher_tx(tx, qualification_str, verbose=False):\n",
    "    cypher_str = '''\n",
    "        MATCH (qs:QualificationStrings {qualification_str: $qualification_str})\n",
    "        DETACH DELETE qs;\n",
    "        '''\n",
    "    results_list = tx.run(query=cypher_str, parameters={'qualification_str': qualification_str})\n",
    "\n",
    "    return [dict(record.items()) for record in results_list]\n",
    "with cu.driver.session() as session:\n",
    "    row_objs_list = session.write_transaction(do_cypher_tx, qualification_str=child_str, verbose=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a188b08e-874a-41db-a163-2232b9c7cc55",
   "metadata": {},
   "source": [
    "\n",
    "----"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 296,
   "id": "dbe5ce2c-5c8c-4196-96de-ee38953e10e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pickling to C:\\Users\\daveb\\OneDrive\\Documents\\GitHub\\job-hunting\\saves\\pkl\\hunting_df.pkl\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'fn': <Node element_id='984158' labels=frozenset({'FileNames'}) properties={'file_name': '8eda6f13ecba3a8c_Senior_Associate_Manager_Digital_Tax_Services_Washington_DC_20005_Indeed_com.html', 'posting_url': 'https://www.indeed.com/rc/clk/dl?jk=8eda6f13ecba3a8c&from=ja&qd=RnZhMybXSk4M3QtTVGXWocPDA-jVn_f73KUcK2QrGXxWzxuTTZnceBTcgT1wk7VUhH6vRsR2kLpXgXBggkmABvNDuymhiEN80F4AmgvDj8k&rd=ZwhSdEcVBMuVg3-ULQMg5h1nad7mHeJIvvNS1DT4gjQ&tk=1grb91v2e2sr5001&alid=63b02dca1ef86228dd5d5128', 'is_verified': False}>}]"
      ]
     },
     "execution_count": 296,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "# Mark the file name as needing retraining everywhere\n",
    "# file_name = 'b4e994e1d282ffa9_Digital_Platform_Services_Data_Analytics_and_Insights_Senior_Manager_Salt_Lake_City_UT_84111_Indeed_com.html'\n",
    "\n",
    "# Check if the lru has retrained its isqualified classifier\n",
    "if not hasattr(lru, 'hunting_df'):\n",
    "    lru.retrain_isqualified_classifier(verbose=True)\n",
    "\n",
    "mask_series = lru.hunting_df.percent_fit.isin([file_name])\n",
    "lru.hunting_df.loc[mask_series, 'percent_fit'] = np.nan\n",
    "s.store_objects(hunting_df=lru.hunting_df)\n",
    "def do_cypher_tx(tx, file_name, verbose=False):\n",
    "    cypher_str = \"\"\"\n",
    "        MATCH (fn:FileNames {file_name: $file_name})\n",
    "        SET fn.percent_fit = NULL, fn.is_verified = false\n",
    "        RETURN fn;\"\"\"\n",
    "    if verbose:\n",
    "        clear_output(wait=True)\n",
    "        print(cypher_str.replace('$file_name', f'\"{file_name}\"'))\n",
    "    results_list = tx.run(query=cypher_str, parameters={'file_name': file_name})\n",
    "\n",
    "    return [dict(record.items()) for record in results_list]\n",
    "with cu.driver.session() as session:\n",
    "    row_objs_list = session.write_transaction(do_cypher_tx, file_name=file_name, verbose=False)\n",
    "row_objs_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f00b97db-00ab-4ab9-8a22-323bbb5c6ddd",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# You've made no changes to the qualification dictionary (regardless of parts-of-speech changes)\n",
    "def do_cypher_tx(tx, file_name, verbose=False):\n",
    "    cypher_str = \"\"\"\n",
    "        MATCH (fn:FileNames {file_name: $file_name})\n",
    "        SET fn.is_verified = true\n",
    "        RETURN fn;\"\"\"\n",
    "    if verbose:\n",
    "        clear_output(wait=True)\n",
    "        print(cypher_str.replace('$file_name', f'\"{file_name}\"'))\n",
    "    parameter_dict = {'file_name': file_name}\n",
    "    results_list = tx.run(query=cypher_str, parameters=parameter_dict)\n",
    "    values_list = []\n",
    "    for record in results_list:\n",
    "        values_list.append(dict(record.items()))\n",
    "\n",
    "    return values_list\n",
    "with cu.driver.session() as session:\n",
    "    row_objs_list = session.write_transaction(do_cypher_tx, file_name=file_name, verbose=True)\n",
    "row_objs_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2f8adbd-5b7c-43d8-8680-aea4da63a134",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Mark the file name as closed\n",
    "cypher_str = f'''\n",
    "    MATCH (fn:FileNames {{file_name: \"{file_name}\"}})\n",
    "    SET fn.is_closed = true\n",
    "    RETURN fn;'''\n",
    "print(cypher_str)\n",
    "with cu.driver.session() as session:\n",
    "    row_objs_list = session.write_transaction(cu.do_cypher_tx, cypher_str)\n",
    "row_objs_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "befe9664-96e1-43cb-8de7-99e40d5524f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Manually label the unscored qual\n",
    "qualification_str = quals_list[13]\n",
    "print(qualification_str)\n",
    "basic_quals_dict = s.load_object('basic_quals_dict')\n",
    "basic_quals_dict[qualification_str] = 0\n",
    "s.store_objects(basic_quals_dict=basic_quals_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a4a2c62-661a-4652-b743-efa5e336ad65",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Remove file name from database\n",
    "# file_name = '3c031ea6ad293e92_General_Service_Technician_Westborough_MA_01581_Indeed_com.html'\n",
    "cu.delete_filename_node(file_name, verbose=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0646f71a-405a-44d9-aebd-18e70c97f08a",
   "metadata": {},
   "source": [
    "\n",
    "----\n",
    "# Take a badly written requirements section and see if you can programmatically parse the qualification string out of it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "id": "41271930-53f3-42df-bc7d-e021140f17bc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<p>REQUIREMENTS: Requires a Bachelor’s degree, or foreign equivalent degree in Computer Science, Computer Engineering, or Electronic Engineering and four (4) years of experience in the job offered, or four (4) years of experience in a related occupation driving strategy and approach through solution and enterprise testing; executing automation through Ginger, CI/CD, Agile, Python, Java languages, and testing tools such as Selenium; collaborating with cross functional teams to analyze, develop, and implement end-to-end solutions; using existing and modernized tooling such as, Jira Align, iTrack, Zephyr, AI/ML, and AQUA; performing the walkthrough and grooming of capabilities and features in cases with ARTs; creating test plans, scenarios/use cases and test cases associated with capabilities and features; ensuring that all test cases are in alignment with automation frameworks; writing E2E scenario test cases, maximizing test coverage for a feature, and minimizing the impact of disruptive test cases; designing and implementing automation tests and frameworks to enable continuous deployment and continuous testing for CTP across all phases; and ensuring that all automation scripts have gone through standard code quality checks, incorporating Gerrit Code Review and Cloud Review.</p>'"
      ]
     },
     "execution_count": 219,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "# Take a badly written requirements section and see if you can programmatically parse the qualification string out of it\n",
    "child_str = '<p>REQUIREMENTS: Requires a Bachelor’s degree, or foreign equivalent degree in Computer Science, Computer Engineering, or Electronic Engineering and four (4) years of experience in the job offered,'\n",
    "child_str += ' or four (4) years of experience in a related occupation driving strategy and approach through solution and enterprise testing; executing automation through Ginger, CI/CD, Agile, Python, Java languages,'\n",
    "child_str += ' and testing tools such as Selenium; collaborating with cross functional teams to analyze, develop, and implement end-to-end solutions; using existing and modernized tooling such as, Jira Align,'\n",
    "child_str += ' iTrack, Zephyr, AI/ML, and AQUA; performing the walkthrough and grooming of capabilities and features in cases with ARTs; creating test plans,'\n",
    "child_str += ' scenarios/use cases and test cases associated with capabilities and features; ensuring that all test cases are in alignment with automation frameworks; writing E2E scenario test cases,'\n",
    "child_str += ' maximizing test coverage for a feature, and minimizing the impact of disruptive test cases;'\n",
    "child_str += ' designing and implementing automation tests and frameworks to enable continuous deployment and continuous testing for CTP across all phases;'\n",
    "child_str += ' and ensuring that all automation scripts have gone through standard code quality checks, incorporating Gerrit Code Review and Cloud Review.</p>'\n",
    "child_str"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "232f78ce-e9d8-43b6-b138-38641dcedd42",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Take a badly written requirements section and see if you can programmatically parse the qualification string out of it\n",
    "import re\n",
    "from nltk.tokenize import sent_tokenize\n",
    "\n",
    "# sampling_strategy_limit=6_400 gets 10,635 labeled parts of speech and takes 49 minutes and 30 seconds\n",
    "# sampling_strategy_limit=7_000 gets 10,635 labeled parts of speech and takes 49 minutes and 30 seconds\n",
    "lru.build_pos_logistic_regression_elements(sampling_strategy_limit=70_000, verbose=True)\n",
    "\n",
    "qual_paragraph = re.sub('</?[^<>]+>', '', child_str.strip(), 0, re.MULTILINE)\n",
    "if len(sent_tokenize(qual_paragraph)) < 2:\n",
    "    child_strs_list = re.split(' *: *', qual_paragraph, 0)\n",
    "    child_tags_list = ha.get_child_tags_list(child_strs_list)\n",
    "    is_header_list = []\n",
    "    for is_header, child_str in zip(ha.get_is_header_list(child_strs_list), child_strs_list):\n",
    "        if is_header is None:\n",
    "            probs_list = lru.ISHEADER_PREDICT_PERCENT_FIT(child_str)\n",
    "            idx = probs_list.index(max(probs_list))\n",
    "            is_header = [True, False][idx]\n",
    "        is_header_list.append(is_header)\n",
    "    feature_dict_list = hc.get_feature_dict_list(child_tags_list, is_header_list, child_strs_list)\n",
    "    feature_tuple_list = []\n",
    "    for feature_dict in feature_dict_list:\n",
    "        feature_tuple_list.append(hc.get_feature_tuple(feature_dict, pos_lr_predict_single=lru.pos_lr_predict_single, pos_crf_predict_single=None))\n",
    "    crf_list = crf.CRF.predict_single(crf.sent2features(feature_tuple_list))\n",
    "    if crf_list[0] == 'H-RQ':\n",
    "        child_strs_list = re.split(' *; *', ': '.join(child_strs_list[1:]), 0)\n",
    "        child_tags_list = ha.get_child_tags_list(child_strs_list)\n",
    "        is_header_list = []\n",
    "        for is_header, child_str in zip(ha.get_is_header_list(child_strs_list), child_strs_list):\n",
    "            if is_header is None:\n",
    "                probs_list = lru.ISHEADER_PREDICT_PERCENT_FIT(child_str)\n",
    "                idx = probs_list.index(max(probs_list))\n",
    "                is_header = [True, False][idx]\n",
    "            is_header_list.append(is_header)\n",
    "        feature_dict_list = hc.get_feature_dict_list(child_tags_list, is_header_list, child_strs_list)\n",
    "        feature_tuple_list = []\n",
    "        for feature_dict in feature_dict_list:\n",
    "            feature_tuple_list.append(hc.get_feature_tuple(feature_dict, pos_lr_predict_single=lru.pos_lr_predict_single, pos_crf_predict_single=None))\n",
    "        crf_list = crf.CRF.predict_single(crf.sent2features(feature_tuple_list))\n",
    "        db_pos_list = []\n",
    "        for navigable_parent in child_strs_list:\n",
    "            db_pos_list = cu.append_parts_of_speech_list(navigable_parent, pos_list=db_pos_list)\n",
    "        pos_list, indices_list = su.visualize_basic_quals_section(crf_list, child_strs_list, db_pos_list=db_pos_list, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "id": "16fe3a9a-8b2b-4bfc-b348-ce01a188c19d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['BlanklineTokenizer', 'LegalitySyllableTokenizer', 'LineTokenizer', 'MWETokenizer', 'NLTKWordTokenizer', 'PunktSentenceTokenizer', 'RegexpTokenizer', 'ReppTokenizer', 'SExprTokenizer', 'SpaceTokenizer', 'StanfordSegmenter', 'SyllableTokenizer', 'TabTokenizer', 'TextTilingTokenizer', 'ToktokTokenizer', 'TreebankWordDetokenizer', 'TreebankWordTokenizer', 'TweetTokenizer', 'WhitespaceTokenizer', 'WordPunctTokenizer', '__builtins__', '__cached__', '__doc__', '__file__', '__loader__', '__name__', '__package__', '__path__', '__spec__', '_treebank_word_tokenizer', 'api', 'blankline_tokenize', 'casual', 'casual_tokenize', 'destructive', 'legality_principle', 'line_tokenize', 'load', 'mwe', 'punkt', 're', 'regexp', 'regexp_span_tokenize', 'regexp_tokenize', 'repp', 'sent_tokenize', 'sexpr', 'sexpr_tokenize', 'simple', 'sonority_sequencing', 'stanford_segmenter', 'string_span_tokenize', 'texttiling', 'toktok', 'treebank', 'util', 'word_tokenize', 'wordpunct_tokenize']"
      ]
     },
     "execution_count": 191,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "import nltk.tokenize\n",
    "\n",
    "dir(nltk.tokenize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "id": "60f18ee0-88df-42c4-b48d-04a9ed3508c1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['nltk.tokenize.BlanklineTokenizer', 'nltk.tokenize.LegalitySyllableTokenizer', 'nltk.tokenize.LineTokenizer', 'nltk.tokenize.MWETokenizer', 'nltk.tokenize.NLTKWordTokenizer', 'nltk.tokenize.PunktSentenceTokenizer', 'nltk.tokenize.RegexpTokenizer', 'nltk.tokenize.ReppTokenizer', 'nltk.tokenize.SExprTokenizer', 'nltk.tokenize.SpaceTokenizer', 'nltk.tokenize.SyllableTokenizer', 'nltk.tokenize.TabTokenizer', 'nltk.tokenize.TextTilingTokenizer', 'nltk.tokenize.ToktokTokenizer', 'nltk.tokenize.TreebankWordTokenizer', 'nltk.tokenize.TweetTokenizer', 'nltk.tokenize.WhitespaceTokenizer', 'nltk.tokenize.WordPunctTokenizer']"
      ]
     },
     "execution_count": 194,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "[f'nltk.tokenize.{fn}' for fn in dir(nltk.tokenize) if 'Tokenize' in fn]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "id": "854c3df8-2841-4579-a681-9b03518d5360",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[1;31mInit signature:\u001b[0m\n",
       "\u001b[0mnltk\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtokenize\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTweetTokenizer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m\n",
       "\u001b[0m    \u001b[0mpreserve_case\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
       "\u001b[0m    \u001b[0mreduce_len\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
       "\u001b[0m    \u001b[0mstrip_handles\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
       "\u001b[0m    \u001b[0mmatch_phone_numbers\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
       "\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
       "\u001b[1;31mDocstring:\u001b[0m     \n",
       "Tokenizer for tweets.\n",
       "\n",
       "    >>> from nltk.tokenize import TweetTokenizer\n",
       "    >>> tknzr = TweetTokenizer()\n",
       "    >>> s0 = \"This is a cooool #dummysmiley: :-) :-P <3 and some arrows < > -> <--\"\n",
       "    >>> tknzr.tokenize(s0) # doctest: +NORMALIZE_WHITESPACE\n",
       "    ['This', 'is', 'a', 'cooool', '#dummysmiley', ':', ':-)', ':-P', '<3', 'and', 'some', 'arrows', '<', '>', '->',\n",
       "     '<--']\n",
       "\n",
       "Examples using `strip_handles` and `reduce_len parameters`:\n",
       "\n",
       "    >>> tknzr = TweetTokenizer(strip_handles=True, reduce_len=True)\n",
       "    >>> s1 = '@remy: This is waaaaayyyy too much for you!!!!!!'\n",
       "    >>> tknzr.tokenize(s1)\n",
       "    [':', 'This', 'is', 'waaayyy', 'too', 'much', 'for', 'you', '!', '!', '!']\n",
       "\u001b[1;31mInit docstring:\u001b[0m\n",
       "Create a `TweetTokenizer` instance with settings for use in the `tokenize` method.\n",
       "\n",
       ":param preserve_case: Flag indicating whether to preserve the casing (capitalisation)\n",
       "    of text used in the `tokenize` method. Defaults to True.\n",
       ":type preserve_case: bool\n",
       ":param reduce_len: Flag indicating whether to replace repeated character sequences\n",
       "    of length 3 or greater with sequences of length 3. Defaults to False.\n",
       ":type reduce_len: bool\n",
       ":param strip_handles: Flag indicating whether to remove Twitter handles of text used\n",
       "    in the `tokenize` method. Defaults to False.\n",
       ":type strip_handles: bool\n",
       ":param match_phone_numbers: Flag indicating whether the `tokenize` method should look\n",
       "    for phone numbers. Defaults to True.\n",
       ":type match_phone_numbers: bool\n",
       "\u001b[1;31mFile:\u001b[0m           c:\\users\\daveb\\onedrive\\documents\\github\\job-hunting\\jh_env\\lib\\site-packages\\nltk\\tokenize\\casual.py\n",
       "\u001b[1;31mType:\u001b[0m           ABCMeta\n",
       "\u001b[1;31mSubclasses:\u001b[0m     "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "nltk.tokenize.TweetTokenizer?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "id": "e5c1b67b-0583-4e3e-a2ff-9ffbc8adf7dd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['nltk.tokenize.TreebankWordDetokenizer', 'nltk.tokenize._treebank_word_tokenizer', 'nltk.tokenize.blankline_tokenize', 'nltk.tokenize.casual_tokenize', 'nltk.tokenize.line_tokenize', 'nltk.tokenize.regexp_span_tokenize', 'nltk.tokenize.regexp_tokenize', 'nltk.tokenize.sent_tokenize', 'nltk.tokenize.sexpr_tokenize', 'nltk.tokenize.string_span_tokenize', 'nltk.tokenize.word_tokenize', 'nltk.tokenize.wordpunct_tokenize']"
      ]
     },
     "execution_count": 193,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "[f'nltk.tokenize.{fn}' for fn in dir(nltk.tokenize) if 'tokenize' in fn]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "id": "c02d533e-41c9-4ad3-94a5-312f6bc4966c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['<', 'p', '>', 'REQUIREMENTS', ':', 'Requires', 'a', 'Bachelor', '’', 's', 'degree', ',', 'or', 'foreign', 'equivalent', 'degree', 'in', 'Computer', 'Science', ',', 'Computer', 'Engineering', ',', 'or', 'Electronic', 'Engineering', 'and', 'four', '(', '4', ')', 'years', 'of', 'experience', 'in', 'the', 'job', 'offered', ',', 'or', 'four', '(', '4', ')', 'years', 'of', 'experience', 'in', 'a', 'related', 'occupation', 'driving', 'strategy', 'and', 'approach', 'through', 'solution', 'and', 'enterprise', 'testing', ';', 'executing', 'automation', 'through', 'Ginger', ',', 'CI', '/', 'CD', ',', 'Agile', ',', 'Python', ',', 'Java', 'languages', ',', 'and', 'testing', 'tools', 'such', 'as', 'Selenium', ';', 'collaborating', 'with', 'cross', 'functional', 'teams', 'to', 'analyze', ',', 'develop', ',', 'and', 'implement', 'end', '-', 'to', '-', 'end', 'solutions', ';', 'using', 'existing', 'and', 'modernized', 'tooling', 'such', 'as', ',', 'Jira', 'Align', ',', 'iTrack', ',', 'Zephyr', ',', 'AI', '/', 'ML', ',', 'and', 'AQUA', ';', 'performing', 'the', 'walkthrough', 'and', 'grooming', 'of', 'capabilities', 'and', 'features', 'in', 'cases', 'with', 'ARTs', ';', 'creating', 'test', 'plans', ',', 'scenarios', '/', 'use', 'cases', 'and', 'test', 'cases', 'associated', 'with', 'capabilities', 'and', 'features', ';', 'ensuring', 'that', 'all', 'test', 'cases', 'are', 'in', 'alignment', 'with', 'automation', 'frameworks', ';', 'writing', 'E2E', 'scenario', 'test', 'cases', ',', 'maximizing', 'test', 'coverage', 'for', 'a', 'feature', ',', 'and', 'minimizing', 'the', 'impact', 'of', 'disruptive', 'test', 'cases', ';', 'designing', 'and', 'implementing', 'automation', 'tests', 'and', 'frameworks', 'to', 'enable', 'continuous', 'deployment', 'and', 'continuous', 'testing', 'for', 'CTP', 'across', 'all', 'phases', ';', 'and', 'ensuring', 'that', 'all', 'automation', 'scripts', 'have', 'gone', 'through', 'standard', 'code', 'quality', 'checks', ',', 'incorporating', 'Gerrit', 'Code', 'Review', 'and', 'Cloud', 'Review', '.</', 'p', '>']"
      ]
     },
     "execution_count": 217,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "nltk.tokenize.wordpunct_tokenize(child_str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "id": "62d56c92-2077-4e48-b287-8a8314c12214",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['<', 'p', '>', 'REQUIREMENTS', ':', 'Requires', 'a', 'Bachelor', '’', 's', 'degree', ',', 'or', 'foreign', 'equivalent', 'degree', 'in', 'Computer', 'Science', ',', 'Computer', 'Engineering', ',', 'or', 'Electronic', 'Engineering', 'and', 'four', '(', '4', ')', 'years', 'of', 'experience', 'in', 'the', 'job', 'offered', ',', 'or', 'four', '(', '4', ')', 'years', 'of', 'experience', 'in', 'a', 'related', 'occupation', 'driving', 'strategy', 'and', 'approach', 'through', 'solution', 'and', 'enterprise', 'testing', ';', 'executing', 'automation', 'through', 'Ginger', ',', 'CI/CD', ',', 'Agile', ',', 'Python', ',', 'Java', 'languages', ',', 'and', 'testing', 'tools', 'such', 'as', 'Selenium', ';', 'collaborating', 'with', 'cross', 'functional', 'teams', 'to', 'analyze', ',', 'develop', ',', 'and', 'implement', 'end-to-end', 'solutions', ';', 'using', 'existing', 'and', 'modernized', 'tooling', 'such', 'as', ',', 'Jira', 'Align', ',', 'iTrack', ',', 'Zephyr', ',', 'AI/ML', ',', 'and', 'AQUA', ';', 'performing', 'the', 'walkthrough', 'and', 'grooming', 'of', 'capabilities', 'and', 'features', 'in', 'cases', 'with', 'ARTs', ';', 'creating', 'test', 'plans', ',', 'scenarios/use', 'cases', 'and', 'test', 'cases', 'associated', 'with', 'capabilities', 'and', 'features', ';', 'ensuring', 'that', 'all', 'test', 'cases', 'are', 'in', 'alignment', 'with', 'automation', 'frameworks', ';', 'writing', 'E2E', 'scenario', 'test', 'cases', ',', 'maximizing', 'test', 'coverage', 'for', 'a', 'feature', ',', 'and', 'minimizing', 'the', 'impact', 'of', 'disruptive', 'test', 'cases', ';', 'designing', 'and', 'implementing', 'automation', 'tests', 'and', 'frameworks', 'to', 'enable', 'continuous', 'deployment', 'and', 'continuous', 'testing', 'for', 'CTP', 'across', 'all', 'phases', ';', 'and', 'ensuring', 'that', 'all', 'automation', 'scripts', 'have', 'gone', 'through', 'standard', 'code', 'quality', 'checks', ',', 'incorporating', 'Gerrit', 'Code', 'Review', 'and', 'Cloud', 'Review.', '<', '/p', '>']"
      ]
     },
     "execution_count": 215,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "nltk.tokenize.word_tokenize(child_str, preserve_line=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "id": "09a43f97-3e61-4100-8026-d73f2437eb21",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0, 1298)]"
      ]
     },
     "execution_count": 213,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "list(nltk.tokenize.string_span_tokenize(child_str, r';\\s*'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "id": "771cf5ab-3264-4833-bc8e-3763ae9284cc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['<p>REQUIREMENTS: Requires a Bachelor’s degree, or foreign equivalent degree in Computer Science, Computer Engineering, or Electronic Engineering and four (4) years of experience in the job offered, or four (4) years of experience in a related occupation driving strategy and approach through solution and enterprise testing; executing automation through Ginger, CI/CD, Agile, Python, Java languages, and testing tools such as Selenium; collaborating with cross functional teams to analyze, develop, and implement end-to-end solutions; using existing and modernized tooling such as, Jira Align, iTrack, Zephyr, AI/ML, and AQUA; performing the walkthrough and grooming of capabilities and features in cases with ARTs; creating test plans, scenarios/use cases and test cases associated with capabilities and features; ensuring that all test cases are in alignment with automation frameworks; writing E2E scenario test cases, maximizing test coverage for a feature, and minimizing the impact of disruptive test cases; designing and implementing automation tests and frameworks to enable continuous deployment and continuous testing for CTP across all phases; and ensuring that all automation scripts have gone through standard code quality checks, incorporating Gerrit Code Review and Cloud Review.</p>']"
      ]
     },
     "execution_count": 210,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "nltk.tokenize.sent_tokenize(child_str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "id": "87620f95-6be5-46de-bc5b-8548d9316f5d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['p', 'REQUIREMENTS', 'Requires', 'a', 'Bachelor', 's', 'degree', 'or', 'foreign', 'equivalent', 'degree', 'in', 'Computer', 'Science', 'Computer', 'Engineering', 'or', 'Electronic', 'Engineering', 'and', 'four', '4', 'years', 'of', 'experience', 'in', 'the', 'job', 'offered', 'or', 'four', '4', 'years', 'of', 'experience', 'in', 'a', 'related', 'occupation', 'driving', 'strategy', 'and', 'approach', 'through', 'solution', 'and', 'enterprise', 'testing', 'executing', 'automation', 'through', 'Ginger', 'CI', 'CD', 'Agile', 'Python', 'Java', 'languages', 'and', 'testing', 'tools', 'such', 'as', 'Selenium', 'collaborating', 'with', 'cross', 'functional', 'teams', 'to', 'analyze', 'develop', 'and', 'implement', 'end', 'to', 'end', 'solutions', 'using', 'existing', 'and', 'modernized', 'tooling', 'such', 'as', 'Jira', 'Align', 'iTrack', 'Zephyr', 'AI', 'ML', 'and', 'AQUA', 'performing', 'the', 'walkthrough', 'and', 'grooming', 'of', 'capabilities', 'and', 'features', 'in', 'cases', 'with', 'ARTs', 'creating', 'test', 'plans', 'scenarios', 'use', 'cases', 'and', 'test', 'cases', 'associated', 'with', 'capabilities', 'and', 'features', 'ensuring', 'that', 'all', 'test', 'cases', 'are', 'in', 'alignment', 'with', 'automation', 'frameworks', 'writing', 'E2E', 'scenario', 'test', 'cases', 'maximizing', 'test', 'coverage', 'for', 'a', 'feature', 'and', 'minimizing', 'the', 'impact', 'of', 'disruptive', 'test', 'cases', 'designing', 'and', 'implementing', 'automation', 'tests', 'and', 'frameworks', 'to', 'enable', 'continuous', 'deployment', 'and', 'continuous', 'testing', 'for', 'CTP', 'across', 'all', 'phases', 'and', 'ensuring', 'that', 'all', 'automation', 'scripts', 'have', 'gone', 'through', 'standard', 'code', 'quality', 'checks', 'incorporating', 'Gerrit', 'Code', 'Review', 'and', 'Cloud', 'Review', 'p']"
      ]
     },
     "execution_count": 208,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "list(nltk.tokenize.regexp_tokenize(child_str, r'\\w+'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "id": "0ebf922d-1e93-48e0-82f4-f47e64cc42f6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0, 1298)]"
      ]
     },
     "execution_count": 204,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "list(nltk.tokenize.regexp_span_tokenize(child_str, r'\\s\\s+'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "id": "a5c338cc-6e72-49e8-a7b8-1cc093008d2b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['<p>REQUIREMENTS: Requires a Bachelor’s degree, or foreign equivalent degree in Computer Science, Computer Engineering, or Electronic Engineering and four (4) years of experience in the job offered, or four (4) years of experience in a related occupation driving strategy and approach through solution and enterprise testing; executing automation through Ginger, CI/CD, Agile, Python, Java languages, and testing tools such as Selenium; collaborating with cross functional teams to analyze, develop, and implement end-to-end solutions; using existing and modernized tooling such as, Jira Align, iTrack, Zephyr, AI/ML, and AQUA; performing the walkthrough and grooming of capabilities and features in cases with ARTs; creating test plans, scenarios/use cases and test cases associated with capabilities and features; ensuring that all test cases are in alignment with automation frameworks; writing E2E scenario test cases, maximizing test coverage for a feature, and minimizing the impact of disruptive test cases; designing and implementing automation tests and frameworks to enable continuous deployment and continuous testing for CTP across all phases; and ensuring that all automation scripts have gone through standard code quality checks, incorporating Gerrit Code Review and Cloud Review.</p>']"
      ]
     },
     "execution_count": 201,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "nltk.tokenize.line_tokenize(child_str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "id": "8cd7daf2-70b0-4cc3-868d-1f8e34384722",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['<p>', 'REQUIREMENTS', ':', 'Requires', 'a', 'Bachelor', '’', 's', 'degree', ',', 'or', 'foreign', 'equivalent', 'degree', 'in', 'Computer', 'Science', ',', 'Computer', 'Engineering', ',', 'or', 'Electronic', 'Engineering', 'and', 'four', '(', '4', ')', 'years', 'of', 'experience', 'in', 'the', 'job', 'offered', ',', 'or', 'four', '(', '4', ')', 'years', 'of', 'experience', 'in', 'a', 'related', 'occupation', 'driving', 'strategy', 'and', 'approach', 'through', 'solution', 'and', 'enterprise', 'testing', ';', 'executing', 'automation', 'through', 'Ginger', ',', 'CI', '/', 'CD', ',', 'Agile', ',', 'Python', ',', 'Java', 'languages', ',', 'and', 'testing', 'tools', 'such', 'as', 'Selenium', ';', 'collaborating', 'with', 'cross', 'functional', 'teams', 'to', 'analyze', ',', 'develop', ',', 'and', 'implement', 'end-to-end', 'solutions', ';', 'using', 'existing', 'and', 'modernized', 'tooling', 'such', 'as', ',', 'Jira', 'Align', ',', 'iTrack', ',', 'Zephyr', ',', 'AI', '/', 'ML', ',', 'and', 'AQUA', ';', 'performing', 'the', 'walkthrough', 'and', 'grooming', 'of', 'capabilities', 'and', 'features', 'in', 'cases', 'with', 'ARTs', ';', 'creating', 'test', 'plans', ',', 'scenarios', '/', 'use', 'cases', 'and', 'test', 'cases', 'associated', 'with', 'capabilities', 'and', 'features', ';', 'ensuring', 'that', 'all', 'test', 'cases', 'are', 'in', 'alignment', 'with', 'automation', 'frameworks', ';', 'writing', 'E2E', 'scenario', 'test', 'cases', ',', 'maximizing', 'test', 'coverage', 'for', 'a', 'feature', ',', 'and', 'minimizing', 'the', 'impact', 'of', 'disruptive', 'test', 'cases', ';', 'designing', 'and', 'implementing', 'automation', 'tests', 'and', 'frameworks', 'to', 'enable', 'continuous', 'deployment', 'and', 'continuous', 'testing', 'for', 'CTP', 'across', 'all', 'phases', ';', 'and', 'ensuring', 'that', 'all', 'automation', 'scripts', 'have', 'gone', 'through', 'standard', 'code', 'quality', 'checks', ',', 'incorporating', 'Gerrit', 'Code', 'Review', 'and', 'Cloud', 'Review', '.', '</p>']"
      ]
     },
     "execution_count": 199,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "nltk.tokenize.casual_tokenize(child_str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "id": "a3287dff-0406-4c4d-89ee-82b6ef750457",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['<p>REQUIREMENTS: Requires a Bachelor’s degree, or foreign equivalent degree in Computer Science, Computer Engineering, or Electronic Engineering and four (4) years of experience in the job offered, or four (4) years of experience in a related occupation driving strategy and approach through solution and enterprise testing; executing automation through Ginger, CI/CD, Agile, Python, Java languages, and testing tools such as Selenium; collaborating with cross functional teams to analyze, develop, and implement end-to-end solutions; using existing and modernized tooling such as, Jira Align, iTrack, Zephyr, AI/ML, and AQUA; performing the walkthrough and grooming of capabilities and features in cases with ARTs; creating test plans, scenarios/use cases and test cases associated with capabilities and features; ensuring that all test cases are in alignment with automation frameworks; writing E2E scenario test cases, maximizing test coverage for a feature, and minimizing the impact of disruptive test cases; designing and implementing automation tests and frameworks to enable continuous deployment and continuous testing for CTP across all phases; and ensuring that all automation scripts have gone through standard code quality checks, incorporating Gerrit Code Review and Cloud Review.</p>']"
      ]
     },
     "execution_count": 197,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "nltk.tokenize.blankline_tokenize(child_str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55f74215-ceda-4159-a101-90417d72f694",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Job Hunting (Python 3.10.9)",
   "language": "python",
   "name": "jh_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
