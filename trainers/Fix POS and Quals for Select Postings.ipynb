{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7675b025-b593-4d53-9759-6c1e01772c22",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pretty printing has been turned OFF\n"
     ]
    }
   ],
   "source": [
    "\n",
    "%pprint"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ed3ad81-7ac1-4a85-ba90-62ccbde24f49",
   "metadata": {
    "tags": []
   },
   "source": [
    "\n",
    "---\n",
    "# Load needed libraries and functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e1febade-58e7-47f1-8c6d-5447b5737d57",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from IPython.display import clear_output\n",
    "from datetime import datetime\n",
    "from neo4j.exceptions import ServiceUnavailable\n",
    "from pandas import DataFrame\n",
    "import humanize\n",
    "import numpy as np\n",
    "import os\n",
    "import sys\n",
    "import time\n",
    "import warnings\n",
    "import winsound\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "duration = 1000  # milliseconds\n",
    "freq = 880  # Hz\n",
    "\n",
    "# Insert at 1, 0 is the script path (or '' in REPL)\n",
    "sys.path.insert(1, '../py')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ff7aa81f-314d-4d0d-a238-355d5210f157",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======== Neo4j/4.4.7 ========\n",
      "Utility libraries created in 7 seconds\n",
      "Last run on 2023-11-16 13:53:21.285788\n"
     ]
    }
   ],
   "source": [
    "\n",
    "t0 = time.time()\n",
    "\n",
    "# Get the Neo4j driver\n",
    "from storage import Storage\n",
    "s = Storage(\n",
    "    data_folder_path=os.path.abspath('../data'),\n",
    "    saves_folder_path=os.path.abspath('../saves')\n",
    ")\n",
    "\n",
    "from ha_utils import HeaderAnalysis\n",
    "ha = HeaderAnalysis(s=s, verbose=False)\n",
    "\n",
    "from scrape_utils import WebScrapingUtilities\n",
    "wsu = WebScrapingUtilities(\n",
    "    s=s,\n",
    "    secrets_json_path=os.path.abspath('../data/secrets/jh_secrets.json')\n",
    ")\n",
    "uri = wsu.secrets_json['neo4j']['connect_url']\n",
    "user =  wsu.secrets_json['neo4j']['username']\n",
    "password = wsu.secrets_json['neo4j']['password']\n",
    "\n",
    "# Get the neo4j object\n",
    "from cypher_utils import CypherUtilities\n",
    "cu = CypherUtilities(\n",
    "    uri=uri, user=user, password=password, driver=None, s=s, ha=ha\n",
    ")\n",
    "\n",
    "from is_header_sgd_classifier import IsHeaderSgdClassifier\n",
    "ihu = IsHeaderSgdClassifier(ha=ha, cu=cu, verbose=False)\n",
    "\n",
    "try:\n",
    "        \n",
    "    version_str = cu.driver.get_server_info().agent\n",
    "    print(f'======== {version_str} ========')\n",
    "except ServiceUnavailable as e:\n",
    "    print('You need to start Neo4j as a console')\n",
    "    raise\n",
    "except Exception as e:\n",
    "    print(f'{e.__class__}: {str(e).strip()}')\n",
    "\n",
    "from hc_utils import HeaderCategories\n",
    "hc = HeaderCategories(cu=cu, verbose=False)\n",
    "\n",
    "# Keep the total creation time to less than one hour by adjusting the sampling strategy limit\n",
    "from lr_utils import LrUtilities\n",
    "lru = LrUtilities(ha=ha, cu=cu, hc=hc, verbose=False)\n",
    "\n",
    "from section_classifier_utils import SectionLRClassifierUtilities, SectionSGDClassifierUtilities, SectionCRFClassifierUtilities\n",
    "slrcu = SectionLRClassifierUtilities(ha=ha, cu=cu, verbose=False)\n",
    "ssgdcu = SectionSGDClassifierUtilities(ha=ha, cu=cu, verbose=False)\n",
    "scrfcu = SectionCRFClassifierUtilities(cu=cu, ha=ha, verbose=False)\n",
    "\n",
    "from crf_utils import CrfUtilities\n",
    "crf = CrfUtilities(ha=ha, hc=hc, cu=cu, lru=lru, slrcu=slrcu, scrfcu=scrfcu, ssgdcu=ssgdcu, verbose=True)\n",
    "\n",
    "from section_utils import SectionUtilities\n",
    "su = SectionUtilities(wsu=wsu, ihu=ihu, hc=hc, crf=crf, slrcu=slrcu, scrfcu=scrfcu, ssgdcu=ssgdcu, verbose=False)\n",
    "\n",
    "duration_str = humanize.precisedelta(time.time() - t0, minimum_unit='seconds', format='%0.0f')\n",
    "winsound.Beep(freq, duration)\n",
    "print(f'Utility libraries created in {duration_str}')\n",
    "print(f'Last run on {datetime.now()}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b062dfbb-7833-4809-9fea-565ae14a865a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "predict_single is now available\n",
      "Parts-of-speech conditional random field elements built in 5 seconds\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Parts-of-speech CRF elements built in 28 minutes and 51 seconds\n",
    "t1 = time.time()\n",
    "if not (hasattr(scrfcu, 'pos_symbol_crf') or crf.is_flask_running()):\n",
    "    scrfcu.build_pos_conditional_random_field_elements(verbose=True)\n",
    "if hasattr(scrfcu, 'pos_symbol_crf'):\n",
    "    print('predict_single is now available')\n",
    "else:\n",
    "    print('predict_single is not available')\n",
    "duration_str = humanize.precisedelta(time.time() - t1, minimum_unit='seconds', format='%0.0f')\n",
    "print(f'Parts-of-speech conditional random field elements built in {duration_str}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3b60aeb5-2919-449b-aec2-f115c2d94c83",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I have 49,102 labeled parts of speech in here\n",
      "predict_single is now available\n",
      "Parts-of-speech stochastic gradient decent elements built in 15 seconds\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Parts-of-speech stochastic gradient decent elements built in 17 seconds\n",
    "t1 = time.time()\n",
    "if not (hasattr(ssgdcu, 'pos_predict_percent_fit_dict') or crf.is_flask_running()):\n",
    "    ssgdcu.build_pos_stochastic_gradient_descent_elements(sampling_strategy_limit=None, verbose=True)\n",
    "if hasattr(ssgdcu, 'pos_predict_percent_fit_dict'):\n",
    "    print('predict_single is now available')\n",
    "else:\n",
    "    print('predict_single is not available')\n",
    "duration_str = humanize.precisedelta(time.time() - t1, minimum_unit='seconds', format='%0.0f')\n",
    "print(f'Parts-of-speech stochastic gradient decent elements built in {duration_str}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "24b92034-a498-4e1f-b4f9-a37038c4f432",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "POS classifier trained in 0 seconds\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Check if the crf has built its parts-of-speech classifier\n",
    "# POS classifier trained in 12 hours, 15 minutes and 36 seconds\n",
    "t1 = time.time()\n",
    "if not hasattr(crf, 'CRF'):\n",
    "    if s.pickle_exists('crf_CRF'):\n",
    "        crf.CRF = s.load_object('crf_CRF')\n",
    "    else:\n",
    "        crf.retrain_pos_classifier(header_pattern_dict=s.load_object('HEADER_PATTERN_DICT'), verbose=True)\n",
    "duration_str = humanize.precisedelta(time.time() - t1, minimum_unit='seconds', format='%0.0f')\n",
    "print(f'POS classifier trained in {duration_str}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "86510da3-4df5-4f7e-93c2-e0534789cec9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I have 49,102 labeled parts of speech in here\n",
      "predict_single is now available\n",
      "Parts-of-speech logistic regression elements built in 10 seconds\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Parts-of-speech logistic regression elements built in 1 hour, 59 minutes and 41 seconds\n",
    "t1 = time.time()\n",
    "if not (hasattr(slrcu, 'pos_predict_percent_fit_dict') or crf.is_flask_running()):\n",
    "    slrcu.build_pos_logistic_regression_elements(sampling_strategy_limit=None, verbose=True)\n",
    "if hasattr(slrcu, 'pos_predict_percent_fit_dict'):\n",
    "    print('predict_single is now available')\n",
    "else:\n",
    "    print('predict_single is not available')\n",
    "duration_str = humanize.precisedelta(time.time() - t1, minimum_unit='seconds', format='%0.0f')\n",
    "print(f'Parts-of-speech logistic regression elements built in {duration_str}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7075bbf4-e2ca-44e0-9516-81da82548dcb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Is-qualified LR classifier built in 6 seconds\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Is-qualified LR classifier built in 5 seconds\n",
    "t1 = time.time()\n",
    "lru.build_isqualified_logistic_regression_elements(sampling_strategy_limit=5_000, verbose=False)\n",
    "duration_str = humanize.precisedelta(time.time() - t1, minimum_unit='seconds', format='%0.0f')\n",
    "print(f'Is-qualified LR classifier built in {duration_str}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "60978370-e643-4d47-96ad-f8893714bfae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I have 49,070 hand-labeled header htmls prepared\n",
      "7 iterations seen during training fit for a total of 49,070 records trained\n",
      "Is-header SGD classifer built in 8 seconds\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Is-header SGD classifer built in 9 seconds\n",
    "t1 = time.time()\n",
    "ihu.build_pos_stochastic_gradient_descent_elements(verbose=True)\n",
    "duration_str = humanize.precisedelta(time.time() - t1, minimum_unit='seconds', format='%0.0f')\n",
    "print(f'Is-header SGD classifer built in {duration_str}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "268c479f-c162-454d-b4a0-cccb123c567f",
   "metadata": {},
   "source": [
    "\n",
    "---\n",
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e57ef681-27d5-4610-8496-ff1d30b09c89",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I have 13,272 hand-labeled qualification strings in here\n",
      "I have 440,487 is-qualified vocabulary tokens in here\n",
      "Is-qualified classifer retrained in 10 seconds\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# You need to run this again if you changed the qualification dictionary in another notebook\n",
    "t1 = time.time()\n",
    "\n",
    "# Keep the total retraining time to less than two minutes by adjusting the sampling strategy limit\n",
    "lru.sync_basic_quals_dict(sampling_strategy_limit=8_000, verbose=False)\n",
    "lru.retrain_isqualified_classifier(verbose=True)\n",
    "\n",
    "duration_str = humanize.precisedelta(time.time() - t1, minimum_unit='seconds', format='%0.0f')\n",
    "print(f'Is-qualified classifer retrained in {duration_str}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "82dab229-b28c-4ddd-812d-6abf153aae3b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Only 10 more mis-estimated minimum-requirements-met percentages to go!\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from pandas import DataFrame\n",
    "\n",
    "cypher_str = f'''\n",
    "    MATCH (fn:FileNames)\n",
    "    WHERE\n",
    "        fn.percent_fit = 0.0 AND\n",
    "        ((fn.is_closed IS NULL) OR (fn.is_closed = false)) AND\n",
    "        ((fn.is_verified IS NULL) OR (fn.is_verified = false)) AND\n",
    "        ((fn.is_opportunity_application_emailed IS NULL) OR\n",
    "        (fn.is_opportunity_application_emailed = false))\n",
    "    RETURN\n",
    "        fn.percent_fit AS percent_fit,\n",
    "        fn.file_name AS file_name,\n",
    "        fn.posting_url AS url\n",
    "    ORDER BY fn.percent_fit ASC;'''\n",
    "row_objs_list = []\n",
    "with cu.driver.session() as session:\n",
    "    row_objs_list = session.write_transaction(cu.do_cypher_tx, cypher_str)\n",
    "files_list = []\n",
    "if row_objs_list:\n",
    "    files_list = DataFrame(row_objs_list).file_name.tolist()\n",
    "print(f'Only {len(files_list)} more mis-estimated minimum-requirements-met percentages to go!')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbed91ef-2d2e-46c9-be8b-f6458185fad8",
   "metadata": {},
   "source": [
    "\n",
    "----\n",
    "## Fix POS and Quals for a job posting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "151cdf71-a078-43b6-9a1b-649bb51461e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "eb92a1192904e96e_Data_and_Analytics_Summer_Help_Schofield_WI_54476_Indeed_com.html\n",
      "CRF and child strings list recreated in 1 minute and 12 seconds\n"
     ]
    }
   ],
   "source": [
    "\n",
    "t1 = time.time()\n",
    "# file_name = 'f8b158d98ceff1ef_Machine_Learning_Algorithm_Developer_Assistant_Staff_Lexington_MA_Indeed_com.html'\n",
    "file_name = files_list.pop()\n",
    "file_path = os.path.join(cu.SAVES_HTML_FOLDER, file_name)\n",
    "if os.path.isfile(file_path):\n",
    "    child_strs_list = ha.get_child_strs_from_file(file_name=file_name)\n",
    "    cu.ensure_filename(file_name, verbose=False)\n",
    "    cu.populate_from_child_strings(child_strs_list, file_name, verbose=False)\n",
    "    child_tags_list = ha.get_child_tags_list(child_strs_list)\n",
    "    feature_dict_list = cu.get_feature_dict_list(child_tags_list, child_strs_list)\n",
    "    feature_tuple_list = []\n",
    "    for feature_dict in feature_dict_list:\n",
    "        feature_tuple_list.append(hc.get_feature_tuple(\n",
    "            feature_dict, pos_lr_predict_single=slrcu.predict_single, pos_crf_predict_single=scrfcu.predict_single,\n",
    "            pos_sgd_predict_single=ssgdcu.predict_single\n",
    "        ))\n",
    "    crf_list = crf.CRF.predict_single(crf.sent2features(feature_tuple_list))\n",
    "    print(file_name)\n",
    "duration_str = humanize.precisedelta(time.time() - t1, minimum_unit='seconds', format='%0.0f')\n",
    "print(f'CRF and child strings list recreated in {duration_str}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e8afe8b-e9cc-4dca-866e-88cf37461f2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "db_pos_list = []\n",
    "for navigable_parent in child_strs_list:\n",
    "    db_pos_list = cu.append_parts_of_speech_list(navigable_parent, pos_list=db_pos_list)\n",
    "pos_list, indices_list = su.visualize_basic_quals_section(crf_list, child_strs_list, db_pos_list=db_pos_list, verbose=True)\n",
    "winsound.Beep(freq, duration)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7070cc54-d512-456d-92fc-bd7bdb3f3e43",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Restarting the kernel and getting to this cell took 2 minutes and 3 seconds\n",
    "duration_str = humanize.precisedelta(time.time() - t0, minimum_unit='seconds', format='%0.0f')\n",
    "print(f'Restarting the kernel and getting to this cell took {duration_str}'); raise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "1d97acb2-2b60-4a56-9b3b-ecfc2b6582e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[4, 6, 8, 10]\n",
      "1\n",
      "10 O-RQ) <p>Selected candidate will be subject to a pre-employment background investigation and must be able to obtain and maintain a Secret level DoD security clearance.</p>\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Display the context of an individual child string\n",
    "idx = 10\n",
    "print(indices_list); child_str = child_strs_list[idx]; pos_symbol = pos_list[idx]; basic_quals_dict = s.load_object('basic_quals_dict')\n",
    "print(str(basic_quals_dict[child_str]) + '\\n' if(child_str in basic_quals_dict) else '', end=''); print(f'{idx} {pos_symbol}) {child_str}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "3f1b210b-8651-4291-8c80-046a9c896392",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pickling to C:\\Users\\daveb\\OneDrive\\Documents\\GitHub\\job-hunting\\saves\\pkl\\basic_quals_dict.pkl\n",
      "\"<p>The candidate must possess a Bachelor of Science degree in computer science, mathematics, electrical engineering, or an equivalent field. Prior exposure to machine learning algorithms, through coursework or research, is required. This includes deep learning techniques in addition to more general machine learning methods (e.g., Bayesian inference). Proficiency with Python is required, while familiarity with other development platforms like Matlab or C/C++ is considered an advantage. The candidate should also have some experience with common computer vision, natural language processing, or machine learning toolboxes (PyTorch, Pandas, OpenCV, TensorRT, libtorch, etc).</p>\" in basic_quals_dict: 1\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Hand-label this particular child string in the quals dictionary\n",
    "basic_quals_dict = s.load_object('basic_quals_dict')\n",
    "basic_quals_dict[child_str] = 1\n",
    "s.store_objects(basic_quals_dict=basic_quals_dict); print(f'\"{child_str}\" in basic_quals_dict: {basic_quals_dict[child_str]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "5bb3f749-0a02-4266-876d-08983725feb2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 iterations seen during updating fit for a total of 49,070 records trained\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'navigable_parent': '<p>Selected candidate will be subject to a pre-employment background investigation and must be able to obtain and maintain a Secret level DoD security clearance.</p>', 'is_header': False, 'is_task_scope': False, 'is_minimum_qualification': True, 'is_preferred_qualification': False, 'is_legal_notification': False, 'is_job_title': False, 'is_office_location': False, 'is_job_duration': False, 'is_supplemental_pay': False, 'is_educational_requirement': False, 'is_interview_procedure': False, 'is_corporate_scope': False, 'is_posting_date': False, 'is_other': False}]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "def do_cypher_tx(tx, navigable_parent, verbose=False):\n",
    "    cypher_str = \"\"\"MATCH (np:NavigableParents {navigable_parent: $navigable_parent})\n",
    "        SET\n",
    "            np.is_header = false,\n",
    "            np.is_task_scope = false,\n",
    "            np.is_minimum_qualification = true,\n",
    "            np.is_preferred_qualification = false,\n",
    "            np.is_educational_requirement = false,\n",
    "            np.is_legal_notification = false,\n",
    "            np.is_other = false,\n",
    "            np.is_corporate_scope = false,\n",
    "            np.is_job_title = false,\n",
    "            np.is_office_location = false,\n",
    "            np.is_job_duration = false,\n",
    "            np.is_supplemental_pay = false,\n",
    "            np.is_interview_procedure = false,\n",
    "            np.is_posting_date = false\n",
    "        \"\"\" + cu.return_everything_str + ';'\n",
    "    return [dict(record.items()) for record in tx.run(query=cypher_str, parameters={'navigable_parent': navigable_parent})]\n",
    "with cu.driver.session() as session:\n",
    "    row_objs_list = session.write_transaction(do_cypher_tx, navigable_parent=child_str)\n",
    "ihu.retrain_classifier(row_objs_list[0]['navigable_parent'], row_objs_list[0]['is_header'], verbose=True)\n",
    "row_objs_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "28836b66-b8d4-48cb-92a0-f28efbb39c8e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'navigable_parent': '<b>Who makes a DatOps Engineer?</b>', 'is_header': None, 'is_task_scope': None, 'is_minimum_qualification': None, 'is_preferred_qualification': None, 'is_legal_notification': None, 'is_job_title': None, 'is_office_location': None, 'is_job_duration': None, 'is_supplemental_pay': None, 'is_educational_requirement': None, 'is_interview_procedure': None, 'is_corporate_scope': None, 'is_posting_date': None, 'is_other': None}]"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "# Show what's in the database already for this html string\n",
    "def do_cypher_tx(tx, navigable_parent, verbose=False):\n",
    "    cypher_str = '''MATCH (np:NavigableParents {navigable_parent: $navigable_parent})\n",
    "        ''' + cu.return_everything_str + ';'\n",
    "    results_list = tx.run(query=cypher_str, parameters={'navigable_parent': navigable_parent})\n",
    "\n",
    "    return [dict(record.items()) for record in results_list]\n",
    "with cu.driver.session() as session:\n",
    "    row_objs_list = session.write_transaction(do_cypher_tx, navigable_parent=child_str)\n",
    "row_objs_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 335,
   "id": "f549b31a-da3c-4114-b015-e30ab12f172d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>qualification_str</th>\n",
       "      <th>is_qualified</th>\n",
       "      <th>length</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1353</th>\n",
       "      <td>&lt;li&gt;Hadoop and Machine Learning&lt;/li&gt;</td>\n",
       "      <td>0</td>\n",
       "      <td>36</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1573</th>\n",
       "      <td>&lt;p&gt;· Experience with API testing&lt;/p&gt;</td>\n",
       "      <td>0</td>\n",
       "      <td>36</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4903</th>\n",
       "      <td>&lt;orq&gt;3 - MySQL (P3 - Advanced)&lt;/orq&gt;</td>\n",
       "      <td>0</td>\n",
       "      <td>36</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2823</th>\n",
       "      <td>&lt;li&gt;An understanding of rating.&lt;/li&gt;</td>\n",
       "      <td>0</td>\n",
       "      <td>36</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6492</th>\n",
       "      <td>&lt;orq&gt;1 - MySQL (P1 - Beginner)&lt;/orq&gt;</td>\n",
       "      <td>1</td>\n",
       "      <td>36</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                         qualification_str  is_qualified  length\n",
       "1353  <li>Hadoop and Machine Learning</li>             0      36\n",
       "1573  <p>· Experience with API testing</p>             0      36\n",
       "4903  <orq>3 - MySQL (P3 - Advanced)</orq>             0      36\n",
       "2823  <li>An understanding of rating.</li>             0      36\n",
       "6492  <orq>1 - MySQL (P1 - Beginner)</orq>             1      36"
      ]
     },
     "execution_count": 335,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "lru.sync_basic_quals_dict(sampling_strategy_limit=8_000, verbose=False)\n",
    "df = lru.basic_quals_df.copy()\n",
    "df['length'] = df['qualification_str'].apply(len)\n",
    "df = df.sort_values('length')\n",
    "df.to_csv('../saves/csv/basic_quals_dict.csv')\n",
    "df.head(1000).tail(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 330,
   "id": "c767d2cb-2c13-421a-b29e-546d2b1b401f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pickling to C:\\Users\\daveb\\OneDrive\\Documents\\GitHub\\job-hunting\\saves\\pkl\\basic_quals_dict.pkl\n",
      "\"<orq>Ability to nice to have).</orq>\" in basic_quals_dict: False\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Remove the child string by idx from the quals dictionary and database\n",
    "basic_quals_dict = s.load_object('basic_quals_dict')\n",
    "child_str = df.loc[13195].qualification_str\n",
    "basic_quals_dict.pop(child_str, None)\n",
    "s.store_objects(basic_quals_dict=basic_quals_dict)\n",
    "print(f'\"{child_str}\" in basic_quals_dict: {child_str in basic_quals_dict}')\n",
    "def do_cypher_tx(tx, qualification_str, verbose=False):\n",
    "    cypher_str = '''\n",
    "        MATCH (qs:QualificationStrings {qualification_str: $qualification_str})\n",
    "        DETACH DELETE qs;\n",
    "        '''\n",
    "    results_list = tx.run(query=cypher_str, parameters={'qualification_str': qualification_str})\n",
    "\n",
    "    return [dict(record.items()) for record in results_list]\n",
    "with cu.driver.session() as session:\n",
    "    row_objs_list = session.write_transaction(do_cypher_tx, qualification_str=child_str, verbose=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4157faba-d33d-45b7-afd6-1017ceec1902",
   "metadata": {},
   "outputs": [],
   "source": [
    "raise\n",
    "# Remove this particular child string from the quals dictionary and database\n",
    "basic_quals_dict = s.load_object('basic_quals_dict')\n",
    "child_str = child_strs_list[idx]\n",
    "basic_quals_dict.pop(child_str, None)\n",
    "# basic_quals_dict[child_str] = 0\n",
    "s.store_objects(basic_quals_dict=basic_quals_dict)\n",
    "print(f'\"{child_str}\" in basic_quals_dict: {child_str in basic_quals_dict}')\n",
    "def do_cypher_tx(tx, qualification_str, verbose=False):\n",
    "    cypher_str = '''\n",
    "        MATCH (qs:QualificationStrings {qualification_str: $qualification_str})\n",
    "        DETACH DELETE qs;\n",
    "        '''\n",
    "    results_list = tx.run(query=cypher_str, parameters={'qualification_str': qualification_str})\n",
    "\n",
    "    return [dict(record.items()) for record in results_list]\n",
    "with cu.driver.session() as session:\n",
    "    row_objs_list = session.write_transaction(do_cypher_tx, qualification_str=child_str, verbose=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a188b08e-874a-41db-a163-2232b9c7cc55",
   "metadata": {},
   "source": [
    "\n",
    "----\n",
    "## Maintenance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "dbe5ce2c-5c8c-4196-96de-ee38953e10e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pickling to C:\\Users\\daveb\\OneDrive\\Documents\\GitHub\\job-hunting\\saves\\pkl\\hunting_df.pkl\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'fn': <Node element_id='988302' labels=frozenset({'FileNames'}) properties={'file_name': 'f8b158d98ceff1ef_Machine_Learning_Algorithm_Developer_Assistant_Staff_Lexington_MA_Indeed_com.html', 'posting_url': 'https://www.indeed.com/rc/clk/dl?jk=f8b158d98ceff1ef&from=ja&qd=RnZhMybXSk4M3QtTVGXWocPDA-jVn_f73KUcK2QrGXzs5Dt3MAdRluWXCJcV97lyuB6k-jOww-IM6KWrTzOElDsCsVSx1_HPhTTU3XPOEs8&rd=FzlTg_mIv5FOCOtUGrVy1MoNBk--8gR6CvhRrl83wHU&tk=1gs4rka4ip2em805&alid=63b02da7edaed13019025096', 'is_verified': True}>}]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "# Mark the file name as needing retraining everywhere\n",
    "# file_name = 'b4e994e1d282ffa9_Digital_Platform_Services_Data_Analytics_and_Insights_Senior_Manager_Salt_Lake_City_UT_84111_Indeed_com.html'\n",
    "\n",
    "# Check if the lru has retrained its isqualified classifier\n",
    "if not hasattr(lru, 'hunting_df'):\n",
    "    lru.retrain_isqualified_classifier(verbose=True)\n",
    "\n",
    "mask_series = lru.hunting_df.percent_fit.isin([file_name])\n",
    "lru.hunting_df.loc[mask_series, 'percent_fit'] = np.nan\n",
    "s.store_objects(hunting_df=lru.hunting_df)\n",
    "def do_cypher_tx(tx, file_name, verbose=False):\n",
    "    cypher_str = \"\"\"\n",
    "        MATCH (fn:FileNames {file_name: $file_name})\n",
    "        SET fn.percent_fit = NULL, fn.is_verified = true\n",
    "        RETURN fn;\"\"\"\n",
    "    if verbose:\n",
    "        clear_output(wait=True)\n",
    "        print(cypher_str.replace('$file_name', f'\"{file_name}\"'))\n",
    "    results_list = tx.run(query=cypher_str, parameters={'file_name': file_name})\n",
    "\n",
    "    return [dict(record.items()) for record in results_list]\n",
    "with cu.driver.session() as session:\n",
    "    row_objs_list = session.write_transaction(do_cypher_tx, file_name=file_name, verbose=False)\n",
    "row_objs_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f00b97db-00ab-4ab9-8a22-323bbb5c6ddd",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# You've made no changes to the qualification dictionary (regardless of parts-of-speech changes)\n",
    "def do_cypher_tx(tx, file_name, verbose=False):\n",
    "    cypher_str = \"\"\"\n",
    "        MATCH (fn:FileNames {file_name: $file_name})\n",
    "        SET fn.is_verified = true\n",
    "        RETURN fn;\"\"\"\n",
    "    if verbose:\n",
    "        clear_output(wait=True)\n",
    "        print(cypher_str.replace('$file_name', f'\"{file_name}\"'))\n",
    "    parameter_dict = {'file_name': file_name}\n",
    "    results_list = tx.run(query=cypher_str, parameters=parameter_dict)\n",
    "    values_list = []\n",
    "    for record in results_list:\n",
    "        values_list.append(dict(record.items()))\n",
    "\n",
    "    return values_list\n",
    "with cu.driver.session() as session:\n",
    "    row_objs_list = session.write_transaction(do_cypher_tx, file_name=file_name, verbose=True)\n",
    "row_objs_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2f8adbd-5b7c-43d8-8680-aea4da63a134",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Mark the file name as closed\n",
    "cypher_str = f'''\n",
    "    MATCH (fn:FileNames {{file_name: \"{file_name}\"}})\n",
    "    SET fn.is_closed = true\n",
    "    RETURN fn;'''\n",
    "print(cypher_str)\n",
    "with cu.driver.session() as session:\n",
    "    row_objs_list = session.write_transaction(cu.do_cypher_tx, cypher_str)\n",
    "row_objs_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "befe9664-96e1-43cb-8de7-99e40d5524f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Manually label the unscored qual\n",
    "qualification_str = quals_list[13]\n",
    "print(qualification_str)\n",
    "basic_quals_dict = s.load_object('basic_quals_dict')\n",
    "basic_quals_dict[qualification_str] = 0\n",
    "s.store_objects(basic_quals_dict=basic_quals_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a4a2c62-661a-4652-b743-efa5e336ad65",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Remove file name from database\n",
    "# file_name = '3c031ea6ad293e92_General_Service_Technician_Westborough_MA_01581_Indeed_com.html'\n",
    "cu.delete_filename_node(file_name, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3aef40b-f090-4b8d-9cab-afb370407893",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Fix the unhashable mess you made\n",
    "lru.basic_quals_df = lru.basic_quals_df.iloc[:-2]\n",
    "s.store_objects(basic_quals_df=lru.basic_quals_df)\n",
    "lru.basic_quals_dict = lru.basic_quals_df.set_index(\n",
    "    'qualification_str'\n",
    ").is_qualified.to_dict()\n",
    "lru.s.store_objects(basic_quals_dict=lru.basic_quals_dict, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b1033e0-944b-4770-b820-a4e6a3644e6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Fix the unhashable mess you made\n",
    "def do_cypher_tx(tx, verbose=False):\n",
    "    cypher_str = '''\n",
    "        MATCH (qs:QualificationStrings)\n",
    "        WHERE NOT qs.is_qualified IN [0, 1]\n",
    "        DETACH DELETE qs;\n",
    "        '''\n",
    "    results_list = tx.run(query=cypher_str, parameters={})\n",
    "\n",
    "    return [dict(record.items()) for record in results_list]\n",
    "with cu.driver.session() as session:\n",
    "    row_objs_list = session.write_transaction(do_cypher_tx, verbose=False)\n",
    "cypher_str = '''\n",
    "    MATCH (qs:QualificationStrings)\n",
    "    RETURN qs;'''\n",
    "row_objs_list = cu.get_execution_results(cypher_str, verbose=False)\n",
    "DataFrame(\n",
    "    [{k: v for k, v in row_obj['qs'].items()} for row_obj in row_objs_list]\n",
    ").tail(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0646f71a-405a-44d9-aebd-18e70c97f08a",
   "metadata": {},
   "source": [
    "\n",
    "----\n",
    "# Take a badly written requirements section and see if you can programmatically parse the qualification string out of it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "bb796bf8-0976-4f29-b731-ab41a311f658",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def combine_adjacent(split_strs_list):\n",
    "    combined_list = []\n",
    "    for i, s in enumerate(split_strs_list):\n",
    "        if i == 0:\n",
    "            combined_list.append(s)\n",
    "        elif combined_list[-1].lower().endswith(' and'):\n",
    "            combined_list[-1] = combined_list[-1] + ' ' + s\n",
    "        else:\n",
    "            combined_list.append(s)\n",
    "    \n",
    "    return combined_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "5ee1b865-e430-4859-8589-3a9b6a3af89c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Split O-RQs DataFrame built in 2 seconds\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Break the long HTML string into sentences and check if each is a qualification string\n",
    "t1 = time.time()\n",
    "from langchain.text_splitter import SpacyTextSplitter\n",
    "import re\n",
    "\n",
    "fake_stops_list = ['e.g.', 'etc.', 'M.S.', 'B.S.', 'Ph.D.', '(ex.', '(Ex.',\n",
    "                   'U.S.', 'i.e.', '&amp;', 'E.g.', 'Bsc.', 'MSc.', 'incl.']\n",
    "replacements_list = ['eg', 'etc', 'MS', 'BS', 'PhD', '(eg', '(eg', 'US',\n",
    "                     'ie', '&', 'eg', 'BS', 'MS', 'include']\n",
    "text_splitter = SpacyTextSplitter()\n",
    "tag_regex = re.compile('<([a-z][a-z0-9]*)[^<>]*>')\n",
    "rows_list = []\n",
    "unhtml_str = re.sub('</?[^><]+>', '', child_str)\n",
    "for fake_stop, replacement in zip(fake_stops_list, replacements_list):\n",
    "    unhtml_str = unhtml_str.replace(fake_stop, replacement)\n",
    "split_strs_list = combine_adjacent([str(split_str) for split_str in text_splitter._tokenizer(unhtml_str).sents])\n",
    "for split_str in split_strs_list:\n",
    "    row_dict = {}\n",
    "    split_str = re.sub(r'\\s*[:;.*]+\\s*$', '', split_str)\n",
    "    row_dict['split_str'] = split_str\n",
    "    row_dict['char_count'] = len(split_str)\n",
    "    match_obj = tag_regex.search(child_str)\n",
    "    if match_obj:\n",
    "        tag_name = match_obj.group()\n",
    "        split_str = f'<{tag_name}>{split_str}</{tag_name}>'\n",
    "    else:\n",
    "        tag_name = 'plaintext'\n",
    "    row_dict['tag_name'] = tag_name\n",
    "    score = 1.0\n",
    "    score *= slrcu.pos_predict_percent_fit_dict['O-RQ'](split_str)\n",
    "    score *= scrfcu.pos_predict_percent_fit_dict['O-RQ'](split_str)\n",
    "    score *= ssgdcu.pos_predict_percent_fit_dict['O-RQ'](split_str)\n",
    "    row_dict['orq_score'] = score\n",
    "    score = 1.0\n",
    "    score *= slrcu.pos_predict_percent_fit_dict['O-PQ'](split_str)\n",
    "    score *= scrfcu.pos_predict_percent_fit_dict['O-PQ'](split_str)\n",
    "    score *= ssgdcu.pos_predict_percent_fit_dict['O-PQ'](split_str)\n",
    "    row_dict['opq_score'] = score\n",
    "    rows_list.append(row_dict)\n",
    "split_orqs_df = DataFrame(rows_list)\n",
    "duration_str = humanize.precisedelta(time.time() - t1, minimum_unit='seconds', format='%0.0f')\n",
    "print(f'Split O-RQs DataFrame built in {duration_str}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "cebf9223-1670-472a-b245-6e0930fb2ab4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>split_str</th>\n",
       "      <th>char_count</th>\n",
       "      <th>tag_name</th>\n",
       "      <th>orq_score</th>\n",
       "      <th>opq_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>3-5 years of experience in data science and da...</td>\n",
       "      <td>78</td>\n",
       "      <td>plaintext</td>\n",
       "      <td>3.818604e-01</td>\n",
       "      <td>1.237190e-06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>Experience using AI/ML python libraries –PyTor...</td>\n",
       "      <td>259</td>\n",
       "      <td>plaintext</td>\n",
       "      <td>3.729124e-01</td>\n",
       "      <td>1.369416e-03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>Experience working with MLOps infrastructure s...</td>\n",
       "      <td>128</td>\n",
       "      <td>plaintext</td>\n",
       "      <td>3.484962e-01</td>\n",
       "      <td>1.842955e-04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>Proficient in Python programming</td>\n",
       "      <td>32</td>\n",
       "      <td>plaintext</td>\n",
       "      <td>2.616722e-01</td>\n",
       "      <td>1.276113e-08</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>This position requires a motivated individual ...</td>\n",
       "      <td>280</td>\n",
       "      <td>plaintext</td>\n",
       "      <td>1.635554e-01</td>\n",
       "      <td>1.455531e-07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>Legally authorized to work in the US</td>\n",
       "      <td>36</td>\n",
       "      <td>plaintext</td>\n",
       "      <td>7.631722e-02</td>\n",
       "      <td>1.789053e-06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>Knowledge and experience in some of these: Dee...</td>\n",
       "      <td>219</td>\n",
       "      <td>plaintext</td>\n",
       "      <td>6.289732e-02</td>\n",
       "      <td>4.190706e-04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>Architect, develop, deploy, and maintain scala...</td>\n",
       "      <td>219</td>\n",
       "      <td>plaintext</td>\n",
       "      <td>5.318624e-02</td>\n",
       "      <td>3.283486e-10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>Work with cross-functional teams to derive the...</td>\n",
       "      <td>159</td>\n",
       "      <td>plaintext</td>\n",
       "      <td>4.171702e-02</td>\n",
       "      <td>9.168556e-09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>Develop tools in analyzing diverse sets of imp...</td>\n",
       "      <td>222</td>\n",
       "      <td>plaintext</td>\n",
       "      <td>3.348758e-02</td>\n",
       "      <td>7.542002e-11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>Understand the system operations, limitations,...</td>\n",
       "      <td>63</td>\n",
       "      <td>plaintext</td>\n",
       "      <td>9.729517e-03</td>\n",
       "      <td>4.782181e-09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>The combination of excellence in battery techn...</td>\n",
       "      <td>205</td>\n",
       "      <td>plaintext</td>\n",
       "      <td>8.384333e-03</td>\n",
       "      <td>4.784805e-06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>They must be able to work independently in a d...</td>\n",
       "      <td>82</td>\n",
       "      <td>plaintext</td>\n",
       "      <td>5.564461e-03</td>\n",
       "      <td>3.445191e-06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>LGES Vertech empowers and expects its team mem...</td>\n",
       "      <td>179</td>\n",
       "      <td>plaintext</td>\n",
       "      <td>3.759914e-03</td>\n",
       "      <td>8.587309e-06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>Skills and Experience: Required: Master’s or B...</td>\n",
       "      <td>132</td>\n",
       "      <td>plaintext</td>\n",
       "      <td>3.147256e-03</td>\n",
       "      <td>1.057429e-09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>Work with DevOps team to architect software mo...</td>\n",
       "      <td>140</td>\n",
       "      <td>plaintext</td>\n",
       "      <td>8.100080e-04</td>\n",
       "      <td>6.038575e-09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Our systems address our customers' needs to re...</td>\n",
       "      <td>258</td>\n",
       "      <td>plaintext</td>\n",
       "      <td>2.543704e-04</td>\n",
       "      <td>5.344878e-06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>Work closely with cross-functional teams of da...</td>\n",
       "      <td>147</td>\n",
       "      <td>plaintext</td>\n",
       "      <td>1.702549e-04</td>\n",
       "      <td>3.468670e-07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Position Overview The Senior Data Scientist is...</td>\n",
       "      <td>102</td>\n",
       "      <td>plaintext</td>\n",
       "      <td>1.041647e-04</td>\n",
       "      <td>4.756000e-07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Our service capabilities include advanced moni...</td>\n",
       "      <td>134</td>\n",
       "      <td>plaintext</td>\n",
       "      <td>7.431245e-05</td>\n",
       "      <td>1.202627e-04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>Key Responsibilities Be an individual contribu...</td>\n",
       "      <td>193</td>\n",
       "      <td>plaintext</td>\n",
       "      <td>5.024301e-05</td>\n",
       "      <td>2.640119e-10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>Desired: Masters or PhD Graduate with experien...</td>\n",
       "      <td>358</td>\n",
       "      <td>plaintext</td>\n",
       "      <td>4.284255e-05</td>\n",
       "      <td>5.111316e-03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Company Overview LG Energy Solution Vertech, I...</td>\n",
       "      <td>128</td>\n",
       "      <td>plaintext</td>\n",
       "      <td>3.892899e-05</td>\n",
       "      <td>1.107686e-04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Using our core strengths of expert service to ...</td>\n",
       "      <td>215</td>\n",
       "      <td>plaintext</td>\n",
       "      <td>3.555921e-05</td>\n",
       "      <td>3.274243e-04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Our AEROS® energy operating system is the engi...</td>\n",
       "      <td>176</td>\n",
       "      <td>plaintext</td>\n",
       "      <td>3.170556e-05</td>\n",
       "      <td>1.334708e-06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>The individual in this position will be respon...</td>\n",
       "      <td>162</td>\n",
       "      <td>plaintext</td>\n",
       "      <td>1.693438e-06</td>\n",
       "      <td>2.533433e-10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>They will have the opportunity to innovate new...</td>\n",
       "      <td>234</td>\n",
       "      <td>plaintext</td>\n",
       "      <td>2.267425e-07</td>\n",
       "      <td>2.202967e-07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Our diverse and growing team enjoys competitiv...</td>\n",
       "      <td>175</td>\n",
       "      <td>plaintext</td>\n",
       "      <td>1.544309e-07</td>\n",
       "      <td>1.143961e-08</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>For more information about LGESVT, please visi...</td>\n",
       "      <td>66</td>\n",
       "      <td>plaintext</td>\n",
       "      <td>4.586381e-08</td>\n",
       "      <td>4.033160e-07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>Work in MLOps infrastructure to automate ML mo...</td>\n",
       "      <td>236</td>\n",
       "      <td>plaintext</td>\n",
       "      <td>1.397907e-08</td>\n",
       "      <td>1.646237e-11</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            split_str  char_count   tag_name  \\\n",
       "23  3-5 years of experience in data science and da...          78  plaintext   \n",
       "26  Experience using AI/ML python libraries –PyTor...         259  plaintext   \n",
       "27  Experience working with MLOps infrastructure s...         128  plaintext   \n",
       "25                   Proficient in Python programming          32  plaintext   \n",
       "10  This position requires a motivated individual ...         280  plaintext   \n",
       "28               Legally authorized to work in the US          36  plaintext   \n",
       "24  Knowledge and experience in some of these: Dee...         219  plaintext   \n",
       "16  Architect, develop, deploy, and maintain scala...         219  plaintext   \n",
       "18  Work with cross-functional teams to derive the...         159  plaintext   \n",
       "21  Develop tools in analyzing diverse sets of imp...         222  plaintext   \n",
       "17  Understand the system operations, limitations,...          63  plaintext   \n",
       "5   The combination of excellence in battery techn...         205  plaintext   \n",
       "13  They must be able to work independently in a d...          82  plaintext   \n",
       "6   LGES Vertech empowers and expects its team mem...         179  plaintext   \n",
       "22  Skills and Experience: Required: Master’s or B...         132  plaintext   \n",
       "19  Work with DevOps team to architect software mo...         140  plaintext   \n",
       "2   Our systems address our customers' needs to re...         258  plaintext   \n",
       "15  Work closely with cross-functional teams of da...         147  plaintext   \n",
       "9   Position Overview The Senior Data Scientist is...         102  plaintext   \n",
       "4   Our service capabilities include advanced moni...         134  plaintext   \n",
       "14  Key Responsibilities Be an individual contribu...         193  plaintext   \n",
       "29  Desired: Masters or PhD Graduate with experien...         358  plaintext   \n",
       "0   Company Overview LG Energy Solution Vertech, I...         128  plaintext   \n",
       "1   Using our core strengths of expert service to ...         215  plaintext   \n",
       "3   Our AEROS® energy operating system is the engi...         176  plaintext   \n",
       "11  The individual in this position will be respon...         162  plaintext   \n",
       "12  They will have the opportunity to innovate new...         234  plaintext   \n",
       "7   Our diverse and growing team enjoys competitiv...         175  plaintext   \n",
       "8   For more information about LGESVT, please visi...          66  plaintext   \n",
       "20  Work in MLOps infrastructure to automate ML mo...         236  plaintext   \n",
       "\n",
       "       orq_score     opq_score  \n",
       "23  3.818604e-01  1.237190e-06  \n",
       "26  3.729124e-01  1.369416e-03  \n",
       "27  3.484962e-01  1.842955e-04  \n",
       "25  2.616722e-01  1.276113e-08  \n",
       "10  1.635554e-01  1.455531e-07  \n",
       "28  7.631722e-02  1.789053e-06  \n",
       "24  6.289732e-02  4.190706e-04  \n",
       "16  5.318624e-02  3.283486e-10  \n",
       "18  4.171702e-02  9.168556e-09  \n",
       "21  3.348758e-02  7.542002e-11  \n",
       "17  9.729517e-03  4.782181e-09  \n",
       "5   8.384333e-03  4.784805e-06  \n",
       "13  5.564461e-03  3.445191e-06  \n",
       "6   3.759914e-03  8.587309e-06  \n",
       "22  3.147256e-03  1.057429e-09  \n",
       "19  8.100080e-04  6.038575e-09  \n",
       "2   2.543704e-04  5.344878e-06  \n",
       "15  1.702549e-04  3.468670e-07  \n",
       "9   1.041647e-04  4.756000e-07  \n",
       "4   7.431245e-05  1.202627e-04  \n",
       "14  5.024301e-05  2.640119e-10  \n",
       "29  4.284255e-05  5.111316e-03  \n",
       "0   3.892899e-05  1.107686e-04  \n",
       "1   3.555921e-05  3.274243e-04  \n",
       "3   3.170556e-05  1.334708e-06  \n",
       "11  1.693438e-06  2.533433e-10  \n",
       "12  2.267425e-07  2.202967e-07  \n",
       "7   1.544309e-07  1.143961e-08  \n",
       "8   4.586381e-08  4.033160e-07  \n",
       "20  1.397907e-08  1.646237e-11  "
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "split_orqs_df.sort_values('orq_score', ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "232f78ce-e9d8-43b6-b138-38641dcedd42",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Take a badly written requirements section and see if you can programmatically parse the qualification string out of it\n",
    "import re\n",
    "from nltk.tokenize import sent_tokenize\n",
    "\n",
    "# sampling_strategy_limit=6_400 gets 10,635 labeled parts of speech and takes 49 minutes and 30 seconds\n",
    "# sampling_strategy_limit=7_000 gets 10,635 labeled parts of speech and takes 49 minutes and 30 seconds\n",
    "slrcu.build_pos_logistic_regression_elements(sampling_strategy_limit=70_000, verbose=True)\n",
    "\n",
    "qual_paragraph = re.sub('</?[^<>]+>', '', child_str.strip(), 0, re.MULTILINE)\n",
    "if len(sent_tokenize(qual_paragraph)) < 2:\n",
    "    child_strs_list = re.split(' *: *', qual_paragraph, 0)\n",
    "    child_tags_list = ha.get_child_tags_list(child_strs_list)\n",
    "    feature_dict_list = cu.get_feature_dict_list(child_tags_list, child_strs_list)\n",
    "    feature_tuple_list = []\n",
    "    for feature_dict in feature_dict_list:\n",
    "        feature_tuple_list.append(hc.get_feature_tuple(feature_dict, pos_lr_predict_single=slrcu.predict_single, pos_crf_predict_single=None, pos_sgd_predict_single=None))\n",
    "    crf_list = crf.CRF.predict_single(crf.sent2features(feature_tuple_list))\n",
    "    if crf_list[0] == 'H-RQ':\n",
    "        child_strs_list = re.split(' *; *', ': '.join(child_strs_list[1:]), 0)\n",
    "        child_tags_list = ha.get_child_tags_list(child_strs_list)\n",
    "        feature_dict_list = cu.get_feature_dict_list(child_tags_list, child_strs_list)\n",
    "        feature_tuple_list = []\n",
    "        for feature_dict in feature_dict_list:\n",
    "            feature_tuple_list.append(hc.get_feature_tuple(feature_dict, pos_lr_predict_single=slrcu.predict_single, pos_crf_predict_single=None, pos_sgd_predict_single=None))\n",
    "        crf_list = crf.CRF.predict_single(crf.sent2features(feature_tuple_list))\n",
    "        db_pos_list = []\n",
    "        for navigable_parent in child_strs_list:\n",
    "            db_pos_list = cu.append_parts_of_speech_list(navigable_parent, pos_list=db_pos_list)\n",
    "        pos_list, indices_list = su.visualize_basic_quals_section(crf_list, child_strs_list, db_pos_list=db_pos_list, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16fe3a9a-8b2b-4bfc-b348-ce01a188c19d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import nltk.tokenize\n",
    "\n",
    "dir(nltk.tokenize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60f18ee0-88df-42c4-b48d-04a9ed3508c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "[f'nltk.tokenize.{fn}' for fn in dir(nltk.tokenize) if 'Tokenize' in fn]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "854c3df8-2841-4579-a681-9b03518d5360",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "nltk.tokenize.TweetTokenizer?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5c1b67b-0583-4e3e-a2ff-9ffbc8adf7dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "[f'nltk.tokenize.{fn}' for fn in dir(nltk.tokenize) if 'tokenize' in fn]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c02d533e-41c9-4ad3-94a5-312f6bc4966c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "nltk.tokenize.wordpunct_tokenize(child_str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62d56c92-2077-4e48-b287-8a8314c12214",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "nltk.tokenize.word_tokenize(child_str, preserve_line=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09a43f97-3e61-4100-8026-d73f2437eb21",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "list(nltk.tokenize.string_span_tokenize(child_str, r';\\s*'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "771cf5ab-3264-4833-bc8e-3763ae9284cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "nltk.tokenize.sent_tokenize(child_str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87620f95-6be5-46de-bc5b-8548d9316f5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "list(nltk.tokenize.regexp_tokenize(child_str, r'\\w+'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ebf922d-1e93-48e0-82f4-f47e64cc42f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "list(nltk.tokenize.regexp_span_tokenize(child_str, r'\\s\\s+'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5c338cc-6e72-49e8-a7b8-1cc093008d2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "nltk.tokenize.line_tokenize(child_str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cd7daf2-70b0-4cc3-868d-1f8e34384722",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "nltk.tokenize.casual_tokenize(child_str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3287dff-0406-4c4d-89ee-82b6ef750457",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "nltk.tokenize.blankline_tokenize(child_str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55f74215-ceda-4159-a101-90417d72f694",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Job Hunting (Python 3.10.9)",
   "language": "python",
   "name": "jh_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
