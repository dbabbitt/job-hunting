{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pretty printing has been turned OFF\n"
     ]
    }
   ],
   "source": [
    "\n",
    "%pprint"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "\n",
    "---\n",
    "# Load needed libraries and functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from datetime import datetime\n",
    "import humanize\n",
    "import os\n",
    "import sys\n",
    "import time\n",
    "import warnings\n",
    "import winsound\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "duration = 1000  # milliseconds\n",
    "freq = 880  # Hz\n",
    "\n",
    "# Insert at 1, 0 is the script path (or '' in REPL)\n",
    "sys.path.insert(1, '../py')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Utility libraries created in 1 second\n",
      "Last run on 2023-11-30 14:16:47.839746\n"
     ]
    }
   ],
   "source": [
    "\n",
    "t0 = time.time()\n",
    "\n",
    "# Get the Neo4j driver\n",
    "from storage import Storage\n",
    "s = Storage(\n",
    "    data_folder_path=os.path.abspath('../data'),\n",
    "    saves_folder_path=os.path.abspath('../saves')\n",
    ")\n",
    "\n",
    "from ha_utils import HeaderAnalysis\n",
    "ha = HeaderAnalysis(s=s, verbose=False)\n",
    "\n",
    "from scrape_utils import WebScrapingUtilities\n",
    "wsu = WebScrapingUtilities(\n",
    "    s=s,\n",
    "    secrets_json_path=os.path.abspath('../data/secrets/jh_secrets.json')\n",
    ")\n",
    "uri = wsu.secrets_json['neo4j']['connect_url']\n",
    "user =  wsu.secrets_json['neo4j']['username']\n",
    "password = wsu.secrets_json['neo4j']['password']\n",
    "\n",
    "# Get the neo4j object\n",
    "from cypher_utils import CypherUtilities\n",
    "cu = CypherUtilities(\n",
    "    uri=uri, user=user, password=password, driver=None, s=s, ha=ha\n",
    ")\n",
    "\n",
    "duration_str = humanize.precisedelta(time.time() - t0, minimum_unit='seconds', format='%0.0f')\n",
    "print(f'Utility libraries created in {duration_str}')\n",
    "print(f'Last run on {datetime.now()}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from hc_utils import HeaderCategories\n",
    "\n",
    "hc = HeaderCategories(cu=cu, verbose=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Keep the total creation time to less than one hour by adjusting the sampling strategy limit\n",
    "from lr_utils import LrUtilities\n",
    "\n",
    "lru = LrUtilities(ha=ha, cu=cu, hc=hc, verbose=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from section_classifier_utils import SectionLRClassifierUtilities, SectionSGDClassifierUtilities, SectionCRFClassifierUtilities\n",
    "\n",
    "scrfcu = SectionCRFClassifierUtilities(cu=cu, ha=ha, verbose=False)\n",
    "slrcu = SectionLRClassifierUtilities(ha=ha, cu=cu, verbose=False)\n",
    "ssgdcu = SectionSGDClassifierUtilities(ha=ha, cu=cu, verbose=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from crf_utils import CrfUtilities\n",
    "\n",
    "crf = CrfUtilities(ha=ha, hc=hc, cu=cu, lru=lru, slrcu=slrcu, scrfcu=scrfcu, ssgdcu=ssgdcu, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from section_utils import SectionUtilities\n",
    "\n",
    "su = SectionUtilities(wsu=wsu, ihu=None, hc=hc, crf=crf, slrcu=slrcu, verbose=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "predict_single is now available\n",
      "Parts-of-speech conditional random field elements built in 5 seconds\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Parts-of-speech CRF elements built in 28 minutes and 51 seconds\n",
    "t1 = time.time()\n",
    "if not (hasattr(scrfcu, 'pos_symbol_crf') or crf.is_flask_running()):\n",
    "    scrfcu.build_pos_conditional_random_field_elements(verbose=True)\n",
    "if hasattr(scrfcu, 'pos_symbol_crf'):\n",
    "    print('predict_single is now available')\n",
    "else:\n",
    "    print('predict_single is not available')\n",
    "duration_str = humanize.precisedelta(time.time() - t1, minimum_unit='seconds', format='%0.0f')\n",
    "print(f'Parts-of-speech conditional random field elements built in {duration_str}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "POS classifier trained in 0 seconds\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Check if the crf has built its parts-of-speech classifier\n",
    "# POS classifier trained in 12 hours, 15 minutes and 36 seconds\n",
    "t1 = time.time()\n",
    "if not hasattr(crf, 'CRF'):\n",
    "    if s.pickle_exists('crf_CRF'):\n",
    "        crf.CRF = s.load_object('crf_CRF')\n",
    "    else:\n",
    "        crf.retrain_pos_classifier(header_pattern_dict=s.load_object('HEADER_PATTERN_DICT'), verbose=True)\n",
    "duration_str = humanize.precisedelta(time.time() - t1, minimum_unit='seconds', format='%0.0f')\n",
    "print(f'POS classifier trained in {duration_str}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I have 49,102 labeled parts of speech in here\n",
      "predict_single is now available\n",
      "Parts-of-speech stochastic gradient decent elements built in 13 seconds\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Parts-of-speech stochastic gradient decent elements built in 17 seconds\n",
    "t1 = time.time()\n",
    "if not (hasattr(ssgdcu, 'pos_predict_percent_fit_dict') or crf.is_flask_running()):\n",
    "    ssgdcu.build_pos_stochastic_gradient_descent_elements(sampling_strategy_limit=None, verbose=True)\n",
    "if hasattr(ssgdcu, 'pos_predict_percent_fit_dict'):\n",
    "    print('predict_single is now available')\n",
    "else:\n",
    "    print('predict_single is not available')\n",
    "duration_str = humanize.precisedelta(time.time() - t1, minimum_unit='seconds', format='%0.0f')\n",
    "print(f'Parts-of-speech stochastic gradient decent elements built in {duration_str}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from is_header_sgd_classifier import IsHeaderSgdClassifier\n",
    "\n",
    "ihu = IsHeaderSgdClassifier(ha=ha, cu=cu, verbose=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "----"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Run this if you haven't already created the file, but need to edit other_email.html first\n",
    "import re\n",
    "import shutil\n",
    "\n",
    "file_name = ''\n",
    "if file_name: file_path = os.path.join(cu.SAVES_HTML_FOLDER, file_name)\n",
    "else:\n",
    "    file_path = os.path.abspath('../data/html/other_email.html')\n",
    "    command_str = fr'\"C:\\Program Files\\Notepad++\\notepad++.exe\" {file_path}'\n",
    "    print(command_str)\n",
    "    !{command_str}\n",
    "    file_name = re.sub(r'[^A-Za-z0-9]+', ' ', '''\n",
    "        100% Remote Opportunity for Data Scientist Sr.\n",
    "        Satyam Kumar Pandey\n",
    "        ''').strip().replace(' ', '_') + '.html'\n",
    "    new_file_path = os.path.join(cu.SAVES_HTML_FOLDER, file_name)\n",
    "    if os.path.isfile(new_file_path):\n",
    "        file_name = datetime.now().strftime('%Y%m%d%H%M%S%f') + f'_{file_name}'\n",
    "        new_file_path = os.path.join(cu.SAVES_HTML_FOLDER, file_name)\n",
    "    if not os.path.isfile(new_file_path):\n",
    "        shutil.copy(file_path, os.path.join(cu.SAVES_HTML_FOLDER, file_name))\n",
    "        print(file_name)\n",
    "page_soup = wsu.get_page_soup(file_path)\n",
    "div_soup = page_soup.find_all(name='div', id='jobDescriptionText')[0]\n",
    "child_strs_list = ha.get_navigable_children(div_soup, [])\n",
    "cu.ensure_filename(file_name, verbose=False)\n",
    "cu.populate_from_child_strings(child_strs_list, file_name, verbose=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Add the posting URL to the file name only if you have one\n",
    "posting_url = ''\n",
    "if posting_url:\n",
    "    cypher_str = f'''\n",
    "        MATCH (fn:FileNames {{file_name: \"{file_name}\"}})\n",
    "        SET fn.posting_url = \"{posting_url}\"\n",
    "        RETURN fn;'''\n",
    "    with cu.driver.session() as session:\n",
    "        row_objs_list = session.write_transaction(cu.do_cypher_tx, cypher_str)\n",
    "    display(row_objs_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "----"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "child_tags_list = ha.get_child_tags_list(child_strs_list)\n",
    "\n",
    "feature_dict_list = cu.get_feature_dict_list(child_tags_list, child_strs_list)\n",
    "feature_tuple_list = []\n",
    "for feature_dict in feature_dict_list:\n",
    "    feature_tuple_list.append(hc.get_feature_tuple(\n",
    "        feature_dict, pos_lr_predict_single=slrcu.predict_single, pos_crf_predict_single=scrfcu.predict_single\n",
    "    ))\n",
    "crf_list = crf.CRF.predict_single(crf.sent2features(feature_tuple_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['H-JT', 'O-JT', 'H-OL', 'O-OL', 'O-SP', 'O-O', 'O-RQ', 'H-JD', 'O-JD', 'O-O', 'O-RQ', 'O-O', 'O-RQ', 'O-O', 'O-IP', 'H-RQ', 'O-RQ', 'O-TS', 'O-O', 'O-TS', 'H-TS', 'O-TS', 'O-TS', 'H-RQ', 'O-RQ', 'O-RQ', 'O-RQ', 'O-RQ', 'H-PQ', 'O-PQ', 'O-PQ', 'O-O', 'O-RQ']\n",
      "[6, 10, 12, 16, 24, 25, 26, 27]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "0 H-JT) <span style=\"color:#d62728ff;\"><span style=\"font-size:12.0pt\">Job Title (H-JT Job Title Header)</span></span><br />1 O-JT) <span style=\"color:#d6272880;\"><span style=\"font-family:Century Gothic,sans-serif\">Data Scientist Sr (O-JT Job Title Non-header)</span></span><br />2 H-OL) <span style=\"color:#c49c94ff;\"><span style=\"font-size:12.0pt\">Location (H-OL Office Location Header)</span></span><br />3 O-OL) <span style=\"color:#c49c9480;\"><span style=\"font-family:Century Gothic,sans-serif\">100% remote - (O-OL Office Location Non-header)</span></span><br />4 O-SP) <span style=\"color:#17becf80;\"><span style=\"font-family:Century Gothic,sans-serif\">potential for onsite team meetings up to 1x per month (CareFirst will cover travel expenses if necessary) (O-SP Supplemental Pay Non-header)</span></span><br />5 O-O) <span style=\"color:#8c564b80;\"><span style=\"font-family:Century Gothic,sans-serif\">- (O-O Other Non-header)</span></span><br /><hr />6 O-RQ) <span style=\"color:#bcbd2280;\"><span style=\"font-family:Century Gothic,sans-serif\">No west coast candidates (O-RQ Required Qualifications Non-header)</span></span><br />7 H-JD) <span style=\"color:#98df8aff;\"><span style=\"font-size:12.0pt\">Duration (H-JD Job Duration Header)</span></span><br />8 O-JD) <span style=\"color:#98df8a80;\"><span style=\"font-family:Century Gothic,sans-serif\">12 month contract with likelihood of extension. (O-JD Job Duration Non-header)</span></span><br />9 O-O) <span style=\"color:#8c564b80;\"><span style=\"font-size:12.0pt\">Candidate Status Requirement (O-O Other Non-header)</span></span><br />10 O-RQ) <span style=\"color:#bcbd2280;\"><span style=\"font-family:Century Gothic,sans-serif\">None (O-RQ Required Qualifications Non-header)</span></span><br />11 O-O) <span style=\"color:#8c564b80;\"><span style=\"font-size:12.0pt\">Interview Process (O-O Other Non-header)</span></span><br />12 O-RQ) <span style=\"color:#bcbd2280;\"><span style=\"font-family:Century Gothic,sans-serif\">MS Teams (O-RQ Required Qualifications Non-header)</span></span><br />13 O-O) <span style=\"color:#8c564b80;\"><span style=\"font-size:12.0pt\">Project Summary (O-O Other Non-header)</span></span><br />14 O-IP) <span style=\"color:#ffbb7880;\">CareFirst is seeking a Data Scientist to support the Machine Learning component of its AuditIQ Product which helps proactively identify claim overpayments prior to FEPOC’s annual OIG (Office of Inspector General) Audits. C (O-IP Interview Procedures Non-header)</span><br />15 H-RQ) <span style=\"color:#bcbd22ff;\"><strong>areFirst is (H-RQ Required Qualifications Header)</strong></span><br />16 O-RQ) <span style=\"color:#bcbd2280;\"><span style=\"font-family:Century Gothic,sans-serif\">NOT (O-RQ Required Qualifications Non-header)</span></span><br />17 O-TS) <span style=\"color:#9edae580;\"><span style=\"font-family:Century Gothic,sans-serif\">looking for a data science researcher. They are seeking a resource that can assist with the ingestion of data along with the deployment and production of machine learning scripts. (O-TS Task Scope Non-header)</span></span><br />18 O-O) <span style=\"color:#8c564b80;\"><span style=\"font-size:12.0pt\">Essential Functions (O-O Other Non-header)</span></span><br />19 O-TS) <span style=\"color:#9edae580;\">•Identifies and solves business problems by using various numerical techniques, algorithms, and models in statistical modeling, machine learning, operations research, and data mining. (O-TS Task Scope Non-header)</span><br />20 H-TS) <span style=\"color:#9edae5ff;\">•Uses advanced analytical capabilities to support data science initiatives. (H-TS Task Scope Header)</span><br />21 O-TS) <span style=\"color:#9edae580;\">•Communicates across product teams and with customers and educates on artificial intelligence, machine learning, and statistical models. (O-TS Task Scope Non-header)</span><br />22 O-TS) <span style=\"color:#9edae580;\">•Acting as a liaison and interface between analytics, business units and other departments. (O-TS Task Scope Non-header)</span><br />23 H-RQ) <span style=\"color:#bcbd22ff;\"><span style=\"font-size:12.0pt\">Top Five Skills Required (H-RQ Required Qualifications Header)</span></span><br />24 O-RQ) <span style=\"color:#bcbd2280;\">• 5+ years of experience as a Data Scientist – Proficiency in data science modeling (AI, Machine Learning, Deep Learning, Decision Trees, Random Forest, Neural Networks, Supervised / Unsupervised Learning, Forecasting, Predictive Modeling and Clustering) (O-RQ Required Qualifications Non-header)</span><br />25 O-RQ) <span style=\"color:#bcbd2280;\">• Experience with machine learning methods like k-nearest neighbors, random forests, and ensemble methods. (O-RQ Required Qualifications Non-header)</span><br />26 O-RQ) <span style=\"color:#bcbd2280;\">• Experience using Python Machine Learning & Data Pre-processing Libraries. (Scikit Learn, Numpy, Pandas) (O-RQ Required Qualifications Non-header)</span><br />27 O-RQ) <span style=\"color:#bcbd2280;\">• Python and PySpark scripting experience (O-RQ Required Qualifications Non-header)</span><br /><hr />28 H-PQ) <span style=\"color:#c7c7c7ff;\"><span style=\"font-size:12.0pt\">Nice to have skills (H-PQ Preferred Qualifications Header)</span></span><br />29 O-PQ) <span style=\"color:#c7c7c780;\">• Prior healthcare payer experience (O-PQ Preferred Qualifications Non-header)</span><br />30 O-PQ) <span style=\"color:#c7c7c780;\">• Prior cloud data science experience. Experience with Cloudera Data Science Workbench (O-PQ Preferred Qualifications Non-header)</span><br />31 O-O) <span style=\"color:#8c564b80;\"><span style=\"font-size:12.0pt\">Education (O-O Other Non-header)</span></span><br />32 O-RQ) <span style=\"color:#bcbd2280;\"><span style=\"font-family:Century Gothic,sans-serif\">Bachelor's degree, an additional 4 years of relevant work experience is required (O-RQ Required Qualifications Non-header)</span></span><br />"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[6, 10, 12, 16, 24, 25, 26, 27]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "db_pos_list = []\n",
    "for navigable_parent in child_strs_list:\n",
    "    db_pos_list = cu.append_parts_of_speech_list(navigable_parent, pos_list=db_pos_list)\n",
    "pos_list, indices_list = su.visualize_basic_quals_section(crf_list, child_strs_list, db_pos_list=db_pos_list, verbose=True)\n",
    "winsound.Beep(freq, duration)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "----"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[6, 10, 12, 16, 24, 25, 26, 27]\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "list index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[87], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Display the context of an individual child string\u001b[39;00m\n\u001b[0;32m      2\u001b[0m idx \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m33\u001b[39m\n\u001b[1;32m----> 3\u001b[0m \u001b[38;5;28mprint\u001b[39m(indices_list); child_str \u001b[38;5;241m=\u001b[39m \u001b[43mchild_strs_list\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m]\u001b[49m; pos_symbol \u001b[38;5;241m=\u001b[39m pos_list[idx]; basic_quals_dict \u001b[38;5;241m=\u001b[39m s\u001b[38;5;241m.\u001b[39mload_object(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbasic_quals_dict\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;28mstr\u001b[39m(basic_quals_dict[child_str]) \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m'\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m(child_str \u001b[38;5;129;01min\u001b[39;00m basic_quals_dict) \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m'\u001b[39m, end\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m      5\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00midx\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpos_symbol\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m) \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mchild_str\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n",
      "\u001b[1;31mIndexError\u001b[0m: list index out of range"
     ]
    }
   ],
   "source": [
    "\n",
    "# Display the context of an individual child string\n",
    "idx = 33\n",
    "print(indices_list); child_str = child_strs_list[idx]; pos_symbol = pos_list[idx]; basic_quals_dict = s.load_object('basic_quals_dict')\n",
    "print(str(basic_quals_dict[child_str]) + '\\n' if(child_str in basic_quals_dict) else '', end='')\n",
    "print(f'{idx} {pos_symbol}) {child_str}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pickling to C:\\Users\\daveb\\OneDrive\\Documents\\GitHub\\job-hunting\\saves\\pkl\\basic_quals_dict.pkl\n",
      "\"<span style=\"font-family:Century Gothic,sans-serif\">Bachelor's degree, an additional 4 years of relevant work experience is required</span>\" in basic_quals_dict: 1\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Hand-label this particular child string in the quals dictionary\n",
    "basic_quals_dict = s.load_object('basic_quals_dict')\n",
    "basic_quals_dict[child_str] = 1\n",
    "s.store_objects(basic_quals_dict=basic_quals_dict); print(f'\"{child_str}\" in basic_quals_dict: {basic_quals_dict[child_str]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'navigable_parent': '<span style=\"font-family:Century Gothic,sans-serif\">Bachelor\\'s degree, an additional 4 years of relevant work experience is required</span>', 'is_header': False, 'is_task_scope': False, 'is_minimum_qualification': False, 'is_preferred_qualification': False, 'is_legal_notification': False, 'is_job_title': False, 'is_office_location': False, 'is_job_duration': False, 'is_supplemental_pay': False, 'is_educational_requirement': True, 'is_interview_procedure': False, 'is_corporate_scope': False, 'is_posting_date': False, 'is_other': False}]"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "def do_cypher_tx(tx, navigable_parent, verbose=False):\n",
    "    cypher_str = \"\"\"MATCH (np:NavigableParents {navigable_parent: $navigable_parent})\n",
    "        SET\n",
    "            np.is_header = false,\n",
    "            np.is_task_scope = false,\n",
    "            np.is_minimum_qualification = false,\n",
    "            np.is_preferred_qualification = false,\n",
    "            np.is_educational_requirement = true,\n",
    "            np.is_legal_notification = false,\n",
    "            np.is_other = false,\n",
    "            np.is_corporate_scope = false,\n",
    "            np.is_job_title = false,\n",
    "            np.is_office_location = false,\n",
    "            np.is_job_duration = false,\n",
    "            np.is_supplemental_pay = false,\n",
    "            np.is_interview_procedure = false,\n",
    "            np.is_posting_date = false\n",
    "        \"\"\" + cu.return_everything_str + ';'\n",
    "    results_list = tx.run(query=cypher_str, parameters={'navigable_parent': navigable_parent})\n",
    "\n",
    "    return [dict(record.items()) for record in results_list]\n",
    "with cu.driver.session() as session: row_objs_list = session.write_transaction(do_cypher_tx, navigable_parent=child_str)\n",
    "# ihu.retrain_classifier(row_objs_list[0]['navigable_parent'], row_objs_list[0]['is_header'], verbose=True); row_objs_list\n",
    "row_objs_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'navigable_parent': '<p>Benefits:</p>', 'is_header': true, 'is_task_scope': false, 'is_qualification': false, 'is_minimum_qualification': false, 'is_preferred_qualification': false, 'is_legal_notification': false, 'is_job_title': false, 'is_office_location': false, 'is_job_duration': false, 'is_supplemental_pay': true, 'is_educational_requirement': false, 'is_interview_procedure': false, 'is_corporate_scope': false, 'is_posting_date': false, 'is_other': false}]"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "# Show what's in the database already for this html string\n",
    "def do_cypher_tx(tx, navigable_parent, verbose=False):\n",
    "    cypher_str = '''\n",
    "        MATCH (np:NavigableParents {navigable_parent: $navigable_parent})\n",
    "        ''' + cu.return_everything_str + ';'\n",
    "    results_list = tx.run(query=cypher_str, parameters={'navigable_parent': navigable_parent})\n",
    "\n",
    "    return [dict(record.items()) for record in results_list]\n",
    "with cu.driver.session() as session:\n",
    "    row_objs_list = session.write_transaction(do_cypher_tx, navigable_parent=child_str, verbose=False)\n",
    "row_objs_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Remove this particular child string from the quals dictionary\n",
    "basic_quals_dict = s.load_object('basic_quals_dict')\n",
    "child_str = child_strs_list[idx]\n",
    "basic_quals_dict.pop(child_str)\n",
    "# basic_quals_dict[child_str] = 0\n",
    "s.store_objects(basic_quals_dict=basic_quals_dict)\n",
    "print(f'\"{child_str}\" in basic_quals_dict: {child_str in basic_quals_dict}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "----"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Job Hunting (Python 3.10.9)",
   "language": "python",
   "name": "jh_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
