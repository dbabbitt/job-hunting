{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pretty printing has been turned OFF\n"
     ]
    }
   ],
   "source": [
    "\n",
    "%pprint"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "\n",
    "---\n",
    "# Load needed libraries and functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from datetime import datetime\n",
    "import humanize\n",
    "import os\n",
    "import re\n",
    "import shutil\n",
    "import sys\n",
    "import time\n",
    "import warnings\n",
    "import winsound\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "duration = 1000  # milliseconds\n",
    "freq = 880  # Hz\n",
    "\n",
    "# Insert at 1, 0 is the script path (or '' in REPL)\n",
    "sys.path.insert(1, '../py')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Utility libraries created in 3 seconds\n",
      "Last run on 2023-03-22 17:02:30.502738\n"
     ]
    }
   ],
   "source": [
    "\n",
    "t0 = time.time()\n",
    "\n",
    "# Get the Neo4j driver\n",
    "from storage import Storage\n",
    "s = Storage(\n",
    "    data_folder_path=os.path.abspath('../data'),\n",
    "    saves_folder_path=os.path.abspath('../saves')\n",
    ")\n",
    "\n",
    "from ha_utils import HeaderAnalysis\n",
    "ha = HeaderAnalysis(s=s, verbose=False)\n",
    "\n",
    "from scrape_utils import WebScrapingUtilities\n",
    "wsu = WebScrapingUtilities(\n",
    "    s=s,\n",
    "    secrets_json_path=os.path.abspath('../data/secrets/jh_secrets.json')\n",
    ")\n",
    "uri = wsu.secrets_json['neo4j']['connect_url']\n",
    "user =  wsu.secrets_json['neo4j']['username']\n",
    "password = wsu.secrets_json['neo4j']['password']\n",
    "\n",
    "# Get the neo4j object\n",
    "from cypher_utils import CypherUtilities\n",
    "cu = CypherUtilities(\n",
    "    uri=uri, user=user, password=password, driver=None, s=s, ha=ha\n",
    ")\n",
    "\n",
    "from hc_utils import HeaderCategories\n",
    "hc = HeaderCategories(cu=cu, verbose=False)\n",
    "\n",
    "from is_header_sgd_classifier import IsHeaderSgdClassifier\n",
    "ihu = IsHeaderSgdClassifier(ha=ha, cu=cu, verbose=False)\n",
    "\n",
    "from lr_utils import LrUtilities\n",
    "lru = LrUtilities(ha=ha, cu=cu, hc=hc, verbose=False)\n",
    "\n",
    "from crf_utils import CrfUtilities\n",
    "crf = CrfUtilities(ha=ha, hc=hc, cu=cu, lru=lru, verbose=True)\n",
    "\n",
    "from section_classifier_utils import SectionLRClassifierUtilities, SectionCRFClassifierUtilities\n",
    "slrcu = SectionLRClassifierUtilities(ha=ha, cu=cu, verbose=False)\n",
    "scrfcu = SectionCRFClassifierUtilities(cu=cu, ha=ha, verbose=False)\n",
    "\n",
    "from section_utils import SectionUtilities\n",
    "su = SectionUtilities(wsu=wsu, ihu=None, hc=hc, crf=None, slrcu=slrcu, verbose=False)\n",
    "\n",
    "duration_str = humanize.precisedelta(time.time() - t0, minimum_unit='seconds', format='%0.0f')\n",
    "print(f'Utility libraries created in {duration_str}')\n",
    "print(f'Last run on {datetime.now()}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I have 48,226 hand-labeled header htmls prepared\n",
      "7 iterations seen during training fit for a total of 96,452 records trained\n",
      "Is-header classifier trained in 9 seconds\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Train the isheader classifier\n",
    "t1 = time.time()\n",
    "ihu.build_pos_stochastic_gradient_descent_elements(verbose=True)\n",
    "duration_str = humanize.precisedelta(time.time() - t1, minimum_unit='seconds', format='%0.0f')\n",
    "print(f'Is-header classifier trained in {duration_str}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I have 48,758 labeled parts of speech in here\n",
      "predict_single is now available\n",
      "Parts-of-speech logistic regression elements built in 1 hour, 58 minutes and 56 seconds\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Keep the total creation time to less than one hour by adjusting the sampling strategy limit\n",
    "# I have 47,946 labeled parts of speech in here\n",
    "# pos_lr_predict_single is now available\n",
    "# Parts-of-speech logistic regression elements built in 2 hours, 18 minutes and 55 seconds\n",
    "t1 = time.time()\n",
    "if not (hasattr(slrcu, 'pos_predict_percent_fit_dict') or crf.is_flask_running()):\n",
    "    slrcu.build_pos_logistic_regression_elements(sampling_strategy_limit=None, verbose=True)\n",
    "if hasattr(slrcu, 'pos_predict_percent_fit_dict'):\n",
    "    print('predict_single is now available')\n",
    "else:\n",
    "    print('predict_single is not available')\n",
    "duration_str = humanize.precisedelta(time.time() - t1, minimum_unit='seconds', format='%0.0f')\n",
    "print(f'Parts-of-speech logistic regression elements built in {duration_str}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I have 48,758 labeled parts of speech in here\n",
      "predict_single is now available\n",
      "Parts-of-speech CRF elements built in 26 minutes and 21 seconds\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Check if the crf has built its parts-of-speech classifier\n",
    "# I have 46,569 labeled parts of speech in here\n",
    "# Parts-of-speech CRF elements built in 20 minutes and 35 seconds\n",
    "t1 = time.time()\n",
    "if not (hasattr(scrfcu, 'pos_predict_percent_fit_dict') or crf.is_flask_running()):\n",
    "    scrfcu.build_pos_conditional_random_field_elements(verbose=True)\n",
    "if hasattr(scrfcu, 'pos_predict_percent_fit_dict'):\n",
    "    print('predict_single is now available')\n",
    "else:\n",
    "    print('predict_single is not available')\n",
    "duration_str = humanize.precisedelta(time.time() - t1, minimum_unit='seconds', format='%0.0f')\n",
    "print(f'Parts-of-speech CRF elements built in {duration_str}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "----"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Run this if you haven't already created the file\n",
    "file_path = r'C:\\Users\\daveb\\OneDrive\\Documents\\GitHub\\job-hunting\\data\\html\\other_email.html'\n",
    "file_name = re.sub(r'[^A-Za-z0-9]+', ' ', '''\n",
    "    Senior Data Scientist (NLP)\n",
    "    Storm3 United States Remote\n",
    "    ''').strip().replace(' ', '_') + '.html'\n",
    "new_file_path = os.path.join(cu.SAVES_HTML_FOLDER, file_name)\n",
    "if os.path.isfile(new_file_path):\n",
    "    file_name = datetime.now().strftime('%Y%m%d%H%M%S%f') + f'_{file_name}'\n",
    "    new_file_path = os.path.join(cu.SAVES_HTML_FOLDER, file_name)\n",
    "if not os.path.isfile(new_file_path):\n",
    "    shutil.copy(file_path, os.path.join(cu.SAVES_HTML_FOLDER, file_name))\n",
    "    page_soup = wsu.get_page_soup(file_path)\n",
    "    div_soup = page_soup.find_all(name='div', id='jobDescriptionText')[0]\n",
    "    child_strs_list = ha.get_navigable_children(div_soup, [])\n",
    "    cu.ensure_filename(file_name, verbose=False)\n",
    "    cu.populate_from_child_strings(child_strs_list, file_name, verbose=False)\n",
    "    print(file_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Add the posting URL to the file name only if you have one\n",
    "posting_url = ''\n",
    "if posting_url:\n",
    "    cypher_str = f'''\n",
    "        MATCH (fn:FileNames {{file_name: \"{file_name}\"}})\n",
    "        SET fn.posting_url = \"{posting_url}\"\n",
    "        RETURN fn;'''\n",
    "    with cu.driver.session() as session:\n",
    "        row_objs_list = session.write_transaction(cu.do_cypher_tx, cypher_str)\n",
    "    display(row_objs_list)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "\n",
    "file_name = re.sub(r'[^A-Za-z0-9]+', ' ', '''\n",
    "    Senior Data Scientist (NLP)\n",
    "    Storm3 United States Remote\n",
    "    ''').strip().replace(' ', '_') + '.html'\n",
    "file_path = os.path.join(cu.SAVES_HTML_FOLDER, file_name)\n",
    "page_soup = wsu.get_page_soup(file_path)\n",
    "div_soup = page_soup.find_all(name='div', id='jobDescriptionText')[0]\n",
    "child_strs_list = ha.get_navigable_children(div_soup, [])\n",
    "print(file_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "child_tags_list = ha.get_child_tags_list(child_strs_list)\n",
    "\n",
    "feature_dict_list = cu.get_feature_dict_list(child_tags_list, child_strs_list)\n",
    "feature_tuple_list = []\n",
    "for feature_dict in feature_dict_list:\n",
    "    feature_tuple_list.append(hc.get_feature_tuple(\n",
    "        feature_dict, pos_lr_predict_single=slrcu.predict_single, pos_crf_predict_single=scrfcu.predict_single\n",
    "    ))\n",
    "crf_list = crf.CRF.predict_single(crf.sent2features(feature_tuple_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['O-JT', 'O-JT', 'O-OL', 'O-SP', 'O-TS', 'O-TS', 'H-TS', 'O-TS', 'O-TS', 'O-TS', 'O-TS', 'O-TS', 'O-TS', 'O-TS', 'H-RQ', 'H-RQ', 'O-RQ', 'O-RQ', 'O-RQ', 'O-RQ', 'O-RQ', 'O-RQ', 'O-RQ', 'O-RQ', 'O-RQ', 'O-RQ', 'O-RQ', 'H-PQ', 'H-PQ', 'O-PQ', 'O-PQ', 'O-PQ', 'O-PQ', 'O-PQ', 'O-PQ', 'O-PQ', 'H-SP', 'O-SP', 'O-SP', 'O-SP', 'O-SP', 'O-SP', 'O-SP', 'O-IP', 'O-IP', 'O-CS', 'O-CS']\n",
      "[16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "0 O-JT) <span style=\"color:#d6272880;\"><p>⚡ Senior Data Scientist (NLP) ⚡ (O-JT Job Title Non-header)</p></span><br />1 O-JT) <span style=\"color:#d6272880;\"><p>🏥 Recent Series B HealthTech 🏥 (O-JT Job Title Non-header)</p></span><br />2 O-OL) <span style=\"color:#c49c9480;\"><p>🗺️ US - Remote 🗺️ (O-OL Office Location Non-header)</p></span><br />3 O-SP) <span style=\"color:#17becf80;\"><p>💰 Up to $230K Base (Crucial Hire) + Strong Equity 💰 (O-SP Supplemental Pay Non-header)</p></span><br />4 O-TS) <span style=\"color:#9edae580;\"><p>They are looking for a Senior Data Scientist, specializing in NLP, to join the Data Science team, reporting directly to the Head of Data Science. (O-TS Task Scope Non-header)</p></span><br />5 O-TS) <span style=\"color:#9edae580;\"><p>You will be joining a small team tasked with the development and ownership of machine learning algorithms which lie at the heart of the client's product. (O-TS Task Scope Non-header)</p></span><br />6 H-TS) <span style=\"color:#9edae5ff;\"><p>What will you be doing? (H-TS Task Scope Header)</p></span><br />7 O-TS) <span style=\"color:#9edae580;\"><li>Development and ownership of NLP algorithms which leverage text data to solve healthcare problems (O-TS Task Scope Non-header)</li></span><br />8 O-TS) <span style=\"color:#9edae580;\"><li>Working with and improving on our in-house ML platform to train and deploy models (O-TS Task Scope Non-header)</li></span><br />9 O-TS) <span style=\"color:#9edae580;\"><li>Forming modeling hypothesis and validating them with experiments and exploratory analyses (O-TS Task Scope Non-header)</li></span><br />10 O-TS) <span style=\"color:#9edae580;\"><li>Collaborating with other Data Scientists on their algorithms and analyses (O-TS Task Scope Non-header)</li></span><br />11 O-TS) <span style=\"color:#9edae580;\"><li>Writing white papers about your research findings and submitting them to journals and conferences (O-TS Task Scope Non-header)</li></span><br />12 O-TS) <span style=\"color:#9edae580;\"><li>Attending and presenting papers at relevant ML conferences when appropriate (O-TS Task Scope Non-header)</li></span><br />13 O-TS) <span style=\"color:#9edae580;\"><li>Collaborating with our backend team in order to deliver ML predictions to production (O-TS Task Scope Non-header)</li></span><br />14 H-RQ) <span style=\"color:#bcbd22ff;\"><p>Requirements (H-RQ Required Qualifications Header)</p></span><br />15 H-RQ) <span style=\"color:#bcbd22ff;\"><p>Need to have… (H-RQ Required Qualifications Header)</p></span><br /><hr />16 O-RQ) <span style=\"color:#bcbd2280;\"><li>Bachelor’s Degree in a STEM subject (O-RQ Required Qualifications Non-header)</li></span><br />17 O-RQ) <span style=\"color:#bcbd2280;\"><li>Masters/PhD degree in NLP or 3+ years of industry experience in NLP (O-RQ Required Qualifications Non-header)</li></span><br />18 O-RQ) <span style=\"color:#bcbd2280;\"><li>2+ years of experience with production ML systems and platforms (O-RQ Required Qualifications Non-header)</li></span><br />19 O-RQ) <span style=\"color:#bcbd2280;\"><li>2+ years of software development experience in Python (O-RQ Required Qualifications Non-header)</li></span><br />20 O-RQ) <span style=\"color:#bcbd2280;\"><li>Experience with Pandas and NumPy (O-RQ Required Qualifications Non-header)</li></span><br />21 O-RQ) <span style=\"color:#bcbd2280;\"><li>Experience with PyTorch, TensorFlow, JAX, MXNet, or another deep learning framework (O-RQ Required Qualifications Non-header)</li></span><br />22 O-RQ) <span style=\"color:#bcbd2280;\"><li>Experience with UNIX-like environments (O-RQ Required Qualifications Non-header)</li></span><br />23 O-RQ) <span style=\"color:#bcbd2280;\"><li>Experience developing NLP algorithms and deploying them in production (i.e. customer-facing) (O-RQ Required Qualifications Non-header)</li></span><br />24 O-RQ) <span style=\"color:#bcbd2280;\"><li>Experience with modern software practices including automated testing and continuous delivery (O-RQ Required Qualifications Non-header)</li></span><br />25 O-RQ) <span style=\"color:#bcbd2280;\"><li>An ownership mentality, and the desire to take on heterogeneous challenges (O-RQ Required Qualifications Non-header)</li></span><br />26 O-RQ) <span style=\"color:#bcbd2280;\"><p>NOTE: The hours for this role overlap with our team in New Zealand. Specifically, we need you to be online from 9-11am New Zealand time. Of course, we’re happy for you to start your day later in order to accommodate. (O-RQ Required Qualifications Non-header)</p></span><br /><hr />27 H-PQ) <span style=\"color:#c7c7c7ff;\"><p>Nice to have… (H-PQ Preferred Qualifications Header)</p></span><br />28 H-PQ) <span style=\"color:#c7c7c7ff;\"><p>Some additional skills which we’d find useful if you have them, but should not affect your decision to apply: (H-PQ Preferred Qualifications Header)</p></span><br />29 O-PQ) <span style=\"color:#c7c7c780;\"><li>Experience with AWS managed services for ML: SageMaker, Redshift, DynamoDB, Lambda, and Glue (O-PQ Preferred Qualifications Non-header)</li></span><br />30 O-PQ) <span style=\"color:#c7c7c780;\"><li>Experience with MapReduce frameworks such as: Spark, Scalding, Beam, Scio, Crunch, Pig, etc (O-PQ Preferred Qualifications Non-header)</li></span><br />31 O-PQ) <span style=\"color:#c7c7c780;\"><li>Experience with big data querying tools such as: Redshift, BigQuery, Hive, Presto, Mode, etc (O-PQ Preferred Qualifications Non-header)</li></span><br />32 O-PQ) <span style=\"color:#c7c7c780;\"><li>Experience with big data systems: Hadoop, Dataproc, Dataflow, EMR, Databricks, etc (O-PQ Preferred Qualifications Non-header)</li></span><br />33 O-PQ) <span style=\"color:#c7c7c780;\"><li>Experience with No-SQL data bases: DynamoDB, BigTable, Cassandra, Elasticsearch, MongoDB, etc (O-PQ Preferred Qualifications Non-header)</li></span><br />34 O-PQ) <span style=\"color:#c7c7c780;\"><li>Published papers in ML or a related field (O-PQ Preferred Qualifications Non-header)</li></span><br />35 O-PQ) <span style=\"color:#c7c7c780;\"><li>Presented your work at a reputable scientific or technical conference (O-PQ Preferred Qualifications Non-header)</li></span><br />36 H-SP) <span style=\"color:#17becfff;\"><p>Benefits: (H-SP Supplemental Pay Header)</p></span><br />37 O-SP) <span style=\"color:#17becf80;\"><li>Highly Competitive Base (Crucial Hire) (O-SP Supplemental Pay Non-header)</li></span><br />38 O-SP) <span style=\"color:#17becf80;\"><li>Robust healthcare plans and 401K (O-SP Supplemental Pay Non-header)</li></span><br />39 O-SP) <span style=\"color:#17becf80;\"><li>Sick leave &amp; paid parental leave (O-SP Supplemental Pay Non-header)</li></span><br />40 O-SP) <span style=\"color:#17becf80;\"><li>Flexible working hours (O-SP Supplemental Pay Non-header)</li></span><br />41 O-SP) <span style=\"color:#17becf80;\"><li>Fully remote (O-SP Supplemental Pay Non-header)</li></span><br />42 O-SP) <span style=\"color:#17becf80;\"><li>Great people to work with – a diverse team/company who are passionate about improving people’s lives (O-SP Supplemental Pay Non-header)</li></span><br />43 O-IP) <span style=\"color:#ffbb7880;\"><i>📧 Interested in applying? Please click on the ‘Easy Apply’ button or email me at Alexander.hasselbach@storm3.com (O-IP Interview Procedures Non-header)</i></span><br />44 O-IP) <span style=\"color:#ffbb7880;\"><i>After applying, please feel free to connect with me on LinkedIn to discuss the role in more detail. (O-IP Interview Procedures Non-header)</i></span><br />45 O-CS) <span style=\"color:#1f77b480;\"><i>⚡ Storm3 is a HealthTech recruitment firm with clients across London, Europe and North America. To discuss open opportunities or career options, please visit our website www.storm3.com and follow the Storm3 Linked In page for the latest jobs and int (O-CS Corporate Scope Non-header)</i></span><br />46 O-CS) <span style=\"color:#1f77b480;\">el. (O-CS Corporate Scope Non-header)</span><br />"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "db_pos_list = []\n",
    "for navigable_parent in child_strs_list:\n",
    "    db_pos_list = cu.append_parts_of_speech_list(navigable_parent, pos_list=db_pos_list)\n",
    "pos_list, indices_list = su.visualize_basic_quals_section(crf_list, child_strs_list, db_pos_list=db_pos_list, verbose=True)\n",
    "winsound.Beep(freq, duration)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "----"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[3, 7, 11, 15, 16, 17, 18, 19, 20, 22, 23, 24, 25, 42]\n",
      "46 H-RQ) el.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Display the context of an individual child string\n",
    "idx = 46\n",
    "print(indices_list); child_str = child_strs_list[idx]; pos_symbol = pos_list[idx]; basic_quals_dict = s.load_object('basic_quals_dict')\n",
    "print(str(basic_quals_dict[child_str]) + '\\n' if(child_str in basic_quals_dict) else '', end='')\n",
    "print(f'{idx} {pos_symbol}) {child_str}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pickling to C:\\Users\\daveb\\OneDrive\\Documents\\GitHub\\job-hunting\\saves\\pkl\\basic_quals_dict.pkl\n",
      "\"<li>Presented your work at a reputable scientific or technical conference</li>\" in basic_quals_dict: 0\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Hand-label this particular child string in the quals dictionary\n",
    "basic_quals_dict = s.load_object('basic_quals_dict')\n",
    "basic_quals_dict[child_str] = 0\n",
    "s.store_objects(basic_quals_dict=basic_quals_dict); print(f'\"{child_str}\" in basic_quals_dict: {basic_quals_dict[child_str]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 iterations seen during updating fit for a total of 96,490 records trained\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'navigable_parent': 'el.', 'is_header': 'False', 'is_task_scope': 'False', 'is_qualification': None, 'is_minimum_qualification': 'False', 'is_preferred_qualification': 'False', 'is_legal_notification': 'False', 'is_job_title': 'False', 'is_office_location': 'False', 'is_job_duration': 'False', 'is_supplemental_pay': 'False', 'is_educational_requirement': 'False', 'is_interview_procedure': 'False', 'is_corporate_scope': 'True', 'is_posting_date': 'False', 'is_other': 'False'}]"
      ]
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "def do_cypher_tx(tx, navigable_parent, verbose=False):\n",
    "    cypher_str = \"\"\"MATCH (np:NavigableParents {navigable_parent: $navigable_parent})\n",
    "        SET\n",
    "            np.is_header = 'False',\n",
    "            np.is_task_scope = 'False',\n",
    "            np.is_minimum_qualification = 'False',\n",
    "            np.is_preferred_qualification = 'False',\n",
    "            np.is_educational_requirement = 'False',\n",
    "            np.is_legal_notification = 'False',\n",
    "            np.is_other = 'False',\n",
    "            np.is_corporate_scope = 'True',\n",
    "            np.is_job_title = 'False',\n",
    "            np.is_office_location = 'False',\n",
    "            np.is_job_duration = 'False',\n",
    "            np.is_supplemental_pay = 'False',\n",
    "            np.is_interview_procedure = 'False',\n",
    "            np.is_posting_date = 'False'\n",
    "        \"\"\" + cu.return_everything_str + ';'\n",
    "    results_list = tx.run(query=cypher_str, parameters={'navigable_parent': navigable_parent})\n",
    "\n",
    "    return [dict(record.items()) for record in results_list]\n",
    "with cu.driver.session() as session: row_objs_list = session.write_transaction(do_cypher_tx, navigable_parent=child_str)\n",
    "ihu.retrain_classifier(row_objs_list[0]['navigable_parent'], row_objs_list[0]['is_header'], verbose=True); row_objs_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'navigable_parent': '<p>Benefits:</p>', 'is_header': 'True', 'is_task_scope': 'False', 'is_qualification': 'False', 'is_minimum_qualification': 'False', 'is_preferred_qualification': 'False', 'is_legal_notification': 'False', 'is_job_title': 'False', 'is_office_location': 'False', 'is_job_duration': 'False', 'is_supplemental_pay': 'True', 'is_educational_requirement': 'False', 'is_interview_procedure': 'False', 'is_corporate_scope': 'False', 'is_posting_date': 'False', 'is_other': 'False'}]"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "# Show what's in the database already for this html string\n",
    "def do_cypher_tx(tx, navigable_parent, verbose=False):\n",
    "    cypher_str = '''\n",
    "        MATCH (np:NavigableParents {navigable_parent: $navigable_parent})\n",
    "        ''' + cu.return_everything_str + ';'\n",
    "    results_list = tx.run(query=cypher_str, parameters={'navigable_parent': navigable_parent})\n",
    "\n",
    "    return [dict(record.items()) for record in results_list]\n",
    "with cu.driver.session() as session:\n",
    "    row_objs_list = session.write_transaction(do_cypher_tx, navigable_parent=child_str, verbose=False)\n",
    "row_objs_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Remove this particular child string from the quals dictionary\n",
    "basic_quals_dict = s.load_object('basic_quals_dict')\n",
    "child_str = child_strs_list[idx]\n",
    "basic_quals_dict.pop(child_str)\n",
    "# basic_quals_dict[child_str] = 0\n",
    "s.store_objects(basic_quals_dict=basic_quals_dict)\n",
    "print(f'\"{child_str}\" in basic_quals_dict: {child_str in basic_quals_dict}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "----"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Job Hunting (Python 3.10.9)",
   "language": "python",
   "name": "jh_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
