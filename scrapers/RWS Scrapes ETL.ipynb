{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pretty printing has been turned OFF\n"
     ]
    }
   ],
   "source": [
    "\n",
    "%pprint\n",
    "import sys\n",
    "if ('../py' not in sys.path): sys.path.insert(1, '../py')\n",
    "from bs4 import BeautifulSoup as bs\n",
    "from bs4.formatter import  HTMLFormatter\n",
    "from jobpostlib import (cu, datetime, hau, humanize, nu, time, wsu, speech_engine, su)\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from urllib.parse import urlparse, parse_qs, urlencode\n",
    "import os\n",
    "import pyperclip\n",
    "import re"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "----"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Getting the FireFox driver\n",
      "platform.system() = Windows\n",
      "os.name = nt\n",
      "gecko_driver_path = GeckoDriverManager().install() = C:\\Users\\daveb\\.wdm\\drivers\\geckodriver\\win64\\v0.35.0\\geckodriver.exe\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "try:\n",
    "    driver = wsu.get_driver(verbose=True)\n",
    "    search_url = 'https://uscareers-rws.icims.com/jobs/search?pr=0&searchLocation=12781--Remote&schemaId=&o='\n",
    "    wsu.driver_get_url(driver, search_url, verbose=False)\n",
    "finally:\n",
    "    iframe_css_selector = 'iframe[src^=\"https://uscareers-rws.icims.com/jobs/search?pr=0&searchLocation=12781--Remote&schemaId=&o=&in_iframe=1\"]'\n",
    "    assert wsu.check_presence_by_css(driver, iframe_css_selector, wait=1, verbose=False), f\"There is no iframe on this page: {search_url}\"\n",
    "    \n",
    "    # Switch to the iframe with a generalized CSS selector\n",
    "    iframe = WebDriverWait(driver, 10).until(EC.frame_to_be_available_and_switch_to_it((\n",
    "        By.CSS_SELECTOR, iframe_css_selector\n",
    "    )))\n",
    "    \n",
    "    # Get the page soup of the iframe\n",
    "    page_soup = bs(driver.page_source, 'html.parser')\n",
    "    for script_soup in page_soup.find_all(name='script'):\n",
    "        script_soup.decompose()\n",
    "    css_selector = 'a[href^=\"https://uscareers-rws.icims.com/jobs/\"]'\n",
    "    link_soups_list = page_soup.select(css_selector)\n",
    "    \n",
    "    # Get rid of the duplicate URLs\n",
    "    url_strs_set = set()\n",
    "    for link_soup in link_soups_list:\n",
    "        \n",
    "        # URL string\n",
    "        url_str = link_soup['href']\n",
    "        \n",
    "        # Parse the URL; get query parameters as dictionary\n",
    "        # https://uscareers-rws.icims.com/jobs/3868/study-manager/job?in_iframe=1\n",
    "        parsed_url = urlparse(url_str)\n",
    "        query_params = parse_qs(parsed_url.query)\n",
    "        if '3868' in url_str: raise\n",
    "        \n",
    "        # Change the in_iframe parameter so that it doesn't put the updated_url in an iframe\n",
    "        if 'in_iframe' in query_params:\n",
    "            query_params['in_iframe'] = 0\n",
    "        \n",
    "        # Construct the updated query string\n",
    "        updated_query_str = urlencode(query_params, doseq=True)\n",
    "        \n",
    "        # Reconstruct the URL with the updated query string\n",
    "        updated_url = parsed_url.scheme + '://' + parsed_url.netloc + parsed_url.path\n",
    "        if updated_query_str:\n",
    "            updated_url += '?' + updated_query_str\n",
    "        \n",
    "        url_strs_set.add(updated_url)\n",
    "    \n",
    "    display(len(url_strs_set))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def get_iframe_soup(driver, url_str, verbose=False):\n",
    "    wsu.driver_get_url(driver, url_str, verbose=verbose)\n",
    "    iframe_css_selector = f'iframe[src^=\"{url_str}\"]'\n",
    "    assert wsu.check_presence_by_css(driver, iframe_css_selector, wait=1, verbose=verbose), f\"There is no iframe on this page: {url_str}\"\n",
    "    \n",
    "    # Switch to the iframe with a generalized CSS selector\n",
    "    iframe = WebDriverWait(driver, 10).until(EC.frame_to_be_available_and_switch_to_it((\n",
    "        By.CSS_SELECTOR, iframe_css_selector\n",
    "    )))\n",
    "    wsu.wait_for(wait_count=2, verbose=verbose)\n",
    "    \n",
    "    # Get the page soup of the iframe\n",
    "    page_soup = bs(driver.page_source, 'html.parser')\n",
    "    for script_soup in page_soup.find_all(name='script'):\n",
    "        script_soup.decompose()\n",
    "    if verbose:\n",
    "        formatter_obj = HTMLFormatter(indent=4)\n",
    "        html_str = page_soup.prettify(formatter=formatter_obj)\n",
    "        print(html_str)\n",
    "    \n",
    "    return page_soup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def get_link_soups(driver, i=0, verbose=False):\n",
    "    \n",
    "    # Get the page soup of the iframe\n",
    "    search_url = f'https://uscareers-rws.icims.com/jobs/search?pr={i}&searchLocation=12781--Remote&schemaId=&o='\n",
    "    page_soup = get_iframe_soup(driver, search_url, verbose=verbose)\n",
    "    \n",
    "    css_selector = 'a[href^=\"https://uscareers-rws.icims.com/jobs/\"]'\n",
    "    link_soups_list = page_soup.select(css_selector)\n",
    "    \n",
    "    return link_soups_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "link_soups_list = get_link_soups(driver, i=0, verbose=False)\n",
    "\n",
    "# Get rid of the duplicate URLs\n",
    "url_strs_set = set()\n",
    "pages_list = []\n",
    "for link_soup in link_soups_list:\n",
    "    url_str = link_soup['href']\n",
    "    if re.search(r'^https://uscareers-rws\\.icims\\.com/jobs/\\d+/', url_str):\n",
    "        url_strs_set.add(url_str)\n",
    "    elif re.search(r'^https://uscareers-rws\\.icims\\.com/jobs/search\\?pr=\\d+&in_iframe=1&searchLocation=12781--Remote', url_str):\n",
    "        parsed_url = urlparse(url_str)\n",
    "        query_params = parse_qs(parsed_url.query)\n",
    "        pages_list.append(int(query_params['pr'][0]))\n",
    "\n",
    "for i in range(1, max(pages_list)+1):\n",
    "    link_soups_list = get_link_soups(driver, i, verbose=False)\n",
    "    \n",
    "    # Get rid of the duplicate URLs\n",
    "    for link_soup in link_soups_list:\n",
    "        url_str = link_soup['href']\n",
    "        if re.search(r'^https://uscareers-rws\\.icims\\.com/jobs/\\d+/', url_str):\n",
    "            url_strs_set.add(url_str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving to C:\\Users\\daveb\\OneDrive\\Documents\\GitHub\\job-hunting\\saves\\html\\2024_2458_RWS_Data_Annotator_for_AI_Models_US_Remote.html\n",
      "Saving to C:\\Users\\daveb\\OneDrive\\Documents\\GitHub\\job-hunting\\saves\\html\\2023_1528_RWS_Search_Quality_Rater_US_AZ_Remote.html\n",
      "Saving to C:\\Users\\daveb\\OneDrive\\Documents\\GitHub\\job-hunting\\saves\\html\\2023_1534_RWS_Search_Quality_Rater_US_IA_Remote.html\n",
      "Saving to C:\\Users\\daveb\\OneDrive\\Documents\\GitHub\\job-hunting\\saves\\html\\2023_1549_RWS_Search_Quality_Rater_US_PA_Remote.html\n",
      "Fileing 4 postings complete. Delete the email.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Assumes this is the first time you've run this cell\n",
    "files_list = []\n",
    "for url_str in url_strs_set:\n",
    "    page_soup = get_iframe_soup(driver, url_str, verbose=False)\n",
    "    css_selector = 'h1.iCIMS_Header'\n",
    "    job_title_str = page_soup.select(css_selector)[0].text.split('|')[0].strip()\n",
    "    css_selector = 'div.col-xs-6:nth-child(2) > span:nth-child(2)'\n",
    "    subtitles_list = page_soup.select(css_selector)\n",
    "    if subtitles_list:\n",
    "        job_subtitle_str = subtitles_list[0].text.strip()\n",
    "        css_selector = 'div.iCIMS_JobHeaderTag:nth-child(1) > dd:nth-child(2) > span:nth-child(1)'\n",
    "        job_id = page_soup.select(css_selector)[0].text.strip()\n",
    "        page_title = f'{job_id} RWS {job_title_str} {job_subtitle_str}'\n",
    "        file_name = su.ascii_regex.sub(' ', page_title).strip().replace(' ', '_') + '.html'\n",
    "        file_path = os.path.join(cu.SAVES_HTML_FOLDER, file_name)\n",
    "        if not os.path.isfile(file_path):\n",
    "            css_selector = 'div.iCIMS_JobContent'\n",
    "            article_str = page_soup.select(css_selector)[0].text.strip()\n",
    "            \n",
    "            # Prettify the HTML\n",
    "            formatter_obj = HTMLFormatter(indent=4)\n",
    "            page_soup = bs(article_str, 'html.parser')\n",
    "            html_str = page_soup.prettify(formatter=formatter_obj)\n",
    "            \n",
    "            # Save the HTML to the file\n",
    "            with open(file_path, 'w', encoding=nu.encoding_type) as f:\n",
    "                print(f'Saving to {file_path}')\n",
    "                f.write('<html>\\n    <head>\\n        <title>')\n",
    "                f.write(page_title)\n",
    "                f.write('</title>\\n    </head>\\n    <body>\\n        <div id=\"jobDescriptionText\">\\n')\n",
    "                f.write(re.sub('^', '                ', html_str, 0, re.MULTILINE))\n",
    "                f.write('</div></body></html>')\n",
    "            \n",
    "            files_list.append(file_name)\n",
    "            cu.ensure_filename(file_name, verbose=False)\n",
    "            cu.set_posting_url(file_name, url_str, verbose=False)\n",
    "    \n",
    "speech_str = f'Fileing {len(files_list)} postings complete.'; print(speech_str); speech_engine.say(speech_str); speech_engine.runAndWait()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['2023_1540_RWS_Search_Quality_Rater_US_MN_Remote.html', '2023_1549_RWS_Search_Quality_Rater_US_PA_Remote.html', '2023_1534_RWS_Search_Quality_Rater_US_IA_Remote.html', '2023_1528_RWS_Search_Quality_Rater_US_AZ_Remote.html', '2024_2458_RWS_Data_Annotator_for_AI_Models_US_Remote.html', '2023_1536_RWS_Search_Quality_Rater_US_IN_Remote.html', '2023_1538_RWS_Search_Quality_Rater_US_MD_Remote.html', '2024_2358_RWS_Professional_Services_Consultant_Lead_US_Remote_UK_Remote.html', '2023_1541_RWS_Search_Quality_Rater_US_MO_Remote.html', '2023_1539_RWS_Search_Quality_Rater_US_MI_Remote.html', '2023_1550_RWS_Search_Quality_Rater_US_RI_Remote.html', '2023_1543_RWS_Search_Quality_Rater_US_NC_Remote.html', '2023_1530_RWS_Search_Quality_Rater_US_DE_Remote.html', '2023_1546_RWS_Search_Quality_Rater_US_NM_Remote.html', '2023_1548_RWS_Search_Quality_Rater_US_OK_Remote.html', '2023_1544_RWS_Search_Quality_Rater_US_NE_Remote.html', '2023_1533_RWS_Search_Quality_Rater_US_ID_Remote.html', '2023_1531_RWS_Search_Quality_Rater_US_FL_Remote.html', '2023_1545_RWS_Search_Quality_Rater_US_NH_Remote.html', '2023_1542_RWS_Search_Quality_Rater_US_MT_Remote.html', '2023_1551_RWS_Search_Quality_Rater_US_VA_Remote.html', '2024_3868_RWS_Study_Manager_US_Remote.html', '2023_1526_RWS_Search_Quality_Rater_US_TX_Remote_US_TX_Austin.html', '2023_1553_RWS_Search_Quality_Rater_US_WY_Remote.html', '2023_1527_RWS_Search_Quality_Rater_US_AR_Remote.html', '2023_1547_RWS_Search_Quality_Rater_US_OH_Remote.html', '2023_1532_RWS_Search_Quality_Rater_US_GA_Remote.html', '2023_1535_RWS_Search_Quality_Rater_US_IL_Remote.html', '2023_1552_RWS_Search_Quality_Rater_US_WI_Remote.html', '2024_3489_RWS_Data_Annotator_for_AI_Models_US_Remote.html']"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "t1 = time.time()\n",
    "try:\n",
    "    driver.close()\n",
    "except Exception as e:\n",
    "    print(f'{e.__class__.__name__} error: {str(e).strip()}')\n",
    "cu.ensure_navigableparent('END', verbose=False)\n",
    "for file_name in files_list:\n",
    "    file_path = os.path.join(cu.SAVES_HTML_FOLDER, file_name)\n",
    "    wsu.clean_job_posting(file_path)\n",
    "    page_soup = wsu.get_page_soup(file_path)\n",
    "    row_div_list = page_soup.find_all(name='div', id='jobDescriptionText')\n",
    "    assert row_div_list, f'{file_name} is missing <div id=\"jobDescriptionText\">'\n",
    "    for div_soup in row_div_list:\n",
    "        child_strs_list = hau.get_navigable_children(div_soup, [])\n",
    "        assert child_strs_list, f'{file_name} is missing its child strings'\n",
    "        cu.populate_from_child_strings(child_strs_list, file_name, verbose=False)\n",
    "duration_str = humanize.precisedelta(time.time() - t1, minimum_unit='seconds', format='%0.0f')\n",
    "speech_str = f'Populating {len(files_list)} out of {len(url_strs_set)} postings completed in {duration_str}'; speech_engine.say(speech_str); speech_engine.runAndWait()\n",
    "files_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "----\n",
    "# Maintenance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from bs4.formatter import HTMLFormatter\n",
    "from bs4 import BeautifulSoup as bs\n",
    "\n",
    "page_soup = bs(driver.page_source, 'html.parser')\n",
    "for script_soup in page_soup.find_all(name='script'):\n",
    "    script_soup.decompose()\n",
    "formatter_obj = HTMLFormatter(indent=4)\n",
    "html_str = page_soup.prettify(formatter=formatter_obj)\n",
    "pyperclip.copy(html_str)"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Job Hunting (Python 3.10.9)",
   "language": "python",
   "name": "jh_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
