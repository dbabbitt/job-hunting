{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pretty printing has been turned OFF\n",
      "======== Neo4j/4.4.7 ========\n",
      "Utility libraries created in 14 seconds\n"
     ]
    }
   ],
   "source": [
    "\n",
    "%pprint\n",
    "import sys\n",
    "\n",
    "# Insert at 1, 0 is the script path (or '' in REPL)\n",
    "if ('../py' not in sys.path): sys.path.insert(1, '../py')\n",
    "from jobpostlib import (cu, datetime, duration, freq, hau, humanize, nu, scrfcu, slrcu, su, t0, time, wsu, speech_engine)\n",
    "import os\n",
    "from urllib.parse import urlparse, parse_qs\n",
    "import re\n",
    "import warnings\n",
    "from urllib.parse import urlparse, parse_qs, urlencode\n",
    "from selenium.webdriver.common.by import By\n",
    "from bs4 import BeautifulSoup as bs\n",
    "from bs4.formatter import  HTMLFormatter\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "job_title_css = 'h1.t-24'\n",
    "job_subtitle_css = 'div.jobs-unified-top-card__primary-description > span.jobs-unified-top-card__subtitle-primary-grouping'\n",
    "ascii_regex = re.compile('[^A-Za-z0-9]+')\n",
    "article_css = '.jobs-description__container'\n",
    "seemore_button_css = 'button.t-14'\n",
    "seemore_button_xpath = '/html/body//footer/button'\n",
    "details_css = '#job-details'\n",
    "nav_search_css = '.global-nav__content'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "----"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['powershell.exe', '-ExecutionPolicy', 'Bypass', '-Command', 'Start-Process \"C:\\\\Program Files\\\\Notepad++\\\\notepad++.exe\" -ArgumentList \"C:\\\\Users\\\\daveb\\\\OneDrive\\\\Documents\\\\GitHub\\\\job-hunting\\\\data\\\\html\\\\linkedin_email.html\"']\n"
     ]
    }
   ],
   "source": [
    "\n",
    "wsu.save_linkedin_email_to_file(verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Getting the FireFox driver\n",
      "platform.system() = Windows\n",
      "os.name = nt\n",
      "gecko_driver_path = GeckoDriverManager().install() = C:\\Users\\daveb\\.wdm\\drivers\\geckodriver\\win64\\v0.34.0\\geckodriver.exe\n",
      "Getting URL: https://www.linkedin.com/home\n",
      "Clicking /html/body/nav/div/a[2]\n",
      "Filling in the session_key field with dave.babbitt@gmail.com\n",
      "Clicking /html/body/div/main/div[2]/div[1]/form/div[3]/button\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "try:\n",
    "    driver = wsu.get_driver(verbose=True)\n",
    "    wsu.log_into_linkedin(driver)\n",
    "    speech_engine.say('Enter the verification code'); speech_engine.runAndWait()\n",
    "finally:\n",
    "    page_soup = wsu.get_page_soup('../data/html/linkedin_email.html')\n",
    "    css_selector = 'a[href^=\"https://www.linkedin.com/comm/jobs/view\"]'\n",
    "    link_soups_list = page_soup.select(css_selector)\n",
    "    \n",
    "    # Get rid of the duplicate URLs\n",
    "    url_strs_set = set()\n",
    "    for link_soup in link_soups_list:\n",
    "        \n",
    "        # URL string\n",
    "        url_str = link_soup['href']\n",
    "        \n",
    "        # Parse the URL\n",
    "        parsed_url = urlparse(url_str)\n",
    "        \n",
    "        # Get query parameters as dictionary\n",
    "        query_params = parse_qs(parsed_url.query)\n",
    "        \n",
    "        # Remove trk and trkEmail parameters if they exist\n",
    "        if 'trk' in query_params:\n",
    "            del query_params['trk']\n",
    "        if 'trkEmail' in query_params:\n",
    "            del query_params['trkEmail']\n",
    "        \n",
    "        # Construct the updated query string\n",
    "        updated_query_str = urlencode(query_params, doseq=True)\n",
    "        \n",
    "        # Reconstruct the URL with the updated query string\n",
    "        updated_url = parsed_url.scheme + '://' + parsed_url.netloc + parsed_url.path\n",
    "        if updated_query_str:\n",
    "            updated_url += '?' + updated_query_str\n",
    "        \n",
    "        url_strs_set.add(updated_url)\n",
    "    \n",
    "    display(len(url_strs_set))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving to C:\\Users\\daveb\\OneDrive\\Documents\\GitHub\\job-hunting\\saves\\html\\FS8seU7HWR9zf11ye36rlQ_Machine_Learning_Operations_Engineer_II_PySpark_Python_R_Milford_MA.html\n",
      "Saving to C:\\Users\\daveb\\OneDrive\\Documents\\GitHub\\job-hunting\\saves\\html\\Mc3nBYaQbit9hBuZUC7R7g_Machine_Learning_Engineer_Massachusetts_United_States.html\n",
      "Fileing 2 postings complete. Delete the email.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Assumes this is the first time you've run this cell\n",
    "files_list = []\n",
    "\n",
    "for url_str in url_strs_set:\n",
    "    wsu.driver_get_url(driver, url_str, verbose=False)\n",
    "    time.sleep(4)\n",
    "    titles_list = driver.find_elements(By.CSS_SELECTOR, 'h1.t-24')\n",
    "    if not titles_list:\n",
    "        titles_list = driver.find_elements(By.CSS_SELECTOR, '#ember130 > h2:nth-child(1)')\n",
    "    if titles_list:\n",
    "        job_title_str = titles_list[0].text\n",
    "        job_subtitle_str = driver.find_elements(By.CSS_SELECTOR, 'span.tvm__text:nth-child(1)')[0].text\n",
    "        page_title = f'{job_title_str} {job_subtitle_str}'\n",
    "        # print(page_title)\n",
    "        file_name = ascii_regex.sub(' ', page_title).strip().replace(' ', '_')\n",
    "        trackingId = ascii_regex.sub(' ', parse_qs(urlparse(url_str).query).get('trackingId', [''])[0]).strip().replace(' ', '_')\n",
    "        if len(trackingId):\n",
    "            file_name = f'{trackingId}_{file_name}.html'\n",
    "        else:\n",
    "            file_name = f'{file_name}.html'\n",
    "\n",
    "            # Assumes this is the first time you've run this cell\n",
    "            file_path = os.path.join(cu.SAVES_HTML_FOLDER, file_name)\n",
    "            if os.path.isfile(file_path):\n",
    "                file_name = datetime.now().strftime('%Y%m%d%H%M%S%f') + f'_{file_name}'\n",
    "\n",
    "        file_path = os.path.join(cu.SAVES_HTML_FOLDER, file_name)\n",
    "        if not os.path.isfile(file_path):\n",
    "            \n",
    "            # Save the HTML to the file\n",
    "            with open(file_path, 'w', encoding=nu.encoding_type) as f:\n",
    "                print(f'Saving to {file_path}')\n",
    "                f.write('<html>\\n    <head>\\n        <title>')\n",
    "                f.write(page_title)\n",
    "                f.write(\n",
    "                    '</title>\\n    </head>\\n    <body>\\n        <div class=\"jobsearch-JobComponent-description\">'\n",
    "                    + '\\n            <div class=\"jobsearch-JobComponent-description\" id=\"jobDescriptionText\">\\n'\n",
    "                )\n",
    "                overlay_tag = driver.find_elements(By.CSS_SELECTOR, nav_search_css)[0]\n",
    "                driver.execute_script(\"arguments[0].setAttribute('style','display:none;');\", overlay_tag)\n",
    "                wsu.click_by_xpath(driver, seemore_button_xpath, verbose=False)\n",
    "                web_obj = driver.find_elements(By.CSS_SELECTOR, details_css)[0]\n",
    "                article_str = web_obj.get_attribute('innerHTML').strip()\n",
    "                \n",
    "                # Prettify the HTML\n",
    "                formatter_obj = HTMLFormatter(indent=4)\n",
    "                page_soup = bs(article_str, 'html.parser')\n",
    "                html_str = page_soup.prettify(formatter=formatter_obj)\n",
    "                \n",
    "                f.write(re.sub('^', '                ', html_str, 0, re.MULTILINE))\n",
    "                f.write('</div></body></html>')\n",
    "            \n",
    "            # Delete the svg tags and tighten up the parent tag for easier viewing\n",
    "            with open(file_path, 'r', encoding=nu.encoding_type) as f:\n",
    "                html_str = f.read()\n",
    "            html_str = re.sub('<svg[^>]*>(<path[^>]*></path>)+</svg>', '', html_str)\n",
    "            html_str = re.sub('\\s*<style[^>]*>[^><]*</style>', '', html_str)\n",
    "            html_str = re.sub(r'<([^></ ]+)([^></]*)>[\\r\\n]+ +([^><\\r\\n]+)[\\r\\n]+ +</\\1>', r'<\\1\\2>\\3</\\1>', html_str)\n",
    "            with open(file_path, 'w', encoding=nu.encoding_type) as f:\n",
    "                f.write(html_str)\n",
    "            \n",
    "            files_list.append(file_name)\n",
    "        cu.ensure_filename(file_name, verbose=False)\n",
    "        cu.set_posting_url(file_name, url_str, verbose=False)\n",
    "speech_str = f'Fileing {len(files_list)} postings complete. Delete the email.'; print(speech_str); speech_engine.say(speech_str); speech_engine.runAndWait()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['FS8seU7HWR9zf11ye36rlQ_Machine_Learning_Operations_Engineer_II_PySpark_Python_R_Milford_MA.html', 'Mc3nBYaQbit9hBuZUC7R7g_Machine_Learning_Engineer_Massachusetts_United_States.html']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "t1 = time.time()\n",
    "try:\n",
    "    driver.close()\n",
    "except Exception as e:\n",
    "    print(f'{e.__class__.__name__} error: {str(e).strip()}')\n",
    "cu.ensure_navigableparent('END', verbose=False)\n",
    "for file_name in files_list:\n",
    "    file_path = os.path.join(cu.SAVES_HTML_FOLDER, file_name)\n",
    "    page_soup = wsu.get_page_soup(file_path)\n",
    "    row_div_list = page_soup.find_all(name='article')\n",
    "    for div_soup in row_div_list:\n",
    "        child_strs_list = hau.get_navigable_children(div_soup, [])\n",
    "        assert child_strs_list, f'Something is wrong with {file_name}'\n",
    "        cu.populate_from_child_strings(child_strs_list, file_name, verbose=False)\n",
    "duration_str = humanize.precisedelta(time.time() - t1, minimum_unit='seconds', format='%0.0f')\n",
    "speech_str = f'Populating {len(files_list)} out of {len(url_strs_set)} postings completed in {duration_str}'; speech_engine.say(speech_str); speech_engine.runAndWait()\n",
    "files_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Job Hunting (Python 3.10.9)",
   "language": "python",
   "name": "jh_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
