{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pretty printing has been turned OFF\n"
     ]
    }
   ],
   "source": [
    "\n",
    "%pprint"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "\n",
    "---\n",
    "# Load needed libraries and functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import sys\n",
    "\n",
    "# Insert at 1, 0 is the script path (or '' in REPL)\n",
    "if ('../py' not in sys.path): sys.path.insert(1, '../py')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "%matplotlib inline\n",
    "from datetime import datetime\n",
    "from neo4j.exceptions import ServiceUnavailable\n",
    "import humanize\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "import winsound\n",
    "\n",
    "bin_count = 12\n",
    "duration = 1000  # milliseconds\n",
    "freq = 880  # Hz\n",
    "height_inches = 3.0\n",
    "width_inches = 18.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======== Neo4j/4.4.7 ========\n",
      "Pickling to C:\\Users\\daveb\\OneDrive\\Documents\\GitHub\\job-hunting\\saves\\pkl\\basic_quals_dict.pkl\n",
      "Utility libraries created in 1 minute and 32 seconds\n",
      "Last run on 2023-03-02 14:27:14.021955\n"
     ]
    }
   ],
   "source": [
    "\n",
    "t0 = time.time()\n",
    "\n",
    "# Get the Neo4j driver\n",
    "from storage import Storage\n",
    "s = Storage()\n",
    "\n",
    "from ha_utils import HeaderAnalysis\n",
    "ha = HeaderAnalysis(s=s, verbose=False)\n",
    "\n",
    "from scrape_utils import WebScrapingUtilities\n",
    "wsu = WebScrapingUtilities(s=s)\n",
    "uri = wsu.secrets_json['neo4j']['connect_url']\n",
    "user =  wsu.secrets_json['neo4j']['username']\n",
    "password = wsu.secrets_json['neo4j']['password']\n",
    "\n",
    "# Get the neo4j object\n",
    "from cypher_utils import CypherUtilities\n",
    "cu = CypherUtilities(uri=uri, user=user, password=password, driver=None, s=s, ha=ha)\n",
    "\n",
    "try:\n",
    "    \n",
    "    version_str = cu.driver.get_server_info().agent\n",
    "    print(f'======== {version_str} ========')\n",
    "    \n",
    "    from hc_utils import HeaderCategories\n",
    "    hc = HeaderCategories(cu=cu, verbose=False)\n",
    "    \n",
    "    # Vary the sampling strategy limit so that the overall creation time is less than two minutes\n",
    "    from lr_utils import LrUtilities\n",
    "    lru = LrUtilities(ha=ha, cu=cu, hc=hc, verbose=False)\n",
    "    lru.build_isqualified_logistic_regression_elements(sampling_strategy_limit=10_000, verbose=False)\n",
    "    \n",
    "    import warnings\n",
    "    warnings.filterwarnings('ignore')\n",
    "except ServiceUnavailable as e:\n",
    "    print('You need to start Neo4j as a console')\n",
    "    raise\n",
    "except Exception as e:\n",
    "    print(f'{e.__class__}: {str(e).strip()}')\n",
    "duration_str = humanize.precisedelta(time.time() - t0, minimum_unit='seconds', format='%0.0f')\n",
    "winsound.Beep(freq, duration)\n",
    "print(f'Utility libraries created in {duration_str}')\n",
    "print(f'Last run on {datetime.now()}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "---\n",
    "# Load needed libraries and functions"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "\n",
    "%run ../load_magic/storage.py\n",
    "%run ../load_magic/environment.py\n",
    "from matplotlib_inline.backend_inline import set_matplotlib_formats\n",
    "from scipy.stats import entropy\n",
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "from sklearn.ensemble import StackingClassifier, VotingClassifier\n",
    "import gensim\n",
    "import inspect\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "\n",
    "set_matplotlib_formats('retina')\n",
    "notebook_path = get_notebook_path()\n",
    "s = Storage()\n",
    "assert s.pickle_exists('qualification_str_is_qualified_DICT')\n",
    "qualification_str_is_qualified_DICT = s.load_object('qualification_str_is_qualified_DICT')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "---\n",
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Rebuild the datframe from the database\n",
    "import pandas as pd\n",
    "\n",
    "def do_cypher_tx(tx, verbose=False):\n",
    "    cypher_str = '''\n",
    "        MATCH (qs:QualificationStrings)\n",
    "        RETURN\n",
    "            qs.qualification_str AS qualification_str,\n",
    "            qs.is_qualified AS is_qualified;\n",
    "        '''\n",
    "    results_list = tx.run(query=cypher_str)\n",
    "\n",
    "    return [dict(record.items()) for record in results_list]\n",
    "with cu.driver.session() as session:\n",
    "    row_objs_list = session.write_transaction(do_cypher_tx, verbose=False)\n",
    "quals_df = pd.DataFrame(row_objs_list)\n",
    "quals_df.is_qualified = quals_df.is_qualified.map(lambda x: bool(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Retraining complete\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "# Re-transform the bag-of-words and tf-idf from the new manual scores\n",
    "sents_list = quals_df.qualification_str.tolist()\n",
    "assert len(sents_list)\n",
    "\n",
    "# Bag-of-words\n",
    "cv = CountVectorizer(**{'analyzer': 'word', 'binary': False, 'decode_error': 'strict', 'lowercase': False, 'max_df': 1.0,\n",
    "                        'max_features': None, 'min_df': 0.0, 'ngram_range': (1, 5), 'stop_words': None, 'strip_accents': 'ascii',\n",
    "                        'tokenizer': ha.html_regex_tokenizer})\n",
    "bow_matrix = cv.fit_transform(sents_list)\n",
    "\n",
    "# Tf-idf, must get from BOW first\n",
    "tt = TfidfTransformer(**{'norm': 'l1', 'smooth_idf': True, 'sublinear_tf': False, 'use_idf': True})\n",
    "tfidf_matrix = tt.fit_transform(bow_matrix)\n",
    "\n",
    "# Re-train the classifier\n",
    "X = tfidf_matrix\n",
    "y = quals_df.is_qualified.to_numpy()\n",
    "# Best score: 0.850\n",
    "basic_quals_clf = LogisticRegression(C=10.0, class_weight='balanced', dual=False, fit_intercept=True, intercept_scaling=1, l1_ratio=None,\n",
    "                                    max_iter=1000, multi_class='auto', n_jobs=None, penalty='l1', random_state=None, solver='liblinear',\n",
    "                                    tol=0.0001, verbose=0, warm_start=False)\n",
    "basic_quals_clf.fit(X, y)\n",
    "\n",
    "# Re-calibrate the inference engine\n",
    "bq_cv_vocab = cv.vocabulary_\n",
    "bq_cv = CountVectorizer(vocabulary=bq_cv_vocab)\n",
    "bq_cv._validate_vocabulary()\n",
    "CLF_NAME = 'LogisticRegression'\n",
    "print('Retraining complete')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "---\n",
    "# Rescore the quals dataframe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Add the Standard Models to the Fit Estimators and Training Durations List"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.44760292, 0.55239708])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "import numpy as np\n",
    "\n",
    "counts_dict = quals_df.groupby('is_qualified').count().qualification_str.to_dict()\n",
    "np.array([counts_dict[False]/(counts_dict[False]+counts_dict[True]), counts_dict[True]/(counts_dict[False]+counts_dict[True])])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from sklearn.ensemble import AdaBoostClassifier, BaggingClassifier, ExtraTreesClassifier\n",
    "from sklearn.ensemble import GradientBoostingClassifier, RandomForestClassifier\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "# Get the models\n",
    "t0 = time.time()\n",
    "estimators_list = [\n",
    "                   # done in 3.035s\n",
    "                   # Best score: 0.734\n",
    "                   AdaBoostClassifier(algorithm='SAMME.R', learning_rate=1.0, n_estimators=5, random_state=None),\n",
    "\n",
    "                   # done in 332.301s\n",
    "                   # Best score: 0.814\n",
    "                   BaggingClassifier(base_estimator=None, bootstrap=True, bootstrap_features=False, max_features=0.5, max_samples=0.75,\n",
    "                                     n_estimators=10, n_jobs=None, oob_score=False, random_state=None, verbose=0, warm_start=False),\n",
    "\n",
    "                   # done in 221.215s\n",
    "                   # Best score: 0.683\n",
    "                   ExtraTreesClassifier(bootstrap=False, ccp_alpha=0.0, class_weight=None, criterion='gini', max_depth=100,\n",
    "                                        max_features='sqrt', max_leaf_nodes=None, max_samples=None, min_impurity_decrease=0.0,\n",
    "                                        min_samples_leaf=1, min_samples_split=2, min_weight_fraction_leaf=0.0, n_estimators=100,\n",
    "                                        n_jobs=None, oob_score=False, random_state=None, verbose=0, warm_start=False),\n",
    "\n",
    "                   # done in 135.438s\n",
    "                   # Best score: 0.698\n",
    "                   GradientBoostingClassifier(ccp_alpha=0.0, criterion='friedman_mse', init='zero', learning_rate=1.0, loss='deviance',\n",
    "                                              max_depth=3, max_features=None, max_leaf_nodes=None, min_impurity_decrease=0.0,\n",
    "                                              min_samples_leaf=1, min_samples_split=2, min_weight_fraction_leaf=0.0, n_estimators=1000,\n",
    "                                              n_iter_no_change=None, random_state=None, subsample=1.0, tol=0.0001, validation_fraction=0.1,\n",
    "                                              verbose=0, warm_start=False),\n",
    "\n",
    "                   # done in 289.938s\n",
    "                   # Best score: 0.779\n",
    "                   RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight=None, criterion='entropy', max_depth=None,\n",
    "                                          max_features=None, max_leaf_nodes=None, max_samples=None, min_impurity_decrease=0.0,\n",
    "                                          min_samples_leaf=2, min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
    "                                          n_estimators=1000, n_jobs=None, oob_score=False, random_state=None, verbose=0, warm_start=False),\n",
    "\n",
    "                   # done in 13.410s\n",
    "                   # Best score: 0.850\n",
    "                   LogisticRegression(C=10.0, class_weight='balanced', dual=False, fit_intercept=True, intercept_scaling=1, l1_ratio=None,\n",
    "                                      max_iter=1000, multi_class='auto', n_jobs=None, penalty='l1', random_state=None, solver='liblinear',\n",
    "                                      tol=0.0001, verbose=0, warm_start=False),\n",
    "\n",
    "                   # done in 6.014s\n",
    "                   # Best score: 0.764\n",
    "                   SVC(C=1.0, break_ties=False, cache_size=200, class_weight=None, coef0=0.0, decision_function_shape='ovr', degree=3,\n",
    "                       gamma='scale', kernel='rbf', max_iter=-1, probability=True, random_state=None, shrinking=True, tol=0.001,\n",
    "                       verbose=False),\n",
    "                   ]\n",
    "\n",
    "# Fit the data and add the duration and fitted models to lists\n",
    "fit_estimators_list = []\n",
    "training_durations_list = []\n",
    "for clf in estimators_list:\n",
    "    start_time = time.time()\n",
    "    fit_estimators_list.append(clf.fit(X, y))\n",
    "    stop_time = time.time()\n",
    "    training_durations_list.append(stop_time - start_time)\n",
    "duration_str = humanize.precisedelta(time.time() - t0, minimum_unit='seconds', format='%0.0f')\n",
    "winsound.Beep(freq, duration)\n",
    "print(f'Estimators list created in {duration_str}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raise"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Add the LDA Model to the Fit Estimators and Training Durations List"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "\n",
    "# Train the model on the corpus\n",
    "lda = LatentDirichletAllocation(n_components=2, doc_topic_prior=None, topic_word_prior=None, learning_method='batch',\n",
    "                                learning_decay=0.7, learning_offset=10.0, max_iter=10, batch_size=128, evaluate_every=-1,\n",
    "                                total_samples=1000000.0, perp_tol=0.1, mean_change_tol=0.001, max_doc_update_iter=100, n_jobs=None,\n",
    "                                verbose=0, random_state=None)\n",
    "lda.fit(X)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "\n",
    "from gensim.corpora.dictionary import Dictionary\n",
    "from gensim.models.ldamodel import LdaModel\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "# Build model with tokenized words\n",
    "tokenized_sents_list = [ha.html_regex_tokenizer(sent_str) for sent_str in sents_list]\n",
    "\n",
    "# Create a corpus from a list of texts\n",
    "headers_dictionary = Dictionary(tokenized_sents_list)\n",
    "headers_corpus = [headers_dictionary.doc2bow(tag_str) for tag_str in tokenized_sents_list]\n",
    "\n",
    "# Train the model on the corpus\n",
    "lda = LdaModel(corpus=headers_corpus, num_topics=2)\n",
    "fit_estimators_list.append(lda)\n",
    "\n",
    "stop_time = time.time()\n",
    "training_durations_list.append(stop_time - start_time)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Create the Inference Durations List"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "inference_durations_list = []\n",
    "quals_df[clf_name] = np.nan\n",
    "for clf in fit_estimators_list:\n",
    "    clf_name = str(type(clf)).split('.')[-1].split(\"'\")[0]\n",
    "    start_time = time.time()\n",
    "    for row_index, row_series in quals_df.iterrows():\n",
    "        qualification_str = row_series.qualification_str\n",
    "        if(clf_name == 'LdaModel'):\n",
    "            X_test = headers_dictionary.doc2bow(ha.html_regex_tokenizer(qualification_str))\n",
    "            result_list = lda[X_test]\n",
    "            if len(result_list) == 1:\n",
    "                result_tuple = result_list[0]\n",
    "            elif len(result_list) == 2:\n",
    "                result_tuple = result_list[1]\n",
    "                \n",
    "            # Assume it's the probability of the larger topic\n",
    "            y_predict_proba = 1.0 - result_tuple[1]\n",
    "            \n",
    "        else:\n",
    "            X_test = bq_tt.transform(bq_cv.transform([qualification_str])).toarray()\n",
    "            y_predict_proba = clf.predict_proba(X_test)[0][1]\n",
    "        quals_df.loc[row_index, clf_name] = y_predict_proba\n",
    "    stop_time = time.time()\n",
    "    inference_durations_list.append(stop_time - start_time)\n",
    "s.store_objects(quals_df=quals_df, inference_durations_list=inference_durations_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "clf_name"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Add the Stacking Classifier to the Estimators, Training and Durations List"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "clf = StackingClassifier(estimators=[(str(type(e)).split('.')[-1].split(\"'\")[0], e) for e in estimators_list],\n",
    "                         final_estimator=None, cv=None, stack_method='auto', n_jobs=None, passthrough=False, verbose=0)\n",
    "clf_name = str(type(clf)).split('.')[-1].split(\"'\")[0]\n",
    "quals_df = s.load_object('quals_df')\n",
    "quals_df[clf_name] = np.nan\n",
    "bq_cv_vocab = s.load_object('bq_cv_vocab')\n",
    "bq_cv = CountVectorizer(vocabulary=bq_cv_vocab)\n",
    "bq_cv._validate_vocabulary()\n",
    "bq_tt = s.load_object('bq_tt')\n",
    "X = bq_tt.transform(bq_cv.transform(quals_df.qualification_str.tolist())).toarray()\n",
    "y = quals_df.is_qualified.to_numpy()\n",
    "start_time = time.time()\n",
    "fit_estimators_list.append(clf.fit(X, y))\n",
    "stop_time = time.time()\n",
    "training_durations_list = s.load_object('training_durations_list')\n",
    "training_durations_list.append(stop_time - start_time)\n",
    "s.store_objects(fit_estimators_list=fit_estimators_list, training_durations_list=training_durations_list)\n",
    "\n",
    "# Re-score the quals dataframe\n",
    "inference_durations_list = s.load_object('inference_durations_list')\n",
    "start_time = time.time()\n",
    "for row_index, row_series in quals_df.iterrows():\n",
    "    qualification_str = row_series.qualification_str\n",
    "    X_test = bq_tt.transform(bq_cv.transform([qualification_str])).toarray()\n",
    "    y_predict_proba = clf.predict_proba(X_test)[0][1]\n",
    "    quals_df.loc[row_index, clf_name] = y_predict_proba\n",
    "stop_time = time.time()\n",
    "inference_durations_list.append(stop_time - start_time)\n",
    "s.store_objects(quals_df=quals_df, inference_durations_list=inference_durations_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Add the Voting Classifier to the Estimators, Training and Durations List"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "clf = VotingClassifier(estimators=[(str(type(e)).split('.')[-1].split(\"'\")[0], e) for e in estimators_list],\n",
    "                       voting='soft', weights=None, n_jobs=None, flatten_transform=True)\n",
    "clf_name = str(type(clf)).split('.')[-1].split(\"'\")[0]\n",
    "quals_df[clf_name] = np.nan\n",
    "fit_estimators_list = s.load_object('fit_estimators_list')\n",
    "start_time = time.time()\n",
    "fit_estimators_list.append(clf.fit(X, y))\n",
    "stop_time = time.time()\n",
    "training_durations_list = s.load_object('training_durations_list')\n",
    "training_durations_list.append(stop_time - start_time)\n",
    "s.store_objects(fit_estimators_list=fit_estimators_list, training_durations_list=training_durations_list)\n",
    "\n",
    "# Re-score the html quals dataframe for prediction comparisons\n",
    "bq_cv_vocab = s.load_object('bq_cv_vocab')\n",
    "bq_cv = CountVectorizer(vocabulary=bq_cv_vocab)\n",
    "bq_cv._validate_vocabulary()\n",
    "bq_tt = s.load_object('bq_tt')\n",
    "inference_durations_list = s.load_object('inference_durations_list')\n",
    "start_time = time.time()\n",
    "for row_index, row_series in quals_df.iterrows():\n",
    "    qualification_str = row_series.qualification_str\n",
    "    X_test = bq_tt.transform(bq_cv.transform([qualification_str])).toarray()\n",
    "    y_predict_proba = clf.predict_proba(X_test)[0][1]\n",
    "    quals_df.loc[row_index, clf_name] = y_predict_proba\n",
    "stop_time = time.time()\n",
    "inference_durations_list.append(stop_time - start_time)\n",
    "s.store_objects(quals_df=quals_df, inference_durations_list=inference_durations_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Create and store a dictionary of all the fitted classifiers\n",
    "fit_estimators_list = s.load_object('fit_estimators_list')\n",
    "fit_estimators_dict = {str(type(clf)).split('.')[-1].split(\"'\")[0]: clf for clf in fit_estimators_list}\n",
    "s.store_objects(FIT_ESTIMATORS_DICT=fit_estimators_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "print(quals_df.columns.tolist())\n",
    "quals_df.sample(5).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "metrics_list = ['accuracy_score', 'adjusted_mutual_info_score', 'adjusted_rand_score', 'average_precision_score',\n",
    "                'balanced_accuracy_score', 'cohen_kappa_score', 'completeness_score', 'explained_variance_score',\n",
    "                'f1_score', 'fowlkes_mallows_score', 'homogeneity_score', 'jaccard_score', 'mutual_info_score',\n",
    "                'normalized_mutual_info_score', 'precision_score', 'r2_score', 'recall_score', 'roc_auc_score', 'v_measure_score']\n",
    "exec('from sklearn.metrics import {}'.format(', '.join(metrics_list)))\n",
    "fit_estimators_list = s.load_object('fit_estimators_list')\n",
    "clf_name_list = [str(type(clf)).split('.')[-1].split(\"'\")[0] for clf in fit_estimators_list]\n",
    "quals_df = s.load_object('quals_df')\n",
    "y_true = quals_df.is_qualified.tolist()\n",
    "fit_match_series = (quals_df.is_qualified == True)\n",
    "yes_list = quals_df[fit_match_series].is_qualified.tolist()\n",
    "no_list = quals_df[~fit_match_series].is_qualified.tolist()\n",
    "columns_list = ['clf_name', 'training_duration', 'inference_duration', 'boundary_diff', 'clf_yes_entropy',\n",
    "                'relative_yes_entropy'] + metrics_list\n",
    "rows_list = []\n",
    "training_durations_list = s.load_object('training_durations_list')\n",
    "inference_durations_list = s.load_object('inference_durations_list')\n",
    "for column_name, training_duration, inference_duration in zip(clf_name_list, training_durations_list, inference_durations_list):\n",
    "    yes_series = quals_df[fit_match_series][column_name]\n",
    "    upper_bound = yes_series.min()\n",
    "    no_series = quals_df[~fit_match_series][column_name]\n",
    "    lower_bound = no_series.max()\n",
    "    y_pred = []\n",
    "    for p in quals_df[column_name]:\n",
    "        if p > 0.5:\n",
    "            y_pred.append(1)\n",
    "        else:\n",
    "            y_pred.append(0)\n",
    "    row_dict = {}\n",
    "    row_dict['clf_name'] = column_name\n",
    "    row_dict['training_duration'] = training_duration\n",
    "    row_dict['inference_duration'] = inference_duration\n",
    "    row_dict['boundary_diff'] = upper_bound-lower_bound\n",
    "    row_dict['clf_yes_entropy'] = entropy(pk=yes_series.tolist(), base=2)\n",
    "    row_dict['relative_yes_entropy'] = entropy(pk=yes_list, qk=yes_series.tolist(), base=2)\n",
    "    for metric_str in metrics_list:\n",
    "        try:\n",
    "            row_dict[metric_str] = eval('{}(y_true, quals_df[column_name].tolist())'.format(metric_str))\n",
    "        except Exception as e1:\n",
    "            try:\n",
    "                row_dict[metric_str] = eval('{}(y_true, y_pred)'.format(metric_str))\n",
    "            except Exception as e2:\n",
    "                row_dict[metric_str] = np.nan\n",
    "    rows_list.append(row_dict)\n",
    "entropy_df = pd.DataFrame(rows_list, columns=columns_list).dropna(axis='columns', how='all')\n",
    "entropy_df.set_index('clf_name', drop=True, inplace=True)\n",
    "s.store_objects(entropy_df=entropy_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "description_dict = {name: fn.__doc__.strip().split('\\n')[0] for name, fn in inspect.getmembers(sys.modules[__name__],\n",
    "                                                                                               inspect.isfunction) if name in metrics_list}\n",
    "assert s.pickle_exists('entropy_df')\n",
    "entropy_df = s.load_object('entropy_df')\n",
    "for name, cls in inspect.getmembers(sys.modules[__name__], inspect.isclass):\n",
    "    if name in entropy_df.index:\n",
    "        description_dict[name] = cls.__doc__.strip().split('\\n')[0]\n",
    "s.store_objects(metrics_list=metrics_list, description_dict=description_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "print(entropy_df.columns.tolist())\n",
    "metrics_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# 1 - second topic: LdaModel\t3.973441\t1.990886\t0.798208\t-0.819588\n",
    "# 1 - first topic:  LdaModel\t3.973441\t2.222728\t0.211814\t-4.210144\n",
    "# first topic:      LdaModel\t3.973441\t2.371093\t0.787072\t-0.987907\n",
    "# second topic:     LdaModel\t3.973441\t2.371093\t0.200678\t-4.379083\n",
    "columns_list = ['training_duration', 'inference_duration', 'balanced_accuracy_score', 'r2_score']\n",
    "entropy_df[columns_list].sort_values('balanced_accuracy_score', ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "description_dict = s.load_object('description_dict')\n",
    "if 'training_duration' not in description_dict:\n",
    "    description_dict['training_duration'] = 'The average training time in seconds'\n",
    "    s.store_objects(description_dict=description_dict)\n",
    "if 'inference_duration' not in description_dict:\n",
    "    description_dict['inference_duration'] = 'The average inference time in seconds'\n",
    "    s.store_objects(description_dict=description_dict)\n",
    "if 'clf_yes_entropy' not in description_dict:\n",
    "    description_dict['clf_yes_entropy'] = 'The entropy of the distribution for True probability values'\n",
    "    s.store_objects(description_dict=description_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "----"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Assume the entropy dataframe has been populated\n",
    "entropy_df = s.load_object('entropy_df')\n",
    "assert entropy_df.shape[0]\n",
    "description_dict = s.load_object('description_dict')\n",
    "columns_list = ['training_duration', 'inference_duration']\n",
    "for metric in columns_list:\n",
    "    if metric in description_dict:\n",
    "        print('{}: {}'.format(metric, description_dict[metric]))\n",
    "fig = plt.figure(figsize=(18, 8))\n",
    "ax = fig.add_subplot(1, 1, 1)\n",
    "AxesSubplot_obj = entropy_df.sort_values(columns_list[0], ascending=True)[columns_list].plot.bar(rot=45, ax=ax)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Assume the entropy dataframe has been populated\n",
    "entropy_df = s.load_object('entropy_df')\n",
    "assert entropy_df.shape[0]\n",
    "description_dict = s.load_object('description_dict')\n",
    "columns_list = ['balanced_accuracy_score', 'r2_score']\n",
    "for metric in columns_list:\n",
    "    if metric in description_dict:\n",
    "        print('{}: {}'.format(metric, description_dict[metric]))\n",
    "fig = plt.figure(figsize=(18, 8))\n",
    "ax = fig.add_subplot(1, 1, 1)\n",
    "AxesSubplot_obj = entropy_df.sort_values(columns_list[0], ascending=True)[columns_list].plot.bar(rot=45, ax=ax)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Assume the entropy dataframe has been populated\n",
    "entropy_df = s.load_object('entropy_df')\n",
    "assert entropy_df.shape[0]\n",
    "description_dict = s.load_object('description_dict')\n",
    "columns_list = ['training_duration', 'inference_duration', 'balanced_accuracy_score']\n",
    "for metric in columns_list:\n",
    "    if metric in description_dict:\n",
    "        print('{}: {}'.format(metric, description_dict[metric]))\n",
    "fig = plt.figure(figsize=(18, 8))\n",
    "ax = fig.add_subplot(1, 1, 1)\n",
    "ax.set_yscale('log')\n",
    "AxesSubplot_obj = entropy_df.sort_values(columns_list[0], ascending=True)[columns_list].plot.bar(rot=45, ax=ax)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "entropy_df = s.load_object('entropy_df')\n",
    "assert entropy_df.shape[0]\n",
    "metrics_list = s.load_object('metrics_list')\n",
    "custom_metrics_list = ['boundary_diff', 'clf_yes_entropy', 'relative_yes_entropy']\n",
    "columns_list = metrics_list + custom_metrics_list\n",
    "columns_list = [cn for cn, s in sorted([(cn, entropy_df[cn].std()) for cn in columns_list], key=lambda x: x[1], reverse=True)][:3]\n",
    "description_dict = s.load_object('description_dict')\n",
    "for metric in columns_list:\n",
    "    if metric in description_dict:\n",
    "        print('{}: {}'.format(metric, description_dict[metric]))\n",
    "df = entropy_df.sort_values(columns_list[0], ascending=True)[columns_list]\n",
    "AxesSubplot_obj = df.plot.bar(rot=45, figsize=(18, 8))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from IPython.display import display\n",
    "\n",
    "row_dict = {}\n",
    "for column_name in df.columns:\n",
    "    row_dict[column_name] = df[column_name].std()\n",
    "display(df.append(pd.DataFrame([row_dict], index=['Standard Deviation'])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "assert entropy_df.shape[0]\n",
    "metrics_list = s.load_object('metrics_list')\n",
    "columns_list = [cn for cn in metrics_list if 'accur' in cn.lower()]\n",
    "for metric in columns_list:\n",
    "    if metric in description_dict:\n",
    "        print('{}: {}'.format(metric, description_dict[metric]))\n",
    "AxesSubplot_obj = entropy_df.sort_values(columns_list[0], ascending=True)[columns_list].plot.bar(rot=45, figsize=(18, 8))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "entropy_df.sort_values('boundary_diff', ascending=False)[custom_metrics_list]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "assert entropy_df.shape[0]\n",
    "for metric in custom_metrics_list:\n",
    "    if metric in description_dict:\n",
    "        print('{}: {}'.format(metric, description_dict[metric]))\n",
    "AxesSubplot_obj = entropy_df.sort_values('boundary_diff', ascending=True)[custom_metrics_list].plot.bar(rot=45, figsize=(18, 8))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "entropy_df = s.load_object('entropy_df')\n",
    "assert entropy_df.shape[0]\n",
    "columns_list = ['average_precision_score', 'precision_score', 'recall_score']\n",
    "description_dict = s.load_object('description_dict')\n",
    "for metric in columns_list:\n",
    "    print('{}: {}'.format(metric, description_dict[metric]))\n",
    "AxesSubplot_obj = entropy_df.sort_values('precision_score', ascending=True)[columns_list].plot.bar(rot=45, figsize=(18, 8))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "columns_list = ['average_precision_score', 'precision_score', 'recall_score']\n",
    "extended_columns_list = ['training_duration', 'inference_duration'] + columns_list\n",
    "entropy_df.sort_values('precision_score', ascending=True)[extended_columns_list]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "----"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "[(str(type(e)).split('.')[-1].split(\"'\")[0], e) for e in fit_estimators_list]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "idx = 'LdaModel'\n",
    "mask_series = (entropy_df.index == idx)\n",
    "entropy_df[mask_series].T.to_dict()[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "clf_name = 'LdaModel'\n",
    "FIT_ESTIMATORS_DICT = s.load_object('FIT_ESTIMATORS_DICT')\n",
    "clf = FIT_ESTIMATORS_DICT[clf_name]\n",
    "quals_df[clf_name] = np.nan\n",
    "for row_index, row_series in quals_df.iterrows():\n",
    "    qualification_str = row_series.qualification_str\n",
    "    X_test = headers_dictionary.doc2bow(ha.html_regex_tokenizer(qualification_str))\n",
    "    result_list = lda[X_test]\n",
    "    if len(result_list) == 1:\n",
    "        result_tuple = result_list[0]\n",
    "    elif len(result_list) == 2:\n",
    "        \n",
    "        # Assume it's the first topic\n",
    "        result_tuple = result_list[0]\n",
    "        \n",
    "    y_predict_proba = result_tuple[1]\n",
    "    \n",
    "    quals_df.loc[row_index, clf_name] = y_predict_proba\n",
    "s.store_objects(quals_df=quals_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Job Hunting (Python 3.10.9)",
   "language": "python",
   "name": "jh_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
